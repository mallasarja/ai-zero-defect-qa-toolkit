,,,,,,,,R1,,,,R2,,,,R3,,,,,,,
AI Feature,Testscenario for entities,Application Name ,CeligoAI User Query Category,CeligoAI User Query Type,End point URL,Sample Input Used for Testing - For all User Queries,"Golden result (Run once, Validate and then populate this)",Predicted Result,BLEU Score,ROUGE,Similiarity Score,Predicted Result,BLEU Score,ROUGE,Similiarity Score,Predicted Result,BLEU Score,ROUGE,Similiarity Score,Qualified for Score Calculation? Yes /  No ,Reason,Result,Assignee
Copilot_01,To Verify the copilot suggestion are as expected.,Copilot,Simple,QA Query,/gptverse/copilot/suggestions,01_playground.json,"{
    ""extracted_context"": ""Playground > Editor examples > Form builder > Simple form"",
    ""suggestions"": [
        {
            ""title"": ""How to add a new form field?""
        },
        {
            ""title"": ""What are available field types?""
        },
        {
            ""title"": ""How to customize form layout?""
        }
    ]
}",,,,,,,,,,,,,,,,
Copilot_02,To Verify the copilot suggestion are as expected.,Copilot,Simple,QA Query,/gptverse/copilot/suggestions,02_HomeExports.json,"{
    ""extracted_context"": ""Home / Exports - Create Export"",
    ""suggestions"": [
        {
            ""title"": ""How do I configure the export settings?""
        },
        {
            ""title"": ""Can I create a new JDBC connection?""
        },
        {
            ""title"": ""What SQL query should I use for export?""
        }
    ]
}",,,,,,,,,,,,,,,,
Copilot_03,To Verify the copilot suggestion are as expected.,Copilot,Simple,QA Query,/gptverse/copilot/suggestions,03_MyAccount_Profile.json,"{
    ""extracted_context"": ""Home / My account / Profile - Profile"",
    ""suggestions"": [
        {
            ""title"": ""How do I update my profile information?""
        },
        {
            ""title"": ""Can I change my account email address?""
        },
        {
            ""title"": ""What are the password requirements for my account?""
        }
    ]
}",,,,,,,,,,,,,,,,
Copilot_04,To Verify the copilot suggestion are as expected.,Copilot,Simple,QA Query,/gptverse/copilot/suggestions,04_FlowBulder_AddSource.json,"{
    ""extracted_context"": ""Home / Standalone flows / Flow builder - New Flow"",
    ""suggestions"": [
        {
            ""title"": ""How do I clone a flow step?""
        },
        {
            ""title"": ""What are popular marketplace resources?""
        },
        {
            ""title"": ""How to use Advanced Field Editor?""
        }
    ]
}",,,,,,,,,,,,,,,,
Copilot_05,To Verify the copilot suggestion are as expected.,Copilot,Simple,QA Query,/gptverse/copilot/suggestions,05_Connection_Create.json,"{
    ""extracted_context"": ""Home / Connections - Connections List"",
    ""suggestions"": [
        {
            ""title"": ""How to create a new connection?""
        },
        {
            ""title"": ""How to edit an existing connection?""
        },
        {
            ""title"": ""How to troubleshoot an offline connection?""
        }
    ]
}",,,,,,,,,,,,,,,,
Copilot_06,To Verify the copilot suggestion are as expected.,Copilot,Simple,QA Query,/gptverse/copilot/suggestions,06_BuildSQLquery.json,"{
    ""extracted_context"": ""Build SQL query - Where employee id is greater than 10"",
    ""suggestions"": [
        {
            ""title"": ""How to add another condition to the query?""
        },
        {
            ""title"": ""Can I sort results by employee name?""
        },
        {
            ""title"": ""How to save this SQL query for later use?""
        }
    ]
}",,,,,,,,,,,,,,,,
Copilot_07,To Verify the copilot suggestion are as expected.,Copilot,Simple,QA Query,/gptverse/copilot/suggestions,07_Agents.json,"{
    ""extracted_context"": ""Resources - Agents"",
    ""suggestions"": [
        {
            ""title"": ""How to generate an integrator.io access token?""
        },
        {
            ""title"": ""How to troubleshoot agent installation issues?""
        },
        {
            ""title"": ""How to configure agent settings after installation?""
        }
    ]
}",,,,,,,,,,,,,,,,
Copilot_08,To Verify the copilot suggestion are as expected.,Copilot,Simple,QA Query,/gptverse/copilot/suggestions,08_Imports.json,"{
    ""extracted_context"": ""Resources - Imports"",
    ""suggestions"": [
        {
            ""title"": ""How to create a virtual import in JavaScript?""
        },
        {
            ""title"": ""How to handle errors in import scripts?""
        },
        {
            ""title"": ""How to define import data variables?""
        }
    ]
}",,,,,,,,,,,,,,,,
Copilot_09,To Verify the copilot suggestion are as expected.,Copilot,Simple,QA Query,/gptverse/copilot/suggestions,09_Mappings.json,"{
    ""extracted_context"": ""Flow Builder - Edit Mapping: Mssql Imports"",
    ""suggestions"": [
        {
            ""title"": ""How do I configure import mappings using Mapper 2.0?""
        },
        {
            ""title"": ""What is the difference between results and response mapping?""
        },
        {
            ""title"": ""How can I auto-populate destination fields in mappings?""
        }
    ]
}",,,,,,,,,,,,,,,,
Copilot_10,To Verify the copilot suggestion are as expected.,Copilot,Simple,QA Query,/gptverse/copilot/suggestions,10_Filters.json,"{
    ""extracted_context"": ""Home / AI_ML_Testing / Flow builder - Define output filter - Conditions"",
    ""suggestions"": [
        {
            ""title"": ""How do I create an output filter condition?""
        },
        {
            ""title"": ""Can I test filter expressions with sample data?""
        },
        {
            ""title"": ""What happens to records not meeting filter criteria?""
        }
    ]
}",,,,,,,,,,,,,,,,
Copilot_11,To Verify the copilot suggestion are as expected.,Copilot,Simple,QA Query,/gptverse/copilot/suggestions,11_Transformations.json,"{
    ""extracted_context"": ""Home / AI_ML_Testing / Flow builder - Define transformation"",
    ""suggestions"": [
        {
            ""title"": ""How do I create a new transformation rule?""
        },
        {
            ""title"": ""Can I preview the transformed output data?""
        },
        {
            ""title"": ""How do I switch to JavaScript transformation?""
        }
    ]
}",,,,,,,,,,,,,,,,
Copilot_50,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,99_Knowledgebase.json,"{
    ""answer"": ""Based on your question \""What is this?\"", you are currently on the *\""Dashboard - Running flows\""* page in *Celigo's integrator.io platform*. Here's an explanation of what this page is:\n\n### What is the \""Dashboard - Running Flows\"" page?\n\nThe \""Running flows\"" tab in the Account Dashboard shows all the flows that are currently in progress or are waiting in the queue. This is where you monitor active flows in real-time as they process records.\n\n---\n\n### Key Elements on this Page:\n1. **Integration**: Displays the specific integration that contains the currently running flow. (You can filter flows by integration using the funnel icon.)\n2. **Flow**: Lists the specific flow that is actively running or queued. You can filter flows by name using the funnel icon.\n3. **Status**: Tells you if the flow is running, in progress, or waiting in the queue. You can open the *Queued jobs* window by clicking the status to view processing details or cancel jobs.\n4. **Started Column**: Indicates when the flow started, either as a timestamp or relative time (e.g., 3 hours ago, depending on your profile settings).\n5. **Success, Ignored, Errors, and Auto-Resolved Counts**: These columns provide real-time metrics on records being processed:\n   - **Success**: Number of successfully processed records.\n   - **Ignored**: Number of records ignored during processing.\n   - **Errors**: Number of errors encountered.\n   - **Auto-resolved**: Errors resolved automatically using integrator.io trace keys.\n6. **Pages**: Shows the number of processed pages for the flow. Paginated responses are often used for large record exports.\n7. **Actions**: Allows you to cancel a currently running flow or one waiting in the queue.\n\n---\n\n### What should you do on this page?\n1. **Monitor Active Flows**:\n   - View details of flows currently processing records.\n   - Check the status to ensure everything is proceeding correctly.\n2. **Manage Errors in Real-Time**:\n   - Watch for accumulating errors in the *Errors* or *Auto-resolved* columns to pinpoint issues as they arise.\n3. **Cancel or Investigate Jobs**:\n   - Use the *Actions* menu to cancel a flow if it is stuck or unnecessary.\n   - Click on the Status column to see additional details on jobs that are in progress or queued.\n4. **Filter or Drill Down**:\n   - Use the funnel icons to filter flows by integration, flow name, or status.\n   - Click on a specific flow to access more detailed metrics or adjust settings as needed.\n\n> If your question pertains to any specific element you're seeing on this page, feel free to ask for further clarification!"",
    ""completion_tokens"": """",
    ""openai_duration"": 19.17,
    ""prompt_tokens"": """",
    ""sources"": [
        {
            ""articleId"": 4404692954395.0,
            ""content"": ""The Account Dashboard allows you to view all currently running and completed flows in the account. You can access the Account Dashboard from the left navigation menu.\nThe Account Dashboard has two tabs: Running flows and Completed flows.\nRunning flows\nThe Running flows tab displays all flows currently in progress.\nIntegration: displays a link to the integration that contains the currently running flow. Click the funnel icon in the column title to limit the display to specific integrations.\nFlow: displays a link to the flow that is in-flight. Click the funnel icon in the column title to limit the display to specific flows.\nStatus: displays the status of the currently running flow that is in-flight. Click the funnel icon in the column title to limit the display to specific statuses for currently running flows.\nStarted: displays the number of records currently in processing for the flow.\nSuccess: displays the number of successfully processed records for the currently running flow.\nIgnored: displays the number of ignored records for the currently running flow.\nErrors: displays the number of errors that have occurred for the currently running flow.\nAuto-resolved: displays the number of records that errored and were automatically resolved for the currently running flow. (These are errors that have matching trace key values that were resolved by integrator.io).\nPages: displays the number of processed pages for the currently running flow.\nActions: Allows you to cancel the flow that is in progress or waiting in queue.\nCompleted flows\nThe Completed flows tab displays all flows completed within the selected date range.\nCompleted date range: Select a date range to limit the completed flows list. (defaults to Last 24 hours).\nYou can choose any of the following presets:\nToday\nLast 24 hours (default)\nLast 36 hours\nLast 7 days\nLast 15 days\nLast 30 days\nCustom: opens a calendar that you can use to select beginning and end dates to define your date range.\nIntegration: displays a link to the integration that contains the completed flow. Click the funnel icon in the column title to limit the display to specific integrations.\nFlow: displays a link to the completed flow. Click the funnel icon in the column title to limit the display to specific flows.\nOpen errors: displays the total number of errors that have not been resolved in the flow. Click the number to view the application that caused the error. Resolved errors are not included in this count. You can sort the flow list by the number of errors in ascending or descending order by clicking the Open errors column header.\nClick the number in the Errors column to view more detailed information about the errors that happened during the completed flow.\nYou can click Actions (...)>Download errors and select the date range. When you click Download, integrator.io creates a CSV file that lists all errors within the date range you defined.\nMessage: displays the error message returned from the API that encountered the error.\nCode: displays the response code returned from the API that encountered the error.\nSource: displays the application that encountered the error. Click the funnel icon to isolate the list to a specific application.\nClassification: displays the classification of the error. Click the funnel icon to isolate the list of errors to a particular classification set.\nTimestamp: displays the time at which the error occurred. Click the funnel icon to isolate the list of errors to those that occurred within a specific timeframe.\nWhen you select an error, the row is highlighted and the Error details are displayed beside the error.\nEdit retry data: opens the failed record in a text editor for modification.\nView HTTP request: displays the HTTP request sent from integrator.io.\nView HTTP response: displays the HTTP response returned from the application.\nError fields: shows the error-related fields.\nBased on these fields, you can retry or resolve errors individually or as a batch and monitor your retry runs. You can see the number of retried and resolved errors in the upper-right corner.\nLast open error: The time at which the most recent error occurred. For example, if an error occurred on 9/10/2021 at 10:00:00 AM and it was resolved at 10:01:00 AM, the Last open error would still show 9/10/2021 at 10:00:00 AM.\nLast run completed: The time at which the last run of the flow was finished.\nRuns: The number of runs that's been performed for a flow. You can click the run count and view the Run history, which provides details of each flow run. You can also filter the run history for any analysis.Click the arrow next to a flow to drill down and see details at a more granular flow step level.\nAverage run time: The average time it took the listed flow to run for the selected date range.\nSuccess: The number of successful records processed by the flow for the selected date range.\nIgnored: The number of ignored records for the selected date range.\nErrors: The total number of errors generated by the flow for the selected date range.\nAuto-resolved: The number of errors that were auto-resolved based on a match to another error's unique field identifier (trace key) for the selected date range.\nUser-resolved: The number of errors marked as resolved by an integrator.io user.\nPages: The total number of pages processed by the flow for the selected date range."",
            ""date"": ""Thu, 26 Sep 2024 00:48:36 GMT"",
            ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/4404692954395-View-all-activity-in-the-Account-Dashboard"",
            ""linkedArticleIds"": [
                ""360060740672"",
                ""10080012410651"",
                ""4403697564429"",
                ""10080012410651"",
                ""10080012410651"",
                ""10080012410651"",
                ""10107073900571"",
                ""4422562236827"",
                ""360060740672""
            ],
            ""numTokens"": 1073.0,
            ""score"": 0.86661166,
            ""section_id"": 6306461554075.0,
            ""source"": ""articles"",
            ""title"": ""View all activity in the Account Dashboard""
        },
        {
            ""articleId"": 7326016708763.0,
            ""content"": ""The integration dashboard shows you the status of every flow that ran for the last 30 days or however long your data retention plan allows. From here you can watch your flows run in real-time, review errors as they occur, and review your run times. The dashboard has two tabs: Running flows and Completed flows.\nRunning flows\nYou can see any currently running or queued flows from this tab and review any real-time errors. For larger integrations, you can filter by flow name and status.\nFlow status\nWhen a flow runs, integrator.io creates a set of jobs to complete the tasks configured at each flow step. These jobs are stacked in a processing queue until the flow is complete. If a flow is scheduled to run while one or more of its connections are still in use by another active flow, the Dashboard tab displays the message, Waiting in queue next to the flow awaiting processing. integrator.io finishes the active flow before moving on to the next flow in the queue.\nWhen flows are processing, the Status column displays an In progress message. You can click the message in the Status column to open the Queued jobs window. From here you can view all flows and associated messages currently processing (or queued to run) and cancel jobs in progress.\nThe Started column displays the timestamp for your flow’s latest run. You can configure your account profile to display timestamps in any date/time format. If you’ve set your account to relative time, the timestamp is displayed as the length of time between the started date and time and the current date and time (1 day ago, 3 hours ago).\nThe Success and Ignored columns display the number of successful and any ignored records.\nYou can also cancel a running flow using the Actions overflow menu.\nErrors\nThere’s a live count of accumulating errors and auto-resolved errors . integrator.io auto-resolves errors by using trace keys to find and resolve duplicate errors. The trace key is the unique identifier of a record, as determined by integrator.io. For example, when exporting a record from NetSuite, the field \""id\"" is automatically populated as the trace key. Then, Auto-resolve will apply AI/ML analysis to the trace key to attempt to resolve similar errors in identical records.\nPages (pagination)\nWhen configuring exports with a universal connector (like the HTTP connector or the REST connector ), configure the pagination strategy used by the service you are exporting data from. For dedicated connectors that Celigo has created for the service, these settings are configured automatically, but you may need to review the API documentation for services that don't use a connector that Celigo has prebuilt for the service.\nFilter Dashboard by application\nUsing the Select application option, you can now narrow down and filter the list of running and completed flows for the applications that are used in your account.\nCompleted flows\nYou can see all your canceled and completed flows in this tab, along with all the errors that occurred during the flow run.\nSort your flows by flow name and by Completed date range. Your date range is customizable, from the current date to the last 30 days or however long your data retention plan allows; however, you can’t sort your date range by time.\nCustomize the number of Results per page ranging from 10-50 flows per page. You can also refresh the dashboard anytime to get the most up-to-date flow results.\nYou can run or edit a flow using the Actions overflow menu.\nErrors\nThere are several columns you can review to better understand your errors. These columns are based on your Completed date range.\nThe Success and Ignored columns display the number of successful and ignored records.\nThe Last open error displays the timestamp for the most recent error. You can configure your account profile to display timestamps in any date/time format. If you’ve set your account to relative time, the timestamp is displayed as the length of time between the started date and time and the current date and time (1 day ago, 3 hours ago).\nThe Errors column displays the number of errors in your flow, not including open errors. The Auto-resolved column displays the errors that were resolved by integrator.io. The User-resolved column logs errors that were manually resolved by the user.\nRuns\nIf you want to get a quick overview of the number of runs that's been performed for a flow, see the count in the Runs column. You can click the run count to view the Run history, which provides details of each flow run. You can also filter the run history for any analysis.\nClick the arrow next to a flow to drill down and see details at a more granular flow step level.\nPages (pagination)\nWhen configuring exports with a universal connector (like the HTTP connector or the REST connector ), configure the pagination strategy used by the service you are exporting data from. For dedicated connectors that Celigo has created for the service, these settings are configured automatically, but you may need to review the API documentation for services that don't use a connector that Celigo has prebuilt for the service.\nAdd pagination to a universal connection's export\nWhen you click an error, a pop-up displays all your exports and imports, the open error count for each flow step, and the date and time of the last open error. From there you can click the flow step with errors and review both open and resolved errors.\nFlow steps with errors\nOpen Errors\nIn the Open errors tab, you can review the unresolved errors that occurred for every run in the flow.\nOpen errors\nNote\nYou can toggle between a new unified view (default) and the old list view; but, the option to toggle views is only available in the Open errors tab. That is, the unified error view applies to open errors.\nFrom here you can Search for, Resolve, Retry or Refresh the error(s). Errors have a unique Message and Code that you can use to determine the type of error you’ve encountered. Download the errors to get the data as a CSV file.\nYou can filter errors based on source, classification, and timestamp.\nThe Source filter tells you where your error originated. Narrow this down to any area of your flow.\nThe Classification filter enables you to select and view a specific type of error. Classifications include:\nall classifications\nconnection\nduplicate\ngovernance\nintermittent\nmissing\nparse\nvalue\nThe Timestamp filter enables you to view errors based on a day or date range. For instance, today, yesterday, in the past 24 hours, 7 days, 15 days, 30 days, or define a custom timeframe.\nIn the default unified error view when you select an error, the row is highlighted and the Error details are displayed beside the error.\nEdit retry data: opens the failed record in a text editor for modification.\nView HTTP request: displays the HTTP request sent from integrator.io.\nView HTTP response: displays the HTTP response returned from the application.\nError fields: shows the error-related fields.\nWhen you retry or resolve errors, you can view the details in the Retries tab.\nResolved Errors\nThe Resolved errors tab is identical to the Open errors tab, except for the Resolved by, Resolved at, and Actions columns.\nResolved errors\nResolved by identifies the account user that resolved the error. This can be an authorized user or it can be auto-resolved by integrator.io.\nResolved at displays the date and time that your error was resolved. These dates are in your account’s selected Date and Time format. If you’ve opted to show your timestamps as relative, it displays the amount of time elapsed since an event occurred, like 1 hour ago or 1 day ago.\nYou can take Actions such as viewing error details and purging errors."",
            ""date"": ""Thu, 28 Nov 2024 06:42:13 GMT"",
            ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/7326016708763-Explore-the-integration-dashboard"",
            ""label_names"": [
                ""dashboard"",
                ""integration"",
                ""queued flows"",
                ""completed flows"",
                ""flow monitoring"",
                ""resolve errors"",
                ""integration dashboard"",
                ""dashboard integrations"",
                ""flow status"",
                ""running flows"",
                ""open errors"",
                ""resolved errors""
            ],
            ""linkedArticleIds"": [
                ""4422562236827""
            ],
            ""numTokens"": 1557.0,
            ""score"": 0.847180605,
            ""section_id"": 6306461554075.0,
            ""source"": ""articles"",
            ""title"": ""Explore the integration dashboard""
        },
        {
            ""articleId"": 4422562236827.0,
            ""content"": ""When you view a flow, the Run history tab provides an overview of your flow's status, duration, and start and completion times, among other things. You can check the Hide empty runs box to exclude all jobs without records (i.e. Success, Ignored, and Errors columns are all 0).\nFilter by date range\nYou don’t need to scroll through the entire history of your flow to pinpoint a date and time. You can choose from six predetermined ranges or create a custom range:\nToday\nYesterday\nLast 24 hours\nLast 7 days\nLast 15 days\nLast 30 days\nCustom\nUsing ranges helps you review flows that run once a day or less; for example, every other day, weekly, bi-weekly, or monthly flows. On the other hand, if you’re looking for a specific date(s) or time(s) you should try the Custom range.\nCustom ranges are helpful for reviewing large data sets (over 30 days) or more specific dates (Monday to Wednesday). You can also filter by a specific time or range of times, like 12:00 a.m. to 1:00 p.m. These custom ranges can be combined to create unique date and time searches such as 10/20 to 10/23 from 12 a.m. to 1 p.m.\nFilter by status\nYou can filter the run history based on the status. Each status has a distinct color, and this visual cue enables you to get a high-level understanding of flow runs at a glance. For example, if you see a lot of yellow in the Status column, this means that a lot of your flows were canceled and you might want to filter based on the Canceled status to view all the canceled runs and investigate further. Filtering flow runs can enable you to examine and analyze details and find patterns or detect issues. You can select any of the following flow statuses to filter and view your runs:\nIf you select the Canceled status to filter, you can additionally select Canceled by and view flow runs canceled by any user or the system.\nNow, you can easily view who canceled a flow run by clicking the info icon next to the Canceled status."",
            ""date"": ""Thu, 26 Sep 2024 00:49:44 GMT"",
            ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/4422562236827-Filter-the-run-history"",
            ""linkedArticleIds"": [],
            ""numTokens"": 446.0,
            ""score"": 0.842820406,
            ""section_id"": 6306461554075.0,
            ""source"": ""articles"",
            ""title"": ""Filter the run history""
        }
    ],
    ""suggestions"": [
        {
            ""title"": ""Monitor the status column for in-progress or queued flows.""
        },
        {
            ""title"": ""Filter running flows by integration or status to locate specific flows.""
        },
        {
            ""title"": ""Investigate errors or auto-resolved counts to identify and address issues in real-time.""
        }
    ],
    ""thread_id"": ""thread_LySqAmg0j9WG6dc3A3amkzpW"",
    ""vector_duration"": 2.97
}",,,,,,,,,,,,,,,,
Copilot_51,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,Playground_001.json,"{
  ""answer"": ""On the **'Playground - Form builder - Simple form'** page within the Celigo **Form Builder**, your goal is likely to configure or test a custom form field for a URL input. The **Custom URL** field refers to a URL value that can be dynamically or manually entered into the form. Here\u2019s what you need to do:\n\n---\n\n### **Steps to Set a Custom URL:**\n1. **Understand the Purpose of the URL Input:**\n  - If the URL will be used to define an endpoint for an API or integration, ensure you have the correct URL structure from the target application\u2019s documentation.\n  - **Example URL:** `https://api.example.com/v1/resource`.\n\n2. **Set a Custom URL Field in Your JSON Definition:**\n  - Use the JSON schema examples provided in the documentation to define a URL field specifically.\n  - The field type for a URL input is `\""relativeuri\""` or `\""text\""` (depending on how precise the formatting requirements for the URL are).\n\n3. **JSON Definition for a Simple URL Input:**\n  You can include the following JSON snippet in your form to configure the URL:\n  ```json\n  \""customUrl\"": {\n  \""id\"": \""customUrl\"",\n  \""name\"": \""customUrl\"",\n  \""type\"": \""text\"",\n  \""label\"": \""Custom URL\"",\n  \""helpText\"": \""Enter the custom URL for your API or resource.\"",\n  \""required\"": true\n  }\n  ```\n\n4. **Consider Default or Example Values:**\n  If you want to guide users with an example URL or prefill a value, use the `\""defaultValue\""` property:\n  ```json\n  \""defaultValue\"": \""https://example.com/v1/\""\n  ```\n\n5. **Where to Place the Field in the Form (Optional):**\n  Use the `displayAfter` or `layout` property to position the field correctly. For example, to display it after another field, add:\n  ```json\n  \""displayAfter\"": \""anotherFieldId\""\n  ```\n\n---\n\n### **Example JSON for Custom URL Field:**\nHere is a complete configuration for a custom URL field:\n```json\n\""fieldMap\"": {\n \""customUrl\"": {\n \""id\"": \""customUrl\"",\n \""name\"": \""customUrl\"",\n \""type\"": \""text\"",\n \""inputType\"": \""url\"",\n \""label\"": \""Custom URL\"",\n \""helpText\"": \""Enter the custom URL for your API or resource.\"",\n \""required\"": true,\n \""defaultValue\"": \""https://api.example.com/v1/resource\""\n }\n}\n```\n\n---\n\n### **Testing the Custom URL Field:**\n1. Click on the **'Simple form'** tab in the Playground.\n2. Enter your **Custom URL** in the newly visible field.\n3. Validate whether the field recognizes or formats the URL correctly, ensuring it meets the requirements of your integration.\n\n---\n\n**Note:** The exact value you need to set depends on the purpose of the custom URL in your flow (e.g., API endpoint, redirect URL, etc.). If unsure, verify this with the documentation specific to your use case or consult the application\u2019s integration settings. If this explanation does not sufficiently address what you aim to achieve with the custom URL, please provide additional details."",
  ""completion_tokens"": """",
  ""openai_duration"": 22.59,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 360045051311.0,
  ""content"": ""Tip\nSome companies put their documentation on third-party sites. If you can\u2019t find the guide on their website, you can also try a web search.\nSome APIs authorize calls and maintain session information by responding to a cookie request, which is sent back in a set-cookie response header. Cookie authentication is vulnerable to cross-site request forgery (CSRF) attacks and should be used with other security measures, such as CSRF tokens.\nA. Set up an HTTP connection\nStart establishing the universal, or generic, HTTP connection in either of the following ways:\nSelect Connections from the Resources menu.\nNext, click + Create connection at the top right. In the resulting Create source panel, select HTTP from the Application list, under the Universal connectors group.\nWhile working in a new or existing integration , you can add an application to a flow simply by clicking Add source or Add destination/lookup. From the Application list, under Universal connectors, select HTTP.\nAfter the HTTP Application is added, click the Connection setting\u2019s + button to proceed.\nB. Provide general HTTP connection settings\nName (required): Provide a clear and distinguishable name. Throughout integrator.io imports and exports, you will have the option to choose this new connection, and a unique identifier will prove helpful later when selecting among a list of connections that you\u2019ve created.\nApplication (required, non-editable): A reminder of the app you\u2019re editing.\nMode (required): Select one of the following options:\nCloud to connect to a publicly accessible server application\nOn-premise to connect to a server that is publicly inaccessible and has integrator.io agent installed on it\nAgent (required, if On-premise selected for Mode; otherwise not displayed): Select an agent from the list. To connect to an on-premise application, integrator.io requires that an agent be installed on a networked computer. An agent is a small application that allows you to connect to data behind your firewall. When installing an agent, you will specify a unique access token, which then populates the Agent drop-down list. The installed agents connect to integrator.io and establish a reverse SSH tunnel, allowing secure communication without the need to whitelist integrator.io IP addresses in your firewall settings. A single agent can be used by multiple different connections.\nC. Edit cookie auth settings\nContinuing in the Create connection pane, select Cookie for the Authentication type. The settings then become specific to cookie-based connections:\nHTTP method (required): Select GET or POST, depending on the API requirements for making a request for the cookie.\nHTTP request body (enabled and required when HTTP method is POST): Enter the body that will be posted to the URI endpoint. Click the handlebars button () to open the Advanced field editor and create or edit a template. You may reference any fields in your connection or flow's data within handlebars {{placeholders}}.\nAbsolute URL (required): Enter the endpoint that integrator.io will use to make the cookie authorization request. Click the handlebars button () to open the Advanced field editor and create or edit a template. You may reference any fields in your connection or flow's data inside handlebars {{placeholders}}.\nOverride HTTP status code for success (optional): If the application returns any success status code other than 200, enter the expected value.\nOverride HTTP status code for auth errors (optional): Provide an alternate status code if the HTTP status code for auth errors returned by this app is not the standard 401. For example, for an API that returns a generic 400 status code, enter 400 and then specify the field in the HTTP response body that indicates auth errors.\nPath to auth error field in HTTP response body (optional): If the API returns a field that contains auth errors in the HTTP response body, enter the JSON path to that field. For example, when an API returns the field errorMessage with the value Auth failed, then enter errorMessage as the path.\nAuth error values (optional): If you supplied a fail path above, enter the exact values that the API will return to indicate auth errors. Separate multiple values with commas.\nD. Edit common HTTP settings\nAdditional HTTP settings are found in the other sections in the Create connection pane (optional sections are collapsed by default):\nApplication details (contains the required settings Base URI\u2009and Media type)\nNonstandard API rate limiter\nHow to test this connection?\nAdvanced\nFor complete documentation of these settings, see Fundamentals of HTTP connections.\nE. Save, test, and authorize\nOnce you have configured the HTTP connection, you have a few options for continuing:\nSave \u2013 click this button to test the connection, commit the new connection so that it will be available to all integrations for your account\nSave & close \u2013 click to test and save the connection and exit the Create connection pane\nClose \u2013 click to exit without saving any new changes\nTest connection \u2013 click this button to verify that your new connection is free of errors\nWhen you test or save the connection, it is verified before continuing.\nIf the connection fails, double-check the provided settings, and test again.\nSee also\nSet up a cookie-based HTTP connection to Acumatica"",
  ""date"": ""Thu, 17 Apr 2025 10:57:02 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/360045051311-Set-up-a-cookie-based-HTTP-connection"",
  ""label_names"": [],
  ""linkedArticleIds"": [
  ""360048237032""
  ],
  ""numTokens"": 1035.0,
  ""score"": 0.761348784,
  ""section_id"": 20446280977435.0,
  ""source"": ""articles"",
  ""title"": ""Set up a cookie-based HTTP connection""
  },
  {
  ""articleId"": 26265603554331.0,
  ""content"": ""\u200b\u200b Download the Postman schemas\nYou can now build and use a unique HTTP connector to fit your exact needs. This connector will remain private for your use only and will be available in the Celigo platform (visible only to you) after it's created. Currently, you cannot add an image or logo to the connector. To build the connector:\n\u200b\u200b Download the HTTP connector Postman schemas.\nConfigure your schemas.\nCreate the HTTP connector using Postman and the configured schemas.\nThis article goes over some of the concepts around building an HTTP connector.\nAPIs\nAn API is an application program interface exposed so users can interact with data in software. APIs enable the integration between different software products, allowing them to communicate with each other without needing to know the internal workings of each other's software. A connector can have multiple APIs associated with it. For example, Shopify has Partner APIs and Payment APIs. Both are mutually exclusive APIs designed for different purposes. Each API has its own resources and endpoints.\nConnectors\nA connector is a preconfigured form that can connect to a third-party application. The Celigo platform has hundreds of preconfigured application, database, or universal connectors. This Postman collection allows you to create your own HTTP-based connector. A connector creates a link to a software application\u2019s API. Then, you can use any available resources and endpoints to gather, modify, or delete data.\nResources\nA resource typically refers to specific data or information accessible through an API or other integration mechanisms. Resources are the fundamental building blocks you interact with when integrating different systems or applications.\nResources are typically identified using resource identifiers within an API. These are used in API requests to specify which resource you want to interact with. For example, a URL like https://api.example.com/customers/ represents the customer resource, which can have multiple endpoints and methods.\nResources are usually represented in a specific data format, such as JSON or XML, which defines their structure and content when retrieved or sent through an API request.\nEndpoints\nAn endpoint refers to a specific URL or URI (Uniform Resource Identifier) representing an application's unique location or resource. Endpoints are a fundamental concept in API design and usage, and they play a crucial role in defining how data and functionality can be accessed and manipulated.\nEndpoints are typically represented as URLs or URIs. You can use these web addresses to send HTTP requests to interact with the API. For example, an endpoint URL in an application might look like: https://api.example.com/customers or https://api.example.com/products/123.\nEndpoints are not only about where you send requests but also about how you interact with the resources. Different HTTP methods (GET, POST, PUT, DELETE, etc.) are used with endpoints to perform actions like retrieving data, creating new resources, updating existing resources, or deleting resources.\nIn practice, when working with an API, you specify the endpoint URL and use the appropriate HTTP method to interact with the desired resource. For example:\nTo retrieve a list of users, you might send a GET request to the /users endpoint.\nTo create a new user, you might send a POST request to the /users endpoint.\nTo update an existing user's information, you might send a PUT request to the /users/ endpoint, where id represents the specific user's identifier.\nFramework\nA framework is like a bridge that fills the gap between our universal HTTP connector and external HTTP APIs, giving you a streamlined experience for creating connections, imports, and exports in the Celigo platform.\nBefore building a connector, it is essential to understand how to create metadata, preconfigured fields, and fields that a user must set.\nMetadata and preConfiguredFields\nYour metadata refers to data that provides information about other data. In this case, connector metadata consists of preconfigured fields and requirements that remain the same regardless of who created the connection or when it was created. For example, the application\u2019s Base URI (if fixed) or the authentication type (if only one is available) can be set as metadata.\nHere\u2019s a simple exercise to help clarify how connector metadata is structured:\nImagine you\u2019re working with an app that doesn\u2019t have a prebuilt connector. To connect it, you would use a universal HTTP connection. Let\u2019s walk through how two different users, User A and User B, might set up this connection.\nIdentifying preconfigured fields\nUser A successfully establishes the connection using the HTTP form.\nUser B performs the same task by filling in the required fields on the form.\nIf both User A and User B enter the same information in specific fields, these fields are considered preconfigured. You can set these fields up in advance for the connector, so users won\u2019t need to fill them out manually. In other words, you'll configure these values in the connector metadata, and users won\u2019t need to see or edit them.\nExamples of preconfigured fields\nThe app\u2019s Base URI (if it\u2019s fixed for all users)\nThe Authentication Type (if only one type applies)\nfieldsUserMustSet\nSome fields will vary between users. For example, User A and User B may enter different information. You'll want users to fill in these fields when setting up their connection. We call these fieldsUserMustSet.\nExamples of fields a user must set\nA version of the app or API\nCredentials such as a token, username, or password\nSimple vs HTTP form view\nLearn more about a simple vs HTTP form view. Application-specific (simple) forms are based on the universal HTTP connector, including only the most common settings required for integrating with the application. In this view, you\u2019ll see only the fields marked as fieldsUserMustSet. These are the fields where users need to provide specific input because their values can\u2019t be preconfigured. By focusing on these fields, you reduce the cognitive load on users, allowing them to focus only on what\u2019s necessary for the connection.\nThere are two types of fields you will encounter:\nCustom Settings/Encrypted Fields: These fields are specific to the app and often require input that varies from user to user. Connector developers have full control over how these fields are presented and can include helpful text through the metadata to guide users. These fields often capture unique data like credentials or specific configurations needed for the app.\nStandard HTTP Fields: These are the more common fields that users are accustomed to seeing, such as username, password, or token. Most apps follow similar naming conventions for these fields, and if the name is slightly different (like \u201cToken\u201d vs. \u201cAPI Token\u201d), you can reuse the field without changing its label. The goal is to maintain consistency and simplicity across the setup process without confusing the user with minor variations.\nThe HTTP view represents what a user would manually configure using a universal HTTP connection. It is automatically generated based on the provided metadata, including Preconfigured fields and fieldsUserMustSet. The HTTP view is designed for advanced users who may need to make custom modifications. Any changes made here may affect compatibility with the simple view.\nThe BaseURI is the key identifier for this form. If an advanced user changes the configuration, including the BaseURI, the Celigo platform will automatically match it to the correct connector. For example, if the BaseURI on Narvar\u2019s HTTP view is changed to match UPS\u2019s BaseURI, the Celigo platform automatically maps the connection to UPS.\nCustom settings\nCustom settings and common form fields are used to create fields a user can input into. Sometimes, only part of a field will vary between users. You can typically use custom settings in the following ways:\nWhen the corresponding HTTP field has a partial value fixed (like the Base URI or Authentication URLs for OAuth); for example, in a Shopify connector, the base URL might look like this: https://www.{{{storename}}}.myshopify.com. In this case, the part that varies is the storename. Each user must enter their unique store name to complete the URL. You can handle this by creating custom settings, allowing users to enter the information unique to their situation.\nWhen the corresponding HTTP field cannot be presented as is, for example, the value for the License ID needs to be passed as an HTTP header. Asking a user to provide the License ID in the header's key-value format would not be pleasant. In this case, we can create a custom setting and assign its value to the user.\nWhen NOT to use custom settings\nThere are some situations where you wouldn\u2019t want to use custom settings. Typically, authentication-related fields would not be ideal for custom fields. If the underlying application follows a standard authentication mechanism the Celigo platform already supports, you should reuse the available field. For example, the token authentication type is already supported and can be used as a fieldsUserMustSet field.\nEncrypted fields\nThis is an alternative to custom settings and should be used only if you want to save the user\u2019s input in an encrypted manner. It should be used only in cases where the authentication type is custom. In the future, you should change this to a different custom setting."",
  ""date"": ""Thu, 06 Feb 2025 01:27:00 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/26265603554331-Concepts-Build-an-HTTP-connector"",
  ""label_names"": [],
  ""linkedArticleIds"": [
  ""25993508710171"",
  ""25988478035483"",
  ""4409527888923"",
  ""10841316566043"",
  ""360045461272"",
  ""360059205112""
  ],
  ""numTokens"": 1823.0,
  ""score"": 0.759269893,
  ""section_id"": 360007476271.0,
  ""source"": ""articles"",
  ""title"": ""Concepts: Build an HTTP connector""
  },
  {
  ""articleId"": 37129634529691.0,
  ""content"": ""After you set up a connection to Workday, you can create an import and configure these settings.\nSettings\nInstructions\nGeneral\nName your import \u2022\nProvide a clear and distinguishable name. You will have the option to choose this import throughout integrator.io, and a unique identifier will prove helpful later when selecting among a list of imports that you\u2019ve created.\nDescription\nDescribe your import so that you and others can quickly understand its purpose. Be sure to highlight any nuances that a user should be aware of before using this import in a flow. As you make changes to the resource, be sure to keep this description up to date.\nConnection \u2022\nSelect an existing Workday connection. If you want to create a new connection, see Set up a connection to Workday..\nOne to many\nWhen importing records, the default for One to many is No. If you've got a scenario where you want to use\u00a0One to many, that is, you have a single record that internally needs to create multiple records, see Create a one to many import including nested arrays\nHow would you like the records imported?\nChoose type \u2022\nNote\nWhen you create the SQL query, it must include Service=\"" \"" and Version=\"" \"", where Service represents the Workday Web Service and Version represents the Workday API version. For example, see the highlighted text in the image below.\nUse SQL query once per record: This option executes the SQL query once for each record. It allows the query to dynamically reference data fields from a specific records, such as if you need to retrieve employee details based on EmployeeId; or, if you want to add a new company location to your database (see stored procedure call in example below).\nExample\nEXEC ExecuteSOAPOperation Service='Human_Resources', Request='  Test_Location_Celigo Campus  '\nUse SQL query once per page of records : This option executes the SQL query once for each page of records during the import process, rather than individually for every record. It allows the query to operate on a set of records collectively, improving performance and reducing the number of query executions.\nExample\nEXEC ExecuteSOAPOperation Service='Human_Resources', Request='{{{batch_of_records.0.record.Location_Name}}}{{{batch_of_records.0.record.Location_Usage_ID}}}'\nTip\nIf you're familiar with SQL, enter your query in the box.\nIf you would like to use handlebars, click to open the Handlebars editor and create your SQL query.\nAfter you configure the required settings, click Save.\nConfigure optional settings\nThese import settings are optional, and typically don't have to be configured. However, if required in your scenario, you can configure these settings.\nSettings\nInstructions\nMock output\nMock output\nSee Provide mock response data for imports\nAdvanced\nConcurrency ID lock template\nEnter a handlebars template that generates a unique id for each imported record. integrator.io uses the raw imported data to generate the IDs (before any import or mapping logic is invoked), and uses the ID to verify that no two records with the same ID are submitted for import at the same time. For example, if you are exporting Zendesk records and importing them into NetSuite, then you would most likely use\u00a0{{id}}\u00a0(the field Zendesk uses to identify unique records). In this case, no two records with the same Zendesk id value would import into NetSuite at the same time.\nThis setting prevents integrator.io from sending duplicate records simultaneously if the connection associated with this import is using a concurrency level greater than 1.\nThe connection associated with your import can be configured to limit the number of concurrent requests that can be made at any one time. If you are allowing more than 1 request at a time then it is possible for imports to overlap each other if multiple messages/updates for the same record are being processed at the same time. This settings allows you to enforce an ordering across concurrent requests such that imports for a specific record id will queue up and be processed one at a time (while still allowing imports for different record ids to process in parallel).\nData URI template\nWhen your flow runs but has data errors this field can be really helpful in that it allows you to make sure that all the errors in your job dashboard have a link to the target data in the import application (where possible).\nUse a handlebars template to generate the dynamic links based on the data being imported. The template you provide will run against your data after it has been mapped, and then again after it has been submitted to the import application, to maximize the ability to link to the right place. For example, if you are updating a customer record in Shopify, you would most likely set this field to the following value\u00a0https://your-store.myshopify.com/admin/customers/{{{id}}}.\nInvoke\nCopy the URL if you want to invoke this resource via an HTTP request."",
  ""date"": ""Tue, 15 Jul 2025 05:33:02 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/37129634529691-Import-data-into-Workday"",
  ""label_names"": [
  ""import data"",
  ""Workday""
  ],
  ""linkedArticleIds"": [
  ""37129640444955"",
  ""226933887"",
  ""37129640444955"",
  ""12581994146843"",
  ""360043040652"",
  ""7267948784539"",
  ""360039326071""
  ],
  ""numTokens"": 991.0,
  ""score"": 0.758130193,
  ""section_id"": 8148561387291.0,
  ""source"": ""articles"",
  ""title"": ""Import data into Workday""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Define a JSON field for \""customUrl\"" in the Simple Form builder and save the configuration.""
  },
  {
  ""title"": ""Test the custom URL field in the form to ensure it meets integration requirements.""
  },
  {
  ""title"": ""Verify if the entered Custom URL works as an API endpoint in your integration scenario.""
  }
  ],
  ""thread_id"": ""thread_YOeBD0CqrwnMS0LoaSOjdj3r"",
  ""vector_duration"": 2.3
 }",,,,,,,,,,,,,,,,
Copilot_52,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,AuditLogs_001.json,"{
  ""answer"": ""The **Audit log** page contains detailed records of specific activities and changes made to your flows, integrations, and account within Celigo's integrator.io platform. These logs provide important information about the **who, what, when, and how** of actions performed, which can help monitor changes, troubleshoot errors, and maintain accountability.\n\nHere\u2019s what the Audit log contains:\n\n### Key Parameters Recorded in the Audit Log:\n1. **Time**: The timestamp of when the action occurred. The format can be relative (e.g., \""3 hours ago\"") or absolute, depending on your account settings.\n2. **User**: Identifies the user responsible for making the changes. If the account is shared with the same credentials, it will display \""Account Owner\"" or the company name.\n3. **Source**: Shows how the change was made:\n  - **UI**: User interface\n  - **API**: Using the API\n  - **Stack**: Via stack changes\n  - **System**: By backend/system processes\n  - **Integration app**: By an integration app\n4. **Resource Type**: Identifies which resource was updated (e.g., Flow, Import, Export, Script, Connection, etc.).\n5. **Resource Name**: The name or ID of the resource that was modified. If the resource is deleted, it will only display its original ID.\n6. **Action**: The type of action taken, such as:\n  - **Create**\n  - **Update**\n  - **Delete**\n  - **Sign in**\n  - **Sign out**\n  - **View**\n  - **Restore**\n  - **Purge**\n7. **Field**: Specifies the field that was changed (e.g., connection details, pageProcessors/Imports, flow names).\n8. **Old Values/New Values**: Displays the values before and after the change. You can use the \""Click to view edits\"" feature to review side-by-side details of the modifications.\n\n### Practical Uses:\n- **Error Troubleshooting**: Identify changes that might have caused errors in your flows or integrations.\n- **Change Tracking**: Determine who updated fields, changed account permissions, or modified integration settings.\n- **Security Monitoring**: Understand how sensitive resources, like API tokens or connections, were accessed or altered.\n\n### Additional Notes:\n- You can filter the logs to narrow results by **resource type**, **user**, **source**, and/or **action**.\n- Audit logs are retained for a minimum of one year but downloading them is limited to 20,000 records at a time.\n- You can export logs in customizable date/time ranges or retain logs longer by upgrading your subscription.\n\nLet me know if you need help with anything specific on this page!"",
  ""completion_tokens"": """",
  ""openai_duration"": 13.26,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 6514515710107.0,
  ""content"": ""Audit logs maintain a record of specific activities in your flows, including the fields that were changed, the type of change, and who\u2019s responsible for the change. You can view your audit logs at a flow or integration level.\nImportant\nIn audit logs, pageProcessors and pageGenerators represent imports and exports, respectively. It\u2019s just a system naming convention that doesn\u2019t affect your integrations or flows.\nAudit logs\nAccount audit logs\nIntegration audit logs\nThese logs show high-level data for all your logs. To narrow down the information to integration audit logs, use Select resource type, user, and source.\nDownload audit logs\nintegrator.io keeps audit logs for a minimum of a year. For example, if the current date is Jan 3, 2023, then you can view audit logs from Jan 3, 2022, to Jan 3, 2023. If you download logs to a CSV file or use the audit API endpoint , you can download up to 20,000 records simultaneously. If, for example, you\u2019ve generated 30,000 records in the last 24 hours, you can download the records in parts by adjusting the Custom date/time range. You can also filter the logs by resource type , user, source, or action and then download the filtered list of records.\nTo retain audit logs for a longer period, you can upgrade your subscription plan.\nPractical uses for audit logs\nIt\u2019s easy to review audit logs in a vacuum. However, you can use your audit logs to troubleshoot flow errors and pinpoint changes that created issues with your integration or flow. You can also determine who changed account permissions. For example, if someone changed an import field and caused an error, you can use audit logs to review the changes. Then, use Integration Lifecycle Management to revert your changes.\nParameters\nAudit logs record every action or change you\u2019ve made to your flow or integration. The logs record nine (9) parameters for each change:\nTime\nYou can configure your account profile to display timestamps in any date/time format. If you\u2019ve set your account to relative time, the timestamp is displayed as the length of time between the started date and time and the current date and time (1 day ago, 3 hours ago).\nUser\nThe user responsible for the changes. In cases where the account is shared by multiple people with the same password, the only username is something similar to \u201cAccount Owner\u201d or your company name.\nTip\nIf you\u2019re an account owner or administrator you can invite new users (or remove old users) to your account.\nSource\nDetermines how the change was made. This could be:\nUI \u2013 Changes were made from the user interface.\nAPI \u2013 Changes were made in the API.\nStack \u2013 Changes were made from the stack.\nSystem \u2013 Any change made to a resource by integrator.io backend logic or a system process.\nIntegration app \u2013 Changes were made by the integration app\nResource Type\nShows you the resource that was updated. There are nine resources monitored for changes:\nResource Type\nDescription\nAPI\nLogs your API-level updates.\nIntegration\nLogs your integration-level updates. This includes name changes.\nFlow\nLogs your flow-level updates. In integration-level audit logs, this is displayed as an update to the flow, with a specific field name (pageProcessors/Imports or pageGenerators/Exports) and changes available to view.\nExport\nLogs your export-level updates. This includes name, operation, and other export settings changes.\nImport\nLogs your import-level updates. This includes name, operation, and other import settings changes.\nConnection\nLogs your connection-level updates. This includes name and authentication changes.\nLookup caches\nLogs changes to name, description, and any downloads or cloning of lookup caches.\nStack\nLogs your changes to a stack.\nAsync helper\nIf certain data is unavailable, an async helper has the app\u2019s API ping integrator.io when the info is available. The flow will be paused until the data is received. This logs all modifications to your async helpers.\nFile definitions\nLogs all modifications to the rules that define how integrator.io should parse or generate EDI or fixed-width files.\nRevision\nLogs all your version management revisions, including clones, pulls, merges, and snapshots.\nScripts\nLogs all modifications to your scripts.\nNotification\nLogs all notifications when a resource is created or deleted.\nThere are also account-specific resources, including user status changes, deletions by each user and IP address, agent changes, and single sign-on logs.\nTo find these resources, click your profile at the top, right-hand side of the screen \u2192 click My Account \u2192 and click Audit log.\nAccount-specific Resource type\nDescription\nUser\nLogs account, permission, IP address, notification, and updates by user.\nAgent\nLogs modifications to your secure on-premise or cloud tunnels to integrator.io.\nSSO client\nLogs all changes to your single sign-on configurations. integrator.io does not display your client secret. Multiple layers of protection are in place, including AES 256 encryption, to keep your client secret safe. When editing this form later, you must enter this value again.\nAPI tokens\nLogs all API token modifications. integrator.io does not display your API tokens. Multiple layers of protection are in place, including AES 256 encryption, to keep your API token safe. When editing this form later, you must enter this value again.\nAPIs\nLogs all modifications to build APIs using API builder and JavaScript APIs.\nResource Name\nThe name or identifier of your updated resource. Resources with IDs instead of names have been deleted, which is why they\u2019re only available by the ID they were given when they were created.\nAction\nThe action that was taken. Actions include:\nCreate\nUpdate\nDelete\nSign in\nSign out\nView\nRestore\nPurge\nThe most common action is Update, while Create and Delete are reserved for creating/deleting a flow or another significant event. The Sign in and Sign out actions are reserved for the User resource.\nField\nThe Field column gives you an idea of what\u2019s been changed. It can display things like pageProcessors, connections, and names.\nOld and new values\nThe field values before and after the change.\nIn the Old and New value fields, you have options including Click to view edits, dates and times a token was updated, and secure connection details. You should know that we won\u2019t display your secure connection details, like keys, tokens, or passwords, in audit logs under any circumstances, as those details are encoded.\nWhen you use Click to view, old and new edits are displayed side-by-side in a popup diff screen. This is a feature of the flow audit log that\u2019s also available in the integration logs. It drills down and shows you the exact changes you made, including any mapping changes."",
  ""date"": ""Thu, 20 Mar 2025 18:21:34 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/6514515710107-View-or-download-your-audit-logs"",
  ""label_names"": [
  ""Audit Logs"",
  ""audit log dashboard"",
  ""audit log"",
  ""audit log tab"",
  ""audit logs"",
  ""where to find audit logs"",
  ""audit log parameters"",
  ""download audit logs"",
  ""help logs"",
  ""logs"",
  ""where are audit logs"",
  ""how to use audit logs"",
  ""audit"",
  ""audit data""
  ],
  ""linkedArticleIds"": [
  ""360042281231"",
  ""227054148"",
  ""360053966912"",
  ""29494547815579"",
  ""360025642872"",
  ""360025919171"",
  ""226934107"",
  ""226933887"",
  ""227055868"",
  ""360004872932"",
  ""6685420081307"",
  ""6685876051099"",
  ""6767143777179"",
  ""6682560246299"",
  ""115003929872"",
  ""6851018128795"",
  ""360060937871"",
  ""360019782431"",
  ""29494547815579"",
  ""360047267771""
  ],
  ""numTokens"": 1385.0,
  ""score"": 0.829981,
  ""section_id"": 24220435887003.0,
  ""source"": ""articles"",
  ""title"": ""View or download your audit logs""
  },
  {
  ""articleId"": 33739081475099.0,
  ""content"": ""Viewing request logs is essential for monitoring, debugging, and optimizing API performance. Logs provide a detailed record of incoming requests, pre-mapped responses, and responses. By analyzing these logs, you can identify bottlenecks, unauthorized access attempts, and failed API calls, improving security and reliability.\nOpen and view your request logs\nTo open your API's request log from the API builder list:\nClick Resources \u2192 APIs.\nNavigate to the API builder tab.\nFind your API in the list and click Actions (...) \u2192 View request logs.\nTo open your API's request log from API builder:\nClick Resources \u2192 APIs \u2192 API builder \u2192 .\nClick Actions overflow (...) \u2192 View request logs.\nNavigate your request logs\nAfter opening your request logs, you'll see a list of all requests on the left side. These requests can be sorted by timestamp, method, or response code. The Actions (...) menu allows you to delete a request log.\nLog data is displayed on the right side and separated into API request, Pre-mapped response, and API response data tabs.\nYou can refresh the data to fetch the latest API requests by clicking Refresh \u200b. You will not see any request log entries without request history for the filtered criteria. Old request logs are purged based on your data retention policy (30, 60, 90, or 180 days).\nAll the data in your API logs can be copied using the Copy \u200b\u200b button.\nAPI request\nThis tab shows the API request's body, headers, and Other data (e.g., request URL, method, and requestSize). This is the API request as your API consumer sent it.\nPre-mapped response\nThis tab lets you view the records before they're mapped in the API response. This includes the source records as they're sent, execution errors, and configuration errors.\nCheck the API request log execution errors to troubleshoot issues while keeping error details hidden from API consumers. Learn more about creating custom errors.\nConfiguration errors display if your API setup is incomplete\u2014such as missing a Relative URI, HTTP method, status code in response nodes, or an unmerged branch. In these cases, you\u2019ll see a 501 (Not Implemented) error.\nExecution error\nConfiguration error\nAPI response\nThis is the API response sent to your API consumer. This tab shows the API response's body, headers, and Other data (e.g., statusCode, responseSize, and duration)."",
  ""date"": ""Tue, 20 May 2025 16:31:28 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/33739081475099-Request-logs-in-API-Builder"",
  ""label_names"": [],
  ""linkedArticleIds"": [
  ""6359380074779"",
  ""33739081260443""
  ],
  ""numTokens"": 485.0,
  ""score"": 0.821288586,
  ""section_id"": 29494557647259.0,
  ""source"": ""articles"",
  ""title"": ""Request logs in API Builder""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Filter the audit logs by resource type, user, or action to narrow down relevant entries.""
  },
  {
  ""title"": ""View \""Old and New\"" values for a specific record to identify changes or errors.""
  },
  {
  ""title"": ""Download audit logs as a CSV file for deeper analysis or long-term storage.""
  }
  ],
  ""thread_id"": ""thread_Zuc9Y4W8SqUHbXyMMnlHWZVT"",
  ""vector_duration"": 2.48
 }",,,,,,,,,,,,,,,,
Copilot_53,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,MyAccount_profile_001.json,"{
  ""answer"": ""Based on the documentation and your current page, \""Profile - Profile,\"" it seems you are viewing your user profile and likely managing API subscriptions or exploring available APIs.\n\nHere are the subscription options available to you, based on the documentation:\n\n### 1. **Subscribe to an API**\nTo subscribe to an API, you must have an **application** created first. Follow these steps for new subscriptions:\n  - Navigate to the **API Catalog**.\n  - Search for the desired API or browse categories like featured, starred, or trending APIs.\n  - Select your API and click **Subscribe** in the top right corner.\n  - Choose a **Plan** to subscribe to:\n  - Plans include details like version, publish date, access URLs, and which of your applications are already subscribed.\n  - Plans may vary in their rate limits, authentication methods (e.g., API Key, OAuth 2.0, JWT), quotas, or restrictions.\n  - Choose the **Application** you want to subscribe to.\n  - Optionally, send a message to the API developer with your subscription request.\n  - Review and validate your subscription request.\n  - Confirm the subscription.\n\n### 2. **Plan Types When Subscribing**\n  Celigo offers multiple subscription plan types:\n  - **API Key Plans**:\n  - Secure an approved API key and authenticate requests.\n  - Limited to specific applications using the same API key.\n  - **OAuth 2.0 Plans**:\n  - Use tokens instead of credentials for secure delegated access. Requires configuration with an Identity and Access Management (IAM) system.\n  - **JWT (JSON Web Token) Plans**:\n  - Use JWTs verified by RSA public/private keys for secure access.\n  - **Keyless Plans**:\n  - No authentication is required, ideal for exposing APIs for open feedback or experimentation.\n  - **Tiered Plans**:\n  - Include additional controls like rate limiting, quotas, restricted resources, or upgrade policies.\n\n### 3. **Common Workflow for Manual Approvals**\n  If your chosen API requires **manual approval**:\n  - API publishers must **accept your subscription** before you can access the API.\n  - They may specify a start and end date for your subscription.\n\n### 4. **Possible Subscription Restrictions**\n  - You can only subscribe **one application at a time** to a particular API with the same plan and authentication type. Otherwise, you\u2019ll encounter an error.\n  - Subscriptions are a **one-to-one relationship** between an API and application.\n\n### To Unsubscribe:\n  If you want to **cancel a subscription**:\n  - Go to your application\u2019s **Dashboard** or the **Applications page** \u2192 **Subscriptions tab**.\n  - Select the subscription and choose **Close** to unsubscribe.\n\nThese options depend on your existing application, available APIs in the catalog, and the plans they offer. Let me know if you'd like further clarification or instructions on navigating the process!"",
  ""completion_tokens"": """",
  ""openai_duration"": 16.54,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 20110543786523.0,
  ""content"": ""This article shows you how to subscribe to existing APIs quickly. You should be familiar with plans, applications, and subscriptions before starting. This article is for API consumers who want to use APIs created by a company. If you're an API publisher who wants to create and publish APIs using integrator.io, see Push a resource to the APIM console.\nSubscribe to an API\nYou can subscribe to an API as long as you have an application. The subscription request will be sent to the API owner. To see your subscriptions, navigate to your Developer portal \u2192 Dashboard or a specific application. To subscribe to an API using an existing application:\nNavigate to the API catalog.\nSearch for your API or browse available categories, featured, starred, or trending APIs.\nClick your desired API and click Subscribe at the top right-hand side of the page.\nChoose the plan you want to subscribe to. You can see the version, publish date, Access URL, and which applications (if any) are already subscribed to this API.\nSearch for and choose the application you want the API to subscribe to. The system will warn you of any applications already subscribed to this API. You can send a message to the API developer along with your request.\nReview your choices and validate your request.\nConfirm that you\u2019ve subscribed to the application.\nYou may encounter a common error. See the warnings below to mitigate this error:\n\u201cAn error occurred during the subscription. Another OAuth2 or JWT plan is already subscribed by the same Application.\u201d\nWarning\nYour subscription will fail if you have two applications trying to subscribe to the same API with the same tier plan and authentication.\nSubscriptions between an authenticated API and an application are one-to-one. If one application (App 1) is subscribed to an API with Tier 1 authentication (JWT), no other application can subscribe to the same plan. However, a second application (App 2) can subscribe to the same API with a different authentication plan (Tier 2 authentication with API key).\nYou can subscribe to two different APIs with the same authentication type.\nUnsubscribe from an API\nTo cancel a subscription:\nNavigate to your application from the Dashboard or the Applications button.\nIn the navigation bar, click Subscriptions.\nClick on your subscription and click Close on the right-hand side of the screen.\nConfirm that you want to close your subscription."",
  ""date"": ""Thu, 13 Mar 2025 18:29:29 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/20110543786523-Subscribe-to-an-API"",
  ""label_names"": [],
  ""linkedArticleIds"": [
  ""20105899693083""
  ],
  ""numTokens"": 470.0,
  ""score"": 0.807762682,
  ""section_id"": 21179001937307.0,
  ""source"": ""articles"",
  ""title"": ""Subscribe to an API""
  },
  {
  ""articleId"": 21205243031067.0,
  ""content"": ""Before you begin, learn more about API management. You should also be familiar with\u00a0plans, applications, and subscriptions.\nThis guide is for API publishers\u00a0who create and manage plans, as well as approve subscriptions. To subscribe to and use a published API, see\u00a0API consumers.\nCreate your API plans\nA default API plan is created when you publish the API through integrator.io. However, you can create other tiered plans to manage options like rate limiting and quotas. You can also manually customize your plan using the\u00a0Design\u00a0feature.\u00a0Learn more about plans.\nNavigate to your API \u2192 Plans.\nClick + to create a new plan.\nSet your Authentication type.\nOAuth2\nNote\nOAuth2 can only be enabled using Identity and Access Management software (IAM).\nJWT\nNote\nJWT only supports the RSA Public Key format.\nAPI keys: You use API key plans to enforce verification of API keys during request processing, allowing only apps with approved API keys to access your APIs. This plan type ensures that API keys are valid, are not revoked or expired, and are approved to consume the specific resources associated with your API. API key plans offer only a basic level of security, acting more as a unique identifier than a security token, since the API key can easily be found in the app code. For a higher level of security, see OAuth 2.0 and JWT plans.\nThe API key header is x-celigo-api-key. You can find it at Settings menu \u2192 Settings page \u2192 Portal section.\nKeyless plans: Keyless plans allow public access to the API and bypass any security mechanisms on the whole request process. By default, keyless plans offer no security and are most useful for quickly and easily exposing your API to external users and getting their feedback. Due to the lack of a consumer identifier token (API key), keyless consumers are set as\u00a0unknown application\u00a0in the API analytics section.\nCheck scopes: Check the required scopes to access the resource. You can add scopes as needed.\nRate limiting: Limit how many HTTP requests an application can make in a given period of seconds or minutes.\nQuota: Limits how many HTTP requests an application can make in a given period of hours, days, or months.\nResource filtering: Restrict resources according to whitelist and/or blacklist rules.\nPublish your API plan\nBefore anyone can subscribe to your API plan, you'll need to publish it. Don\u2019t forget to deploy your plan first to send the latest changes. To publish:\nNavigate to Plans \u2192 Staging.\nClick the cloud with the arrow to publish your plan.\nConfirm that you want your plan to be published.\nDeprecate your plan\nTo deprecate your plan, click\u00a0Deprecate\u00a0(cloud with an X) and confirm that you want to deprecate the plan.\nCaution\nA deprecated plan is no longer available on the Developer Portal, and new subscriptions to the plan cannot be created. Existing subscriptions are maintained.\nManage applications\nYour applications allow you to consume your APIs. Generally, your API consumers would create an application in the API developer portal.\nConfigure your application types\nBefore you or your users can create an application, you must configure your allowed application types.\nNavigate to Settings \u2192 Client registration.\nConfigure the Allowed application types.\nManage subscriptions\nWhen you create new plans, you can specify auto-validation of subscriptions so API consumers can access the API as soon as they subscribe to the plan. If you use manual approval, you must manually approve all subscriptions.\nApprove a subscription\nWhen publishers create new plans, they can specify auto-validation of subscriptions so consumers can access the API as soon as they subscribe to the plan. If you set manual approval on a plan; however, you must approve subscriptions by following these steps.\nNavigate to your API and click Portal \u2192 Subscriptions.\nSelect the Pending subscription.\nClick Accept, then enter the start and end dates of subscription approval (no end date means forever).\nRevoke a subscription\nYou can revoke a subscription to remove access to APIs.\nNavigate to your API and click Portal \u2192 Subscriptions.\nSelect the subscription you want to revoke and click Close."",
  ""date"": ""Wed, 11 Jun 2025 18:42:07 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/21205243031067-Create-and-manage-API-plans-applications-and-subscriptions"",
  ""label_names"": [],
  ""linkedArticleIds"": [
  ""21179125401755"",
  ""21179125401755"",
  ""21179125401755"",
  ""21179125401755"",
  ""21179125401755""
  ],
  ""numTokens"": 833.0,
  ""score"": 0.80351764,
  ""section_id"": 20103818642843.0,
  ""source"": ""articles"",
  ""title"": ""Create and manage API plans, applications, and subscriptions""
  },
  {
  ""articleId"": 360039234232.0,
  ""content"": ""You can customize additional features in your Integration App with an add-on. Add-ons let you choose specific use-cases based on your business needs.\nAdd-on features\nPre-built and managed.\nSpecific to a single store. To leverage the add-on functionality for multiple stores, you have to purchase additional licenses.\nSupported on all the editions of the integration app - starter (previously \""standard\""),\u00a0 standard (previously \""premium\""), and premium (previously \""enterprise\"").\nUser interface components, like settings and field mappings, are similar to your integration app's UI components.\nFind add-ons in your integration app\nIn your app, you can find your add-ons on the Add-on page. The add-ons there will be specific to your app's endpoints.\nGet a license for your add-on\nAdd-ons require a separate subscription license similar to your integration app subscription. The add-on license is confined to a single store and expires with your integration app subscription.\nNotes:\nThe add-on subscription works only if the integration app base subscription is active.\nIf you uninstall an add-on, the same license can be used for another store (only if you have multiple stores for an integration app) or any other store of a particular tile.\nThe add-on subscription can be renewed along with the integration app subscription.\nIf the base subscription of the integration app is renewed, the add-on subscription is not renewed automatically.\nUse NetSuite and external app connections\nThe NetSuite and external app connection that you use for the integration app is also applicable for add-ons.\nInstall an add-on\nPrerequisite: Ensure that you have a valid subscription license for the add-on. If you do not have a license, go to the Add-Ons section and click Contact Sales.\nIn integrator.io, click on the appropriate\u00a0integration app tile.\nGo to Admin > Subscription > Add-ons.\nNext to the appropriate add-on, click Install.\nAfter a successful installation, in the Flows tab, you will find a new sub-section."",
  ""date"": ""Thu, 02 Mar 2023 07:15:09 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/360039234232-Understand-integration-app-add-ons"",
  ""linkedArticleIds"": [],
  ""numTokens"": 401.0,
  ""score"": 0.792078137,
  ""section_id"": 360012438171.0,
  ""source"": ""articles"",
  ""title"": ""Understand integration app add-ons""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Navigate to the API Catalog to explore available APIs and subscription plans.""
  },
  {
  ""title"": ""Verify your applications under the Developer portal to confirm eligibility for API subscriptions.""
  },
  {
  ""title"": ""Check subscription details, plans, or current active subscriptions in your Profile or Dashboard section.""
  }
  ],
  ""thread_id"": ""thread_2TKRfPCkBvxzX6Y52Q21dche"",
  ""vector_duration"": 1.56
 }",,,,,,,,,,,,,,,,
Copilot_54,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,FlowBulder_AddSource_001.json,"{
  ""answer"": ""Based on your current context on the \""Flow builder - Add Source\"" page and the provided documentation, here is what you should do to fetch MongoDB data:\n\n1. **Select MongoDB as the source application**:\n  - On the \""Flow builder - Add Source\"" page, click **Add source**.\n  - From the options presented in the \""Create Export\"" panel, select **MongoDB** as your source application.\n\n2. **Provide or create a MongoDB connection**:\n  - If you already have a MongoDB connection, select it from the list.\n  - If not, click the **+ button** to establish a new connection. Provide the required details:\n  - **Mode**: Choose between Cloud or On-premise.\n  - **Host(s)**: Specify the hostname or IP address of your MongoDB instance.\n  - **Database**: Enter your database name (e.g., `orders`, `products`, etc.).\n  - **Authentication credentials** (Username & Password): Add the credentials for the database.\n  - Test the connection to ensure it works, then save it.\n\n3. **Describe the export**:\n  - After selecting the MongoDB connection, you'll need to describe the export:\n  - **Name**: Add a name like \""Fetch orders from MongoDB\"".\n  - **Description** (optional): Add a brief description for clarity.\n  - **Collection**: Specify the MongoDB collection you want to fetch data from (for example, `orders`, `items`, etc.).\n  - **Filter (optional)**: If you only want specific data, provide a filter formatted as JSON (e.g., `{\""Name\"": \""John Doe\""}`).\n  - **Projection (optional)**: Define specific fields to fetch from the collection if you don\u2019t need all. Example: `{\""Name\"": true, \""Amount\"": true}`.\n  - Click **Next** to proceed.\n\n4. **Select the export type**:\n  - Choose one of the options for how you want to fetch the data:\n  - **All**: Export all records from the collection.\n  - **Delta**: Fetch records based on a Date field (e.g., recently updated records).\n  - **Once**: Only fetch records that have not been exported yet.\n  - **Limit**: Export a specific number of records (useful for testing).\n\n5. After setting up these parameters, the MongoDB data will be configured as the **source** for your flow.\n\nBy following these steps, you'll configure the flow to fetch MongoDB data. Let me know if you need additional help!"",
  ""completion_tokens"": """",
  ""openai_duration"": 30.26,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 360011884271.0,
  ""content"": ""Before you can import\nThere are a couple of things you'll need to understand and have in order to be able import your data.\nEstablish your connections\nYou will need to set up an FTP and MongoDB connection before you can configure your import. Make sure you understand that process.\nPut your data in the right format\nTo to pull your FTP data into MongoDB, it will need to be in a specific format.\nSample Data\n{\u00a0 \u00a0\""orderID\"": \""TAG-Address-1\"",\u00a0 \u00a0\""FirstName\"": \""John\"",\u00a0 \u00a0\""LastName\"": \""Miller\"",\u00a0 \u00a0\""ItemName\"": \""ACC00002\"",\u00a0 \u00a0\""Quantity\"": \""100\"",\u00a0 \u00a0\""Amount\"": \""280\"",\u00a0 \u00a0\""Notes\"": \""My notes here\"",\u00a0 \u00a0\""units\"": \""100\"",\u00a0 \u00a0\""PO#\"": \""656\"",\u00a0 \u00a0\""Date\"": \""07/15/2017\"",\u00a0 \u00a0\""id\"": \""1\"" }\nImport data into MongoDB\n1. Enter appropriate details in the following fields.\nField Name\nDescription\nName\nEnter a name for the MongoDB import that you are about to create. For example, MongoDB Import.\nDescription\nEnter a brief description of what MongoDB is importing.\nApplication\nSelect MongoDB.\nConnection\nSelect the MongoDB connection that you have already created.\nMethod\nSelect the method to use for adding or updating documents in your MongoDB instance.\nFor example, you can select Insert Many as you may want to import all the data that is coming from the FTP export.\nFor more information on the available methods please refer to the MongoDB documentation.\nCollection\nEnter the MongoDB collection name into which you want to import the export data. For example, MyCollection.\n2. Click Next.\n3. On the Map page, click Launch Field Mapper.\n4. On the MongoDB Data Builder window,\nUnder \""Template\"": You can specify the fields that you want to map based on the Sample Data received.\nUnder \""Preview\"": You can see the data that is mapped.\n5. Click Save."",
  ""date"": ""Mon, 27 Jun 2022 20:38:09 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/360011884271-Import-FTP-data-into-MongoDB"",
  ""linkedArticleIds"": [],
  ""numTokens"": 417.0,
  ""score"": 0.80104655,
  ""section_id"": 360007898172.0,
  ""source"": ""articles"",
  ""title"": ""Import FTP data into MongoDB""
  },
  {
  ""articleId"": 360044796071.0,
  ""content"": ""To begin exporting data from MongoDB, choose one of the following options:\nAttach a new export within a flow\u00a0 \u00a0 \u2013 or \u2013\nCreate a standalone export (for later use in one or more flows)\nCreate an export\nAttach a new export within a flow\nSelect Flow Builder from the Tools menu. A blank flow opens with empty source and destination/lookup applications.\nClick Add source. The Create export panel opens.\nFor the source Application, select MongoDB.\nNext, you\u2019re asked to provide a MongoDB connection. Either select an existing MongoDB connection from the list, or click\u00a0the + button to establish a new\u00a0MongoDB connection.\nClick Next. (Continue with Describe the export.)\nCreate a standalone export\nSelect Exports from the Resources menu. A list of all exports in your account opens.\nIn the resulting Exports page, click + Create export.\u00a0The Create export pane opens.\nFor the source Application, select MongoDB.\nNext, you\u2019re asked to provide a MongoDB connection. Either select an existing MongoDB connection from the list, or click\u00a0the + button to establish a new\u00a0MongoDB connection.\nClick Next.\nDescribe the export\nName (required): Name your export so that you can easily reference it from other integrator.io resources.\nDescription (optional): Describe your export so that others can quickly understand what it is doing without having to read through all the fields and settings. Be sure to highlight any nuances that someone should be aware of before using your export in their flows. Also, as you make changes to the export, be sure to update this field.\nCollection (required): Enter the name of the MongoDB collection in your database that you will query, such as orders, items, users, or customers.\nFilter (optional): To export only specified documents from your collection, enter a filter for this setting. The value must be a valid JSON-formatted string describing a MongoDB filter object \u2013 in the correct format and with the correct operators. Refer to MongoDB documentation for the list of valid query operators and the correct filter object syntax. For example:\n{\""Name\"":\""MongoDBRecord1\""}\nWarning: When filtering the record based on the _id field, the Id must be written as follows, so integrator.io processes it as a MongoDB Id rather than a string:{\""id\"":\""ObejctId('id')\""}\nProjection (optional): To return only a subset of fields from each MongoDB document, enter a projection here. The value of this field must be a valid JSON-formatted string describing a MongoDB projection object \u2013 in the correct format and with the correct operators (not mixing inclusions and exclusions). Refer to MongoDB documentation for the expected projection object syntax and operators.\u00a0For example:\n{\""Name\"":true}\nSelect a type\nFour options are available for the Export type setting:\nAll\nDelta\nOnce\nLimit\nAll\nExports all records in the collection each time the export is run.\nDelta\nExport all records with a\u00a0Date field value that is greater than the last time the data flow was run.\nDate field (required): Provide a date field in MongoDB that integrator.io can use to keep track of records that have changed since the last time the export was run. It is recommended that you pick a system-generated field. For example, many applications maintain a standard Date Last Modified\u00a0or Last Modified Date field that always contains the date and time that a record was last changed. You could select a custom field if you have your own logic in place to set the field accordingly whenever a relevant change is made to a record.\nThe MongoDB field must contain a valid date value, such as a BSON date type. A string containing a timestamp will not be interpreted as a date for purposes of comparing changed records for the export.\nOnce\nExport any records that have not been exported already and automatically update records to mark them as exported.\nBoolean field (required): Select a Boolean (checkbox) field in MongoDB that integrator.io can use to keep track of records that have been exported. integrator.io will export only those records where this field is false (unchecked) and then make a subsequent request\u00a0to MongoDB to set the Boolean field to true for all exported records (so that they are exported once).\nLimit\nExport a limited set of records, primarily used for testing to avoid syncing lots of data.\nNote: When you select the export type as Limit - export a set number of records, another setting How many records would you like to export? is enabled for you to enter the maximum number of records to export each time this flow runs. The default value is set to one. You can export a maximum of 100 records.\nPreview exported data\nYou can validate your expressions by clicking Preview to view the first record in the results or any errors.\nNote: This export preview will show the results for all export types except Delta, in which case you would have to validate any export errors.\nGroup export records\nThe Grouping option allows you to manage your flow\u2019s files by grouping records by field. This feature allows you to aggregate your data based on your chosen field(s)."",
  ""date"": ""Fri, 27 Sep 2024 16:50:18 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/360044796071-Export-data-from-MongoDB"",
  ""linkedArticleIds"": [
  ""360040040071"",
  ""360040040071"",
  ""4405373029019""
  ],
  ""numTokens"": 1025.0,
  ""score"": 0.798262239,
  ""section_id"": 360007898172.0,
  ""source"": ""articles"",
  ""title"": ""Export data from MongoDB""
  },
  {
  ""articleId"": 6904335573403.0,
  ""content"": ""Before you begin\nSet up a connection to MongoDB.\nCreate an import\nFrom the Tools menu, select Flow builder. For the Destination application, click MongoDB. Select your MongoDB connection from the connection list and click Next.\n\u2013\u2013 OR \u2013\u2013\nFrom the Resources menu, select Imports. In the resulting Imports page, click + New Import. From the application list, click MongoDB. Select your MongoDB connection, add a name and a description for your import.\nOne to many (required): There are advanced use cases where a parent record is being passed around in a flow, but you actually need to process child records contained within the parent record context. For example, if you're exporting Sales Order records out of NetSuite and importing them into Salesforce as Opportunity and Opportunity Line Item records, then you will need to import the Opportunity Line Item records using this option. One to many is used in cases where you have a single record that internally needs to create multiple records. This field cannot be used when importing a CSV file.\nPath to many (required): If the records being processed are JSON objects, then use this field to select or enter the JSON path to the child records. This field does not need to be set for array/row-based data.\nHow would you like your records imported?\nMethod (required): Enter the method to use for adding or updating documents in your MongoDB instance. For more information on the available methods please refer to the MongoDB documentation.\nInsert many\nCollection (required): Enter the name of the MongoDB collection in your database that you would like to query from. For example: orders, items, users, customers, etc\u2026\nMongoDB document (optional): By default integrator.io will create new documents in your MongoDB instance using the raw JSON data returned by the exports running in your flow (or the raw JSON data that you submitted via the integrator.io API). If you want to modify the data before it is added to MongoDB (for example, using handlebars to convert timestamps to Dates) then enter a JSON string describing the expected document object structure in this field. The value of this field must be a valid JSON string describing a MongoDB document.\nIgnore existing records (optional): When importing new data, if it is possible for the data being imported to already exist in the import application, or if you are worried that someone might accidentally re-import the same data twice, you can use this field to tell integrator.io to ignore existing data. It is definitely a best practice to have some sort of protection against duplicates, and this field is a good solution for most use-cases. The only downside of using this field is the slight performance hit needed to check first if something exists or not.\nUpdate one\nCollection (required): Enter the name of the MongoDB collection in your database that you would like to query from. For example: orders, items, users, customers, etc\u2026\nMongoDB filter (optional): If you want to update documents in your MongoDB instance please enter a filter object to find existing documents here. The value of this field must be a valid JSON string describing a MongoDB filter object in the correct format and with the correct operators. Refer to the MongoDB documentation for the list of valid query operators and the correct filter object syntax.\nMongoDB document (optional): Enter the update object that specifies the fields to modify when updating documents in your MongoDB instance. The value of this field must be a valid JSON string describing a MongoDB update object in the correct format and with the correct operators. Refer to the MongoDB documentation for the list of valid update operators and the correct update object syntax. If this field is left blank then the default update object of { \""set\"": } will be used.\nUpsert (optional): Set this field to true if you want MongoDB to dynamically create new documents when nothing is found with the provided filter. Set this field to false (i.e. the default) if you want MongoDB to ignore documents that cannot be found with the provided filter.\nAdvanced\nConcurrency ID lock template (optional): This field can be used to help prevent duplicate records from being submitted at the same time when the connection associated with this import is using a concurrency level greater than 1.\nSaying this another way, there are fields on the connection record associated with this import to limit the number of concurrent requests that can be made at any one time, and if you are allowing more than 1 request at a time then it is possible for imports to override each other (i.e. a race condition) if multiple messages/updates for the same record are being processed at the same time.\nThis field allows you to enforce an ordering across concurrent requests such that imports for a specific record ID will queue up and happen one at a time (while still allowing imports for different record ids to happen in parallel).\nThe value of this field should be a handlebars template that generates a unique id for each exported record (note: we are using the raw exported data when generating the IDs -- before any import or mapping logic is invoked), and then with this id the integrator.io back-end will make sure that no two records with the same id are submitted for import at the same time.\nOne example, if you are exporting Zendesk records and importing them into NetSuite then you would most likely use '{{id}}' (the field Zendesk uses to identify unique records), and then no two records with the same Zendesk ID value would import into NetSuite at the same time.\nData URI template (optional): When your flow runs but has data errors this field can be really helpful in that it allows you to make sure that all the errors in your job dashboard have a link to the target data in the import application (where possible).\nThis field uses a handlebars template to generate the dynamic links based on the data being imported.\nNote\nThe template you provide will run against your data after it has been mapped, and then again after it has been submitted to the import application, to maximize the ability to link to the right place.\nFor example, if you are updating a customer record in Shopify, you would most likely set this field to the following value: https://your-store.myshopify.com/admin/customers/{{{id}}}.\nMap data in MongoDB\nMap your data using Mapper 2.0. Mapper 2.0 provides a clear visual representation of both source and destination JSON structures, enabling you to reference sample input data from the source app while building the JSON structure to be sent to a destination app. You can easily build complex JSON structures that include nested arrays and make use of data type validation for required fields and incompatible data types."",
  ""date"": ""Wed, 17 Apr 2024 14:22:37 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/6904335573403-Import-data-into-MongoDB"",
  ""linkedArticleIds"": [
  ""360039632032"",
  ""360039632032"",
  ""360039632032""
  ],
  ""numTokens"": 1333.0,
  ""score"": 0.788858831,
  ""section_id"": 360007898172.0,
  ""source"": ""articles"",
  ""title"": ""Import data into MongoDB""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Select MongoDB as the source application and add a connection if not already created.""
  },
  {
  ""title"": ""Provide the collection name and optional filters to configure the MongoDB export.""
  },
  {
  ""title"": ""Click \""Next\"" to proceed to the export mapping stage.""
  }
  ],
  ""thread_id"": ""thread_f541en1Cb7RBefwG2cWvVnH8"",
  ""vector_duration"": 1.68
 }",,,,,,,,,,,,,,,,
Copilot_55,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,Connection_Create_001.json,"{
  ""answer"": ""An **iClient** in an OAuth 2.0 connection on Celigo's integrator.io platform is a stored set of **client credentials** (Client ID and Client Secret) that are used to authenticate and securely connect to an external application, such as NetSuite. These credentials are provided by the external application (NetSuite in this case) and enable integrator.io to make authorized requests to access the application\u2019s data via APIs.\n\nWithin the integrator.io platform, iClient acts as a reusable resource that manages the integration of these credentials while establishing connections. iClient ensures that sensitive information, like the Client Secret, is encrypted and securely stored.\n\n### iClient in the context of NetSuite\nWhen connecting to NetSuite via an OAuth 2.0 connection, you need to create an iClient that stores the **Client ID** and **Client Secret** generated in your NetSuite account. These credentials are part of the NetSuite OAuth 2.0 setup process and are required to complete the authentication for your connection in integrator.io. Once created, the same iClient can be reused across multiple connections if they share the same NetSuite account and provider.\n\n---\n\n### Steps to configure iClient for a NetSuite OAuth 2.0 Connection\n\n1. **Set up OAuth 2.0 in NetSuite**:\n  - In the NetSuite account, generate your Client ID and Client Secret, and note the OAuth 2.0-related URLs such as the Authorization URL and Token URL. (Refer to NetSuite documentation for details on enabling OAuth 2.0 and creating client credentials.)\n\n2. **Create an iClient in integrator.io**:\n  - On your current page (`Connections - Create connection`):\n  - In the **OAuth 2.0 Authentication Type**, you\u2019ll see a field labeled **iClient (required)**.\n  - Click the **+ button** next to the iClient field to create a new iClient.\n\n  - Fill in the iClient details:\n  - **Client ID**: Enter the Client ID from NetSuite.\n  - **Client Secret**: Enter the Client Secret from NetSuite.\n  - **Grant Type**: Select the appropriate grant type, typically \""Authorization code\"" or \""Client credentials\"".\n  - **Authorization URL**: Enter the Authorization URL provided by NetSuite (e.g., `https://<accountID>.suitetalk.api.netsuite.com/authorize`).\n  - **Access Token URL**: Enter the Access Token URL (e.g., `https://<accountID>.suitetalk.api.netsuite.com/token`).\n  - **Redirect URL**: Use the callback URL provided by integrator.io (likely `https://integrator.io/callback`).\n  - **Revoke Token URL (optional)**: Enter the token revocation URL if required.\n  - **Valid Domain Names**: Add NetSuite\u2019s API domain name(s) for verification.\n\n  Save this iClient configuration.\n\n3. **Use the iClient in the connection**:\n  - Once the iClient is created, select it from the dropdown in the **iClient** field on the \""Create connection\"" page.\n  - Complete the other required fields, like environment, account ID, and other OAuth 2.0 settings.\n\n4. **Test the connection**:\n  - Use the **Save and test connection** buttons to verify that everything is correctly configured, and the connection works.\n\n---\n\n### Example Configuration in NetSuite\n\n1. **Generate OAuth 2.0 Client Credentials in NetSuite**:\n  - Navigate to `Setup > Company > Enable Features > SuiteCloud` and enable **OAuth 2.0** as an authentication method.\n  - Next, go to `Setup > Integration > Manage Integrations > New`, and fill in the details (e.g., name, redirect URL). This will give you your **Client ID** and **Client Secret**.\n\n2. **Create an integrator.io Connection for NetSuite**:\n  - Application: **NetSuite**\n  - Authentication type: **OAuth 2.0**\n  - iClient: [Select or create iClient with NetSuite Client ID, Client Secret, etc.]\n  - Environment: **Production, Sandbox, or Beta**\n  - Account ID: Found in `Setup > Company > Integration > SOAP Web Services Preferences`.\n\n---\n\nBy storing your NetSuite **Client ID** and **Client Secret** in an iClient, you centralize these credentials for reuse, simplify connection management, and enhance security during authentication."",
  ""completion_tokens"": """",
  ""openai_duration"": 21.01,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 14349054963995.0,
  ""content"": ""NetSuite documentation: Authentication\nBuild your own OAuth 1.0 connection with the universal HTTP connector to connect integrator.io to NetSuite SuiteTalk APIs (including SuiteQL).\nImportant\nOAuth 1.0 is a three-stage workflow, where the client first gets temporary credentials, performs authorization, and exchanges the temporary credentials for permanent long-lived tokens. This connector type assumes that you have already completed these steps and obtained the long-lived token and secret before setting up an OAuth 1.0 connection using endpoints. However, in the near future, Celigo will launch an upgraded OAuth 1.0 connector with the ability to retrieve the credentials for you.\nOAuth1.0 requires signed requests with complicated signature methods. While establishing the OAuth1.0 connection, integrator.io will sign the request using the signature method that you choose below.\nA. Set up an HTTP connection\nStart establishing a connection in either of the following ways:\nFrom the Resources menu, select Connections. Then, click+ Create connection at the top right.\nWhile working in a new or existing integration, you can add an application to a flow simply by clicking Add source or Add destination/lookup. From the Application list, under Universal connectors, select HTTP.\nAfter the HTTP Application is added, click the Connection setting\u2019s + button to proceed.\nB. Provide general HTTP connection settings\nEdit the General settings specific to your account and this connection resource.\nName (required): Enter a clear and distinguishable name. Throughout integrator.io imports and exports, you will have the option to choose this new connection. A unique identifier will prove helpful later when selecting it from a list of the connections in your account.\nApplication (required, non-editable): A reminder of the app you\u2019re editing.\nMode (required): Select Cloud.\nC. Provide application details\nEdit the Application details settings specific to your base uri and media type.\nBase URI (required): Enter the SuiteTalk URI. To find this, log in to you're NetSuite account and navigate to Setup > Company > Company Information > Company URLs > SuiteTalk (SOAP and REST Web Services).\nMedia type (required): Enter JSON.\nD. Edit OAuth 1.0 settings\nContinuing in the Create connection panel, select OAuth 1.0 for the Auth type.\nSignature method (required): Select HMAC-SHA256.\nConsumer key (required): Enter the consumer key.\nConsumer secret (required): Enter the consumer secret.\nAccess token (required): Enter the required token ID.\nToken secret (required): Enter the value retrieved with your Token ID , above. Multiple layers of protection are in place, including AES 256 encryption, to keep your connection\u2019s Token ID and Secret safe. When editing this form later, you must enter this value again; it is stored only for a saved connection.\nRealm ( required ): Enter your NetSuite ID as the realm value. To find this, log in to you're NetSuite account and navigate to Company > Integration > SOAP web services preferences > Primary information.\nE. How to test this connection?\nHTTP method (optional): Select GET.\nRelative URI (optional): Enter this endpoint that will always work: /services/rest/record/v1/metadata-catalog/customer.\nF. Save and test the connection\nOnce you have configured the HTTP connection, you have a few options for continuing:\nSave \u2013 click this button to test the connection, commit the new connection so that it will be available to all integrations for your account\nSave & close \u2013 click to test and save the connection and exit the Create connection panel\nClose \u2013 click to exit without saving any new changes\nTest connection \u2013 click this button to verify that your new connection is free of errors\nWhen you test or save the connection, it is verified before continuing.\nIf the connection fails, double-check the provided settings, and test again."",
  ""date"": ""Thu, 19 Jun 2025 12:52:50 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/14349054963995-Set-up-an-OAuth-1-0-connection-to-NetSuite-SuiteTalk-APIs-including-SuiteQL"",
  ""label_names"": [],
  ""linkedArticleIds"": [],
  ""numTokens"": 777.0,
  ""score"": 0.823937416,
  ""section_id"": 360012329152.0,
  ""source"": ""articles"",
  ""title"": ""Set up an OAuth 1.0 connection to NetSuite SuiteTalk APIs (including SuiteQL)""
  },
  {
  ""articleId"": 11933835192859.0,
  ""content"": ""You must use an iClient when authenticating your HTTP connection with OAuth 2.0. You can create an iClient in Resources \u2192 iClient or directly in an HTTP OAuth 2.0 connection setup.\nWhen creating an iClient in an application connector, you can toggle between a simple or HTTP view. If multiple connections have the same provider, you can use a single OAuth 2.0 iClient. To create an iClient for a specific API or version, select your application, and the available APIs or versions will appear.\nWarning\nThere are differences between an OAuth 2.0 iClient created in a simple or HTTP-type form view application connector and a universal HTTP connector. The universal HTTP connector will provide the most granularity when creating a connection. Learn more about the differences between a simple application and universal HTTP connector.\niClient resource configuration\nSimple iClient application configuration\nHTTP iClient configuration\nTo create an OAuth 2.0 iClient, fill in the required fields:\nConfigure OAuth 2.0\nFields\nDescription\nClient ID\nEnter your client ID as found in your application.\nClient secret\nEnter the client secret as found in your application.\nMultiple layers of protection are in place, including AES 256 encryption, to keep your connection\u2019s secret safe. When editing this form later, you must generate this value again; it is stored only when the connection is saved and never displayed as text.\nGrant type\nOAuth 2.0 authentication currently supports four grant types: authorization code, authorization code with PKCE, client credentials, and password credentials.\nChoose Authorization code to use an authorization server to obtain an authorization code as an intermediary between the client and resource owner.\nChoose Authorization code with PKCE (Proof Key for Code Exchange) to extend security to the OAuth 2.0 authorization code grant flow. This improves the security of public client applications, such as mobile or single-page web applications.\nChoose Client credentials if authorization scopes are limited to protected resources under the client's control or previously configured on the authorization server.\nUse JWT \u2013 Use JWT (JSON Web Token) as a secure digital pass to improve your app\u2019s identity for OAuth specification.\nChoose Password credentials to use your login credentials to authenticate your iClient.\nAuthorization code\nThe provided code is obtained by using an authorization server as an intermediary between the client, integrator.io, and resource owner. Selecting Authorization code enables additional required and informational settings.\nSend client credentials via: Sends a basic auth request in the header or client credentials in the request body.\nAuthorization URL: This is the authorization code retrieval endpoint on the API provider\u2019s authorization server.\nRedirect URL: Use the callback URL to exchange secure messages with the authorization server after authentication. You must whitelist this URL with your authorization server.\nAccess token URL: integrator.io retrieves the access token from this URL.\nRevoke token URL: integrator.io makes an HTTP post request to the token revocation endpoint URL to revoke a particular token.\nValid domain names: integrator.io validates the HTTP requests sent to various OAuth URLs using the domain name value. If the Authorization URL, access token URL and revoke token URL have different domain names, provide them as comma-separated values in any order.\nAuthorization code with PKCE (Proof Key for Code Exchange)\nPKCE, often used for public client applications like mobile or single-page web apps, adds an extra layer of security by requiring the generation of a unique code verifier. This random value is transformed into a code challenge using a cryptographic hash function, protecting your data from code injection and interception attacks. Meanwhile, the Password Credentials grant type simplifies the authentication process for trusted clients and direct user interactions, eliminating the need for additional authorization servers.\nCode challenge method: Algorithm used for generating code challenge.\nSend client credentials via: Sends a basic auth request in the header or client credentials in the request body.\nAuthorization URL: This is the authorization code retrieval endpoint on the API provider\u2019s authorization server.\nRedirect URL: Use the callback URL to exchange secure messages with the authorization server after authentication. You must whitelist this URL with your authorization server.\nAccess token URL: integrator.io retrieves the access token from this URL.\nRevoke token URL: integrator.io makes an HTTP post request to the token revocation endpoint URL to revoke a particular token.\nValid domain names: integrator.io validates the HTTP requests sent to various OAuth URLs using the domain name value. If the Authorization URL, access token URL and revoke token URL have different domain names, provide them as comma-separated values in any order. This field protects your OAuth 2.0 credentials so other users creating connections with the same iClient resource can't point their connection to a nonsecure URL that would receive your client secret.\nClient credentials\nLimits the authorization scope to the protected resources under the client's control or to those previously arranged with the authorization server. Selecting Client credentials exposes additional required settings.\nSend client credentials via: Sends a basic auth request in the header or client credentials in the request body.\nUse JWT: Use JWT (JSON Web Token) as a secure digital pass to improve your app\u2019s identity for OAuth specification. You can generate secure JWT tokens by signing in using signature methods such as RSA-SHA and HMAC-SHA. Then, you can utilize these JWT tokens or access tokens to invoke your APIs securely.\nSignature method (JWT only): Select the required method to sign the API call. For HMAC signature options, the secret key appears. For all other options, a private key is required.\nSecret key (HMAC only): Use this key as a secret password that generates the JWT signature.\nPrivate key: Copy the private key from the portal you want to use to authenticate the connection. Before you add it to integrator.io, you must replace all newline characters (\\n) throughout the private key. The private key must be in PEM format. You can convert PFX certificates or convert from a PPK file.\nPayload: The JSON object contains the message sent to the application. This is used along with the secret key to ensure no alteration to the message along the way.\nHeader: The JWT header is a JSON object that typically consists of two properties:\nalg (Algorithm): Specifies the algorithm used to sign the token. It can be HMAC SHA256, RSA SHA256, or others, depending on the chosen cryptographic algorithm.\ntyp (Type): Indicates the token type, typically set to \""JWT\"" for JSON Web Tokens.\nAccess token URL: integrator.io retrieves the access token from this URL.\nRevoke token URL: integrator.io makes an HTTP post request to the token revocation endpoint URL to revoke a particular token.\nValid domain names: integrator.io validates the HTTP requests sent to various OAuth URLs using the domain name value. If the Authorization URL, access token URL and revoke token URL have different domain names, provide them as comma-separated values in any order. This field protects your OAuth 2.0 credentials so other users creating connections with the same iClient resource can't point their connection to a nonsecure URL that would receive your client secret.\nPaste your private key:\nPaste the private key into a text editor.\nFind \\n.\nDelete the \\n characters and press Enter or Return. Repeat this for each instance of \\n.\nEnsure -----BEGIN PRIVATE KEY----- appears before the key, and -----END PRIVATE KEY----- appears after the key.\nCopy and paste the reformatted private key (including the begin and end declarations) into integrator.io.\nPassword credentials\nYour account username and password are used as authentication. Selecting Password credentials exposes additional required settings.\nSend client credentials via: Sends a basic auth request in the header or client credentials in the request body.\nAccess token URL: integrator.io retrieves the access token from this URL.\nRevoke token URL: integrator.io makes an HTTP post request to the token revocation endpoint URL to revoke a particular token.\nValid domain names: Enter each unique domain name for the OAuth URLs (Authorization, Access Token, Revoke Token) as comma-separated values. If your domain name is the same for each URL, only provide it once. For example, if your URLs are:\nhttps://.domain.com\nhttps://.domain1.com\nhttps://.domain2.com\nwrite domain.com, domain1.com, domain2.com.\nOAuth 2.0 overrides\nField\nDescription\nOverride default scope delimiter\nEnter the non-space scope delimiter used by your API provider.\nOverride access token HTTP headers\nIn some rare cases, it may be necessary to include custom HTTP headers with your API requests. The default content-type header value is application/x-www-form-urlencoded. Enter a name and a value to replace default header values with custom values.\nOverride access token request body\nConfigure your own access token body in JSON format if it is different from the default access token body. This JSON format is finally converted to the form-urlencoded format on the wire. Default access token body format if 'Client Authentication' set as 'body':\n{ code: {{{query.code}}}, redirect_uri: {{{redirectUri}}}, client_id: {{{clientId}}}, client_secret: {{{clientSecret}}}, grant_type: \u201cauthorization_code\u201d}.\nDefault access token body format if 'Client Authentication' set as 'header': { client_id: {{{clientId}}}, client_secret: {{{clientSecret}}} grant_type: \u201cclient_credentials\u201d }.\nOverride path to access token field in the HTTP response body\nAdd the path to the access token field in your HTTP response body. For example: accessToken. If the access token is not at the top level, use dot notation: data.accessToken.\nOverride refresh token HTTP headers\nIn some cases, it may be necessary to include custom HTTP headers with your token refresh requests. As with the 'body' field, any value from the connection can be referenced using {{{placeholders}}} with a complete path matching the connection field.\nOverride refresh token HTTP request body\nConfigure your own refresh token body in JSON format if it is different from the default refresh token body. This JSON format is finally converted to the form-urlencoded format on the wire.\nDefault refresh token body format:\n{ client_id: {{{clientId}}}, client_secret: {{{clientSecret}}}, grant_type: \u201crefresh_token\u201d, refresh_token: {{{connection.http.auth.token.refreshToken}}} }\nOverride path to refresh token field in the HTTP response body\nAdd the path to the refresh token field in your HTTP response body. For example: refreshToken. If the refresh token is not at the top level, use dot notation: data.refreshToken.\nOverride revoke token HTTP headers\nIn some rare cases, you may need to include custom HTTP headers with your API requests. The default content-type header value is application/x-www-form-urlencoded, and the authorization header value is basic.\nOverride revoke token HTTP body\nConfigure your own revoke body in JSON format if it is different from the default revoke token body. This JSON format is finally converted to the form-urlencoded format on the wire.\nDefault revoke body format: { token: {{{connection.http.auth.token.token}}} }.\nConfigure token auth\nSend token via: Select the location where your API expects to find the authentication token\nHTTP body\nThe API requires the token to be embedded in the body structure of your HTTP request. In such cases, place the token in your body template using the handlebars placeholder {connection.http.token.token}.\nHTTP header\nAllows you to specify the header name and authentication scheme to use when constructing the HTTP request.\nHeader name: By default, integrator.io will send all authentication type info in the 'Authorization: ' HTTP header field. If the REST API you are connecting to requires a different HTTP header, use this field to provide an override.\nHeader scheme: Use this field to set the HTTP authorization header scheme value. For example, 'Bearer' would be the scheme value for 'Authorization: Bearer my_secret_api_token'\nURL parameter\nThe authentication token is located in the URL. Specify the query string parameter name that holds the token value.\nParameter name: Use this field to specify the name of the URL parameter that holds the API token value. For example, if you specify myAPITokenURLParam, then all HTTP requests will include ?myAPITokenURLParam=[token]\nNon-standard API response patterns\nField\nDescription\nOverride HTTP status code for auth errors\nThis field only needs to be set if the HTTP status code for auth errors is not 401. For example, an API could return a generic 400 status code instead, and then use a field in the HTTP response body to indicate auth errors.\nPath to auth error field in HTTP response body\nThis field only needs to be set if the API returns a field in the HTTP response body to indicate auth errors. For example, if an API returns the field 'errorMessage' with the value 'Auth failed', then you would set this field to 'errorMessage'.\nAuth error values\nUse this field to limit the exact values in the HTTP response body field that should be used to determine auth errors. To provide multiple values, use a comma-separated list."",
  ""date"": ""Sun, 23 Feb 2025 21:35:21 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/11933835192859-Create-an-OAuth-2-0-iClient-resource"",
  ""label_names"": [
  ""OAuth 2.0"",
  ""iclient"",
  ""HTTP connection authentication"",
  ""OAuth grant types"",
  ""JSON Web Token (JWT)"",
  ""OAuth 2.0 iClient"",
  ""OAuth integration"",
  ""universal HTTP connector"",
  ""PKCE security enhancements"",
  ""refresh token"",
  ""Primary Information"",
  ""iclients""
  ],
  ""linkedArticleIds"": [
  ""10841316566043""
  ],
  ""numTokens"": 2635.0,
  ""score"": 0.820841491,
  ""section_id"": 5926003362459.0,
  ""source"": ""articles"",
  ""title"": ""Create an OAuth 2.0 iClient resource""
  },
  {
  ""articleId"": 360057498872.0,
  ""content"": ""Netsuite documentation: Authentication\nOne additional NetSuite authentication type is available: token-based (automatic) . However, if you are installing an integration app or adding a connection to one, you must establish your connection to NetSuite according to the integration app\u2019s instructions. For example, see Install Salesforce - NetSuite (IO).\nBefore you begin\nBefore you begin, read Before you connect to NetSuite.\nA. Set up a NetSuite connection\nStart establishing a connection to NetSuite in either of the following ways:\nFrom the Resources menu, select Connections. Then, click + Create connection at the top right.\n\u2013 or \u2013\nWhile working in a new or existing integration, you can add an application to a flow simply by clicking Add source or Add destination/lookup.\nIn the resulting Application list, select NetSuite.\nThe Create connection pane opens with required and advanced settings.\nB. Describe your NetSuite connection\nFirst, name your connection in the General settings:\nName (required): Provide a clear and distinguishable name. Throughout integrator.io imports and exports, you will have the option to choose this new connection, and a unique identifier will prove helpful later when selecting among a list of connections that you\u2019ve created.\nApplication (required, non-editable): A reminder of the app you\u2019re editing.\nC. Supply required NetSuite account information\nAt this point, you\u2019re presented with options for providing NetSuite authentication.\nAuthentication type (required): Select Token Based Auth (Manual).\nEnvironment (required): Enter the NetSuite environment you want to connect with: a production, sandbox, or beta environment. NetSuite must provision Sandbox NetSuite accounts, and beta environments are typically available only in the weeks prior to a NetSuite upgrade.\nAccount ID (optional): Enter your account ID. Contact NetSuite support to obtain the ID if it\u2019s not exposed in the following steps:\nNote\nMake sure you use underscore (_) instead of dash (-) while you enter your account ID. For example, if your account ID is 12345_SB1.\nHow to retrieve you NetSuite account ID\nSign into your NetSuite account.\nClick Setup.\nNavigate to Company > Integration > SOAP web services preferences > Primary information.\nCopy Account ID.\nToken ID (required): Enter the required token ID. Before creating an access token in NetSuite for integrator.io, verify that you have installed Celigo integrator.io SuiteApp (ID 20038) in your NetSuite account, which creates the role Celigo integrator.io Full Access Role w/o 2FA [Locked].\nToken secret (required): Enter the value retrieved with your Token ID, above. Multiple layers of protection are in place, including AES 256 encryption, to keep your connection\u2019s Token ID and Secret safe. When editing this form later, you must enter this value again; it is stored only for a saved connection.\nHow to retrieve your token and token secret\nIn NetSuite, go to Setup > Users/Roles > Manage roles; then click Customize corresponding to the role Celigo integrator.io Full Access Role w/o 2FA [Locked]; and save the role (clone) with a different name.\nNote\nCeligo integrator.io Full Access Role w/o 2FA [Locked] is a locked custom Celigo role. When cloning the role, you can add additional permissions to the role to fit your requirements.\nNavigate to Manage roles to verify the presence of Celigo integrator.io Full Access w/o 2FA [Locked].\nGo to Manage users, select a user, then click Edit to modify the user.\nClick the Access subtab at bottom of the screen, then the Roles subtab.\nAdd the custom role that you just created, then click Save.\nGo to Setup > Access tokens > Create access token. You must add \u201cintegrator.io\u201d\u00a0in the\u00a0Application\u00a0field for all access tokens the Celigo platform uses.\nCopy the Token ID and Token secret to a safe place, since they will not be shown again.\niClient (required): Select the iClient pair that stores the client ID and client secret provided to you by NetSuite. To add an iClient and configure your credentials, click the plus (+) button. Click the edit () button to modify a selected iClient. Be sure to give the iClient a recognizable name for use in any other connections.\nD. Edit advanced NetSuite settings\nBefore continuing, you have the opportunity to provide additional configuration information, if needed, for the NetSuite connection.\nLink SuiteScript integrator (optional): This setting appears only when you edit a NetSuite connection. Prior to integrator.io, NetSuite integrations built by Celigo ran directly inside your NetSuite account via a managed bundle. If you are still running any of these older integrations, check this box to link integrator.io to your NetSuite account.\nsidebar. Optional concurrency and rate limit settings\nBorrow concurrency from (optional): Select another connection from the list. By default, all data flowing through a connection record is submitted to the respective endpoint application at the concurrency level configured for that connection record. There are cases, however, where multiple integrator.io connections need to share the same concurrency level, and this field allows you to specify which connection you would like to borrow from. With borrowed concurrency, the data flowing through both connections will be submitted to the endpoint application together, via a shared concurrency model.\nAuto-recover rate limit errors: This setting is enabled by default with a predefined value for Target concurrency level. You can disable the setting if required. To set or make changes in any of these settings, you must have admin or manage permissions.\nTarget concurrency level: This field is shown only if Auto-recover rate limit errors is enabled. Change the predefined value as required to limit the number of concurrent API requests allowed by the connection resource. See also, Setting currency level on a connection.\nConcurrency level: When Auto-recover rate limit errors is checked, you can't modify the concurrency level because it matches the value set by the connected platform's runtime. When Auto-recover rate limit errors is not checked, you can enter a value to limit the number of concurrent API requests allowed by the connection resource. See also, Setting currency level on a connection.\nWSDL version: New Web Services Description Language (WSDL) versions are available when creating or editing a NetSuite connection. A WSDL is simply an XML document describing the latest versions of a web service. By choosing a WSDL you are choosing to upgrade to a new or older version of NetSuite\u2019s web services.\nYou can choose between versions 2023.1; 2020.2; and 2018.1. Versions 2018.1 and later support both manual and automatic token-based auth connections. Switching versions for an existing connection might have an impact on running flows, and we recommend reviewing NetSuite\u2019s release notes prior to making a change. We recommend using the latest WSDL version.\nE. Save and test the connection\nOnce you have configured the NetSuite connection, you have a few options for continuing:\nSave \u2013 click this button to test the connection, commit the new connection so that it will be available to all integrations for your account\nSave & close \u2013 click to test and save the connection and exit the Create connection pane\nCancel \u2013 click to exit without saving any new changes\nTest connection \u2013 click this button to verify that your new connection is free of errors\nWhen you test or save the connection, it is verified before continuing.\nThe new connection is now successfully added to your account. It will be applied to the current source or destination app, if you created it within a flow. Otherwise, you may proceed to register the connection with an integration.\nRead more about this connector:\nThe Leader in NetSuite Integrations\nComplete Integration Platform (iPaaS) for NetSuite and Beyond"",
  ""date"": ""Thu, 19 Jun 2025 12:52:40 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/360057498872-Set-up-a-manual-token-based-auth-connection-to-NetSuite"",
  ""label_names"": [],
  ""linkedArticleIds"": [
  ""360038996151"",
  ""360052010032"",
  ""360026938651"",
  ""115001075611"",
  ""360006899231"",
  ""11933835192859"",
  ""115000281032"",
  ""16182564553371"",
  ""115003929872"",
  ""360043926372"",
  ""360043926372"",
  ""360038996151"",
  ""360007568091""
  ],
  ""numTokens"": 1594.0,
  ""score"": 0.817001343,
  ""section_id"": 360012329152.0,
  ""source"": ""articles"",
  ""title"": ""Set up a manual token-based auth connection to NetSuite""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Create an iClient in integrator.io with NetSuite OAuth 2.0 credentials.""
  },
  {
  ""title"": ""Select the created iClient in the current \""Create connection\"" form.""
  },
  {
  ""title"": ""Test and save the connection to verify it works successfully.""
  }
  ],
  ""thread_id"": ""thread_uHgj9EjkjjJ3BX92w6w8Ya9t"",
  ""vector_duration"": 1.97
 }",,,,,,,,,,,,,,,,
Copilot_56,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,BuildSQLquery_001.json,"{
  ""answer"": ""To pass the **Name** dynamically from the **Settings** into your SQL query, you can use **Handlebars expressions**. Handlebars allow you to reference dynamic data stored in your integration's settings or environment variables.\n\n### Steps to Pass `Name` Dynamically in the SQL Query:\n\n1. **Understand Handlebar Usage**:\n  - In your SQL query, you can use double curly braces (`{{ }}`) to reference variables like `Name`.\n\n2. **Define `Name` in the Settings**:\n  - Ensure that the `Name` field you want to pass dynamically is configured as part of the **Settings** in the flow or export configuration.\n  - Settings are typically declared when you're naming or describing your export setup in **General** (e.g., provide dynamic variables like \""employeeName\"").\n\n3. **Modify Your SQL Query**:\n  - To pass `Name` dynamically, update your query to use `{{Name}}` or the appropriate variable format for your field.\n  - For example:\n  ```sql\n  SELECT * FROM Employee WHERE EmployeeId > 10 AND Name = '{{Name}}'\n  ```\n  - This will replace `{{Name}}` with the value of the `Name` field set in the dynamic configuration during runtime.\n\n4. **Optional Handlebars Editor**:\n  - Use the **Handlebars editor** when building your SQL query (available in integrator.io's UI). Click the **Handlebars** button to open the dynamic variable editor and ensure your expression resolves properly.\n\n5. **Preview Your Query**:\n  - Run a **Preview** of the query to confirm that the value of `Name` is injected dynamically, based on your settings.\n\n6. **Testing**:\n  - Ensure that you\u2019ve assigned a value to `Name` either in the flow configuration or via manual testing. If the field is empty, the query won\u2019t resolve correctly.\n\n---\n\n#### Example Scenario:\nIf `Name` is defined as a setting in your flow or export and you set the value as \""John\"", your query:\n```sql\nSELECT * FROM Employee WHERE EmployeeId > 10 AND Name = '{{Name}}'\n```\nWill dynamically resolve and execute as:\n```sql\nSELECT * FROM Employee WHERE EmployeeId > 10 AND Name = 'John'\n```\n\n---\n\n### Key Notes:\n- Make sure all required fields in the settings are properly configured and populated.\n- If using **mock output** or testing, you can prepare a sample JSON input that includes `'Name': 'ExampleName'` to simulate how it works.\n\nLet me know if you need more specific help setting up Handlebars in your query!"",
  ""completion_tokens"": """",
  ""openai_duration"": 12.29,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 37129640412571.0,
  ""content"": ""The Microsoft Active Directory has two form views\u2014Simple and SQL\u2014that you can use to export data.\nSimple view: Enables you to easily view and specify the resource, columns, and operations to filter and export data. This simple option is helpful if you're not very familiar with SQL and would like to\nview all the resources and related options available\nmake UI selections rather than write SQL queries to export data\nSQL view: Enables you to quickly write SQL queries. The SQL option is helpful if you're a technical user familiar with SQL and would like to\nuse SQL programming rather than the graphical user interface (GUI) for quicker database interactions\nwrite SQL queries including complex ones to export data\nExport settings\nThe export settings under the What would you like to export? section differ in Simple and SQL view. Any settings you configure in this section is saved in that view only. If you switch views, this data is cleared. For example, if you switch from SQL to Simple view, then after the switch, the data in the SQL view is cleared. But, you can open the export and select the SQL view to access the most recently saved version of the data.\nEnter all the required *\u00a0fields in the sections below.\nSettings\nInstructions\nGeneral\nName your export *\nProvide a clear and distinguishable name. You will have the option to choose this export throughout integrator.io, and a unique identifier will prove helpful later when selecting among a list of exports/lookups that you\u2019ve created.\nDescription\nDescribe your export so that you and others can quickly understand its purpose. Be sure to highlight any nuances that a user should be aware of before using this import in a flow. As you make changes to the resource, be sure to keep this description up to date.\nConnection *\nSelect the connection if you have an existing Microsoft Active Directory connection. If you want to create a new connection, see Set up a connection to on-premise Microsoft Active Directory.\nWhat would you like to export? (Simple view)\nResource *\nSelect a resource from the list. For more information, see Microsoft Active Directory tables & parameters.\nSelect columns\nBy default, all the columns for the resource are selected, and the number of selected columns is shown.\u00a0 If you want to select specific columns or review columns, click Select columns. In the Configure columns page that opens, based on your requirements,\nTo deselect all columns, clear the Column box\nTo quickly select specific columns, use the search box\nTo filter and view columns, choose All, Selected, or Unselected\nFiltering conditions\nTo filter records, use the logical and conditional operators.\nLogical operators: Use any of the following logical operators to define rules for data:\nNOT: Select the checkbox if you want to use the NOT operator. NOT excludes any data that meets the criteria you define in the rules. If the checkbox is not selected, the filter includes any data that meets your defined rule criteria.\nAND: Select AND if you want data that meets the combined criteria defined in the rules.\nOR: Select OR if you want data that meets any criteria defined in the rules.\nIf you want to exclude all records that belong to a subsidiary company, select NOT. You can also select AND or OR to create filters that combine multiple rules for your filtration criteria. AND filters records that fit all listed rules in the group. OR filters records that fit any listed rules in the group.\nConditional operators: Choose any of the following operators for the selected column. Enter any required fields for a specified operator.\nequals: Filters records with values equal to the one you specify.\nnot equals: Filters records with values that don't equal the one you specify.\nis greater-than: Filters records with values that are greater than the one you specify.\nis greater-than or equals: Filters records with values greater than or equal to the one you specify.\nis less-than: Filters records with values less than the one you specify.\nis less-than or equals: Filters records with values less than or equal to the one you specify.\nbetween: Filters records with values that are within the range you specify.\nnot between: Filters records with values that are outside the range you specify.\nlike: Filters records with values that are similar to the pattern (using wildcards) you specify.\nnot like: Filters records with values that are not similar to the pattern (using wildcards) you specify.\ncontains: Filters records with values that contain details you specify.\ndoes not contain: Filters records with values that don't contain details you specify.\nin: Filters records with values that match the ones you specify. Use commas to separate the values If you don't use commas, it will be considered as an array.\nnot in: Filters records with values that don't match the ones you specify. Use commas to separate the values. If you don't use commas, it will be considered as an array.\nnull: Filters records that don't have values in the specified column.\nis not null: Filters records that have values in the specified column.\nWhat would you like to export? (SQL view)\nSQL query *\nWrite the SQL query to retrieve records based on your requirements.\nTip\nIf you're familiar with SQL, enter your query in the box.\nIf you would like to use handlebars, click to open the Handlebars editor and create your SQL query. To build your SQL query easily and quickly, you can ask Celigo AI to generate a query. Enter a description of your query in plain English in the space provided, and Celigo AI will attempt to generate a query based on your prompt.\nSee\u00a0also, Examples for querying Microsoft Active Directory\nConfigure export type\nExport type *\nSelect one of the following export types:\nAll: To export all data, always\nDelta: To export only data that has changed since the last time the data flow was run\nOnce: To export only data that has not already been exported\nLimit: To export a limited set of records, primarily used for testing to avoid syncing lots of data\nNote\nWhen you select the export type as\u00a0Limit - export a set number of records\u00a0, another setting\u00a0How many records would you like to export?\u00a0is enabled for you to enter the maximum number of records to export each time the flow runs. The default value is set to one. You can export a maximum of 100 records.\nFor more information, see Four different export types to retrieve data in Fine-tune integrator\u2024io for optimal performance and data throughput.\nAfter you configure the required settings, click Preview to check the export. If the SQL query is not executed successfully, revise the Query or Export type values. Then, click Save.\nConfigure optional settings\nThese export settings are optional, and typically don't have to be configured. However, if required in your scenario, you can configure these settings.\nOptional sections/settings\nInstructions\nWould you like to group records?\nGroup records by fields\nSee Sort and group records\nMock output\nMock output\nSee Provide mock response data\nAdvanced\nPage size\nSee Choosing the right page size in Fine-tune integrator\u2024io for optimal performance and data throughput\nData URI template\nWhen your flow runs but has data errors this field can be really helpful in that it allows you to make sure that all the errors in your job dashboard have a link to the original data in the export application.\nUse a handlebars template to generate the dynamic links based on the data being exported. For example, if you are exporting a customer record from Shopify, you would most likely set this field to the following value https://your-store.myshopify.com/admin/customers/{{{id}}}. Or, if you are just exporting a CSV file from an FTP site then this field could simply be one or more columns from the file: {{{internal_id}}, {{{email}}}, etc...\nDo not store retry data\nCheck this box if you do NOT want integrator.io to store retry data for records that fail in your flow. Storing retry data can slow down your flow's overall performance if you are processing very large numbers of records that are failing.\nOverride trace key template\nIf this field is set, you will override the platform default trace key field.\nintegrator.io uses the trace key to identify a unique record. You can use a single field such as {{{field1}}} or use a handlebar expression. For example, this syntax {{join \u201c_\u201d\u00a0field1\u00a0field2}} will generate a trace key template field1_field2."",
  ""date"": ""Thu, 19 Jun 2025 12:49:30 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/37129640412571-Export-data-from-Microsoft-Active-Directory-using-Simple-or-SQL-view"",
  ""label_names"": [
  ""Simple view"",
  ""Export"",
  ""Microsoft Active Directory"",
  ""SQL view""
  ],
  ""linkedArticleIds"": [
  ""31519151018267"",
  ""32898080630939"",
  ""360043040652"",
  ""19845101172507"",
  ""33086848672923"",
  ""360043927292"",
  ""360039842611"",
  ""4405373029019"",
  ""11900462442267"",
  ""360043927292"",
  ""360060740672""
  ],
  ""numTokens"": 1719.0,
  ""score"": 0.759052753,
  ""section_id"": 31518991697691.0,
  ""source"": ""articles"",
  ""title"": ""Export data from Microsoft Active Directory using Simple or SQL view""
  },
  {
  ""articleId"": 25785691659547.0,
  ""content"": ""Before you begin, set up a connection to Microsoft SQL if you don't have an existing connection that you can use.\nExport data from the connected database\nFrom the Resources menu, select Exports to open the list of exports for your account.\nAt the upper right of the Exports page, click + New export.\nIn the resulting Create export panel, give the new export a meaningful name. (The Name field is required; Description is optional, for your reference.)\nContinuing in the Create export panel, select Microsoft SQL from the Application list.\nAfter you select a database application, the Connection list is populated only with existing connection resources for that database type. Select an existing connection, or click the plus (+) button to add a new connection.\nThen, in the Query field, write the SQL statement that fetches the data that you require.\nNote\nSome implementations require you to specify the database name when accessing a table \u2013 for example,\nSELECT * FROM ..\nSelect an Export type:\nAll \u2013 to export all data, always\nDelta \u2013 to export only data that has changed since the last time the data flow was run\nOnce \u2013 to export only data that has not already been exported\nLimit \u2013 to export a limited set of records, primarily used for testing to avoid syncing lots of data\nNote\nWhen you select the export type as Limit - export a set number of records , another setting How many records would you like to export? is enabled for you to enter the maximum number of records to export each time this flow runs. The default value is set to one. You can export a maximum of 100 records.\nClick preview to see sample JSON-formatted data. If the SQL query is not executed successfully, revise the Query or Export type values.\nNote\nIn the export Preview and import mappings Input, you will see the bigint datatype values in string format. Bigint values are returned as strings to avoid precision issues and accurately represent very large numbers.\nClick Save, and proceed to access the new export within a flow.\nGroup export records\nThe Grouping option allows you to manage your flow\u2019s files by grouping records by field. This feature allows you to aggregate your data based on your chosen field(s)."",
  ""date"": ""Thu, 17 Apr 2025 10:55:56 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/25785691659547-Export-data-from-Microsoft-SQL"",
  ""label_names"": [
  ""exports"",
  ""database"",
  ""sql"",
  ""Microsoft"",
  ""microsoft SQL""
  ],
  ""linkedArticleIds"": [
  ""360039003951""
  ],
  ""numTokens"": 444.0,
  ""score"": 0.75747329,
  ""section_id"": 5931291028507.0,
  ""source"": ""articles"",
  ""title"": ""Export data from Microsoft SQL""
  },
  {
  ""articleId"": 360051011932.0,
  ""content"": ""Begin retrieving data from DynamoDB by creating an export that will send your data retrieval request. You can start with a standalone export or add one in Flow Builder, as described below.\nCreate an export\nSelect Flow Builder from the Tools menu.\nChoose DynamoDB for the Source application.\nSelect a DynamoDB connection from the Connection list.\nClick Next.\nThe new export\u2019s Create export pane opens for you to edit the settings:\nName (required): Name the export so that you can easily reference it from other parts of the application.\nDescription (optional): Enter a description for your export so that other users can quickly understand its purpose without having to examine all the fields and settings. Be sure to highlight any important nuances that others might need to know before using your export in their flows. Update the description whenever you make changes to the export in the future.\nRegion (required): Select the DynamoDB location where the request is being made, or accept the default us-east-1.\nTable name\u2009(required): Specify the DynamoDB database table \u2013 such as limit-test orders , items , or customers \u2013 that you would like to query.\nExpression attribute names\u2009(required): An expression attribute name is a placeholder in an Amazon DynamoDB expression as an alternative to an actual attribute name. An expression attribute name must begin with a pound sign ( # ) and be followed by one or more alphanumeric characters, such as { \""#n1\"": \""sku\"", \""#n2\"":\""id\"" }.\nBuild expression attribute values\u2009(required): If you need to compare an attribute with a value, define an expression attribute value as a placeholder. These values substitute for the actual values that you might not know until runtime. An expression attribute value must begin with a colon ( : ) and be followed by one or more alphanumeric characters, such as { \"":p1\"": \""celigo-tshirt\"",\"":p2\"": 99 }.\nKey condition expression (required): To specify the search criteria, provide a key condition expression, a string that determines the items to be read from the table or index, such as #n1 = :p1 AND #n2 > :p2.\nFilter expression (optional): If you want to export only specific documents from your collection, then enter a filter object. This value must contain a valid JSON string describing a DynamoDB filter object in the correct format and with the correct operators.\nProjections (optional): If you want to return only a subset of fields from each DynamoDB document, then enter a projection object. The value of this field must be a valid JSON string describing a DynamoDB projection object in the correct format and with the correct operators.\nExport types\nSelect one of the following export types:\nAll \u2013 Exports all records matching the Key condition expression each time the export runs\nDelta \u2013 Export all records with a value in the Date field that is greater than the last time the flow was run matching the Key condition expression results\nOnce \u2013 Export any records that have not been exported already and will also automatically update records to mark them as exported\nLimit \u2013 Export a limited number of records, primarily used for testing to avoid syncing lots of data\nNote\nWhen you select the export type as Limit - export a set number of records , another setting How many records would you like to export? is enabled for you to enter the maximum number of records to export each time this flow runs. The default value is set to one. You can export a maximum of 100 records.\nAs demonstrated in the following export type examples, you can validate your expressions by clicking the Preview button. Any errors returned by the source application are displayed. The preview returns the first matching record for all export types, except Delta.\nAll\nExports all records matching the Key condition expression each time the export runs.\nSample configuration\nRegion: US East (N. Virginia) [us-east-1]\nTable name: limit-test\nExpression attribute names: { \""#n1\"": \""sku\"", \""#n2\"": \""id\"" }\nBuild expression attribute values: { \"":p1\"": \""celigo-tshirt\"",\"":p2\"": 99 }\nKey condition expression: #n1 = :p1 AND #n2 > :p2\nThis expression translates to sku = celigo-tshirt AND id > 99 \u2013 for example, in plain English, \u201cExport all t-shirts from my Amazon store that were added sequentially starting with the benchmark 100th shirt.\u201d\nDelta\nExport all records with a date value for the mentioned field in the Date field that is greater than the last time the data flow was run matching the Key condition expression results.\nSample configuration\nRegion: US East (N. Virginia) [us-east-1]\nTable name: delta-test\nExpression attribute names: { \""#n1\"": \""joining\"", \""#n2\"": \""sku\"" }\nBuild expression attribute values: { \"":p1\"": \""2019-08-13T19:56:12.008Z\"", \"":p2\"": \""celigo-tshirt\"" }\nKey condition expression: #n1=:p1 AND #n2=:p2\nDate field: joining\nSelect a date field from the export application that integrator.io can use to keep track of records that have changed since the last time the export was run. It is recommended that you pick a system-generated field. For example, many applications maintain a standard Date Last Modified field that always contains the date and time that a record was last changed. You can also select a non-system generated field if you have your own logic in place (in the export application) to set the field accordingly whenever a related change is made to a record.\nOnce\nExport any records that have not been exported already and also automatically update records to mark them as exported.\nSample configuration\nRegion: US East (N. Virginia) [us-east-1]\nTable name: export_once_partition\nExpression attribute names: { \""#n1\"": \""emp_id\"" }\nBuild expression attribute values: { \"":p1\"": 81 }\nKey condition expression: #n1 = :p1\nOnce boolean field: export_once\nProvide a Boolean (checkbox) field from the export application that integrator.io can use to keep track of records that have been exported. integrator.io will export only those records where this value is false (unchecked), and integrator.io will also automatically make a subsequent request back into the export application to set this field to true for all the records that were exported (so that those records are not exported again).\nOnce export partition key: emp_id\nEnter the partition key from the table.\nLimit\nExport a set number of records; used for testing to avoid syncing lots of data.\nNote\nWhen you select the export type as Limit - export a set number of records, another setting How many records would you like to export? is enabled for you to enter the maximum number of records to export each time this flow runs while developing and testing an integration. The default value is set to 1. You can export a maximum of 100 records.\nSample configuration\nRegion: US East (N. Virginia) [us-east-1]\nTable name: limit-test\nExpression attribute names: { \""#n1\"": \""sku\"", \""#n2\"": \""id\"" }\nBuild expression attribute values: { \"":p1\"": \""celigo-tshirt\"",\"":p2\"": 99 }\nKey condition expression: #n1 = :p1 AND #n2 > :p2\nGroup export records\nThe Grouping option allows you to manage your flow\u2019s files by grouping records by field. This feature allows you to aggregate your data based on your chosen field(s)."",
  ""date"": ""Thu, 17 Apr 2025 10:55:42 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/360051011932-Export-data-from-DynamoDB"",
  ""label_names"": [
  ""Exports"",
  ""database"",
  ""sql"",
  ""connectors"",
  ""DynamoDB"",
  ""Amazon MWS""
  ],
  ""linkedArticleIds"": [
  ""360039720112""
  ],
  ""numTokens"": 1547.0,
  ""score"": 0.755001783,
  ""section_id"": 360008022451.0,
  ""source"": ""articles"",
  ""title"": ""Export data from DynamoDB""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Ensure the `Name` variable is defined in the settings for dynamic query injection.""
  },
  {
  ""title"": ""Update your SQL query using `{{Name}}` to dynamically filter results.""
  },
  {
  ""title"": ""Preview the query to test `Name` dynamic value substitution and verify results.""
  }
  ],
  ""thread_id"": ""thread_YvtXykUWMJP4YMqBAnyoI7GM"",
  ""vector_duration"": 1.78
 }",,,,,,,,,,,,,,,,
Copilot_57,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,Agents_001.json,"{
  ""answer"": ""To connect your local SQL database using the Celigo Agent, follow these steps:\n\n---\n\n### Step 1: **Install the Celigo On-Premise Agent**\n1. Make sure your system meets the **minimum requirements**:\n  - **Windows**: Version 7 or later (64-bit OS required).\n  - **RAM**: Minimum 2 GB, recommended 4 GB.\n  - **CPU**: Minimum 2 cores.\n2. Navigate to **Resources \u2192 Agents** in the Celigo integrator.io dashboard.\n3. Click on **+ Create agent**.\n  - Name the agent and add an optional description.\n  - Click **Save** to create the agent. It will appear in the agent list.\n4. From the **Install column**, click **Download** and select the appropriate operating system (Windows).\n5. After downloading, follow the installation process:\n  - Open the `agent-windows.exe` setup file.\n  - Select whether to install for **Only for me (Administrator)** or **For all users**.\n  - Complete the installation. Ensure you allow Windows to trust the executable.\n\nOnce complete, an Agent shortcut will be added, and you'll be prompted for an **agent token**.\n\n---\n\n### Step 2: **Retrieve and Enter the Agent Token**\n1. In integrator.io, go back to **Resources \u2192 Agents**.\n2. Find your created agent in the list and click **Show token**.\n3. Copy the token and paste it into the Celigo Agent software on your local machine.\n  - This token secures communication between the agent and integrator.io.\n4. Verify that the agent comes online in your integrator.io dashboard. You might need to refresh the page.\n\n---\n\n### Step 3: **Install the Java Environment**\nIf your SQL database requires a JDBC connection (for example, MySQL, MS SQL), you will need to install **Java JDK 17** or higher on the local machine:\n1. Download and install Java JDK 17.\n2. Configure the environment variables:\n  - Set the `JAVA_HOME` variable to the installation directory (e.g., `C:\\Program Files\\Java\\jdk-17.0.1`).\n  - Add the `bin` directory from `JAVA_HOME` to the `PATH` variable (e.g., `C:\\Program Files\\Java\\jdk-17.0.1\\bin`).\n3. Download the appropriate **JDBC driver** for your database and place it in a single directory (e.g., `C:\\Celigo\\Drivers`).\n\n---\n\n### Step 4: **Set Up the Connection**\n1. In integrator.io, navigate to **Resources \u2192 Connections**.\n2. Click **+ Create connection**. Select the database application:\n - If your database is not specifically listed (e.g., IBM Db2, MySQL), choose **JDBC**.\n3. Configure the connection:\n  - **Name**: Provide a clear name (e.g., Local SQL DB Connection).\n  - **Application**: Select the appropriate database (or JDBC for generic databases).\n  - **Mode**: Select **On-premise**.\n  - **Agent**: Select your installed agent from the dropdown.\n  - **Host**: Enter your local machine\u2019s hostname or IP address (e.g., `localhost` or `127.0.0.1`).\n  - **Port**: Enter your SQL database's port (e.g., `1433` for MS SQL or `3306` for MySQL).\n  - **Database**: Enter the name of your SQL database.\n  - **Username and Password**: Enter the credentials to access your database.\n4. For JDBC:\n  - Enter the JDBC **connection string** (e.g., `jdbc:mysql://localhost:3306/<database>`).\n  - Specify the **driver directory path** where the JDBC driver is saved.\n5. (Optional) Enable **SSL** if your connection requires encryption.\n6. Click **Test Connection** to verify the setup.\n  - If the test passes, the connection will become active and appear in your list of connections.\n\n---\n\n### Step 5: **Build Flows That Use the Connection**\n1. Return to **Resources \u2192 Connections** and ensure your connection appears as **Online**.\n2. Use the connection in any integration flow by selecting it as a **Source** or **Destination** in the **Flow Builder**.\n\n---\n\n### Additional Notes for Success:\n- Ensure that the SQL database is accessible from the machine where the agent is installed.\n- If the database is on a different machine in the same network, use the correct hostname or IP address from that server.\n- Whitelisting integrator.io IPs is **not required** because the on-premise agent establishes a secure tunnel.\n\nIf you encounter issues, refer to the documentation or contact Celigo Support for further assistance."",
  ""completion_tokens"": """",
  ""openai_duration"": 42.02,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 360004081771.0,
  ""content"": ""Minimum system requirements\nWindows version: Windows 7 or later\nWarning\nEnd of support for Celigo on-prem agents using Windows Server 2012 R2.\nRAM: 2 GB (Minimum), 4 GB (Recommended)\n2 CPU cores\nDownload and install the agent software\nWarning\nEnd of support for Celigo on-prem agents using Windows Server 2012 R2.\nNavigate to Resources \u2192 Agents.\nClick + Create agent.\nName your agent and add a description.\nClick Save. Your agent now appears on the list.\nFrom the Install column, click the Download option and choose the operating system (Windows).\nAfter downloading, install and open the agent software.\nInstall the agent\nIf you downloaded the agent on the target computer, open the agent\u2019s setup file, agent-windows.exe, from your browser and install it.\nOtherwise, copy the downloaded file to the server and double-click to launch setup.\nThe setup prompts you to choose installation options:\nSelect whether to install the agent Only for me (Administrator) or for Anyone who uses this computer (all users) in the currently logged-in profile.\nOnly for me \u2013 accessible to only one user\nAll users \u2013 any user can run the service, but only one user should run it at a time.\nWarning\nIf you select All users, administrative privileges are not automatically assigned and the auto-upgrade feature will not work. Add admin privileges manually as follows:\nStop the service if it's running.\nRight-click Celigo Agent in your Windows Services app.\nClick Properties \u2192 Log on.\nClick This account, write in Administrator as the username, and enter the administrator password.\nRestart the agent service.\nClick Install.\nGive Windows permission to trust the downloaded executable and proceed.\nIn the Completing Agent Setup window, ensure that Run agent is checked. Then, click Finish.\nThe Agent shortcut is added to your desktop and Windows programs, and the software opens to ask you for an integrator.io token.\nEnter agent token\nAn access token secures communication between the agent and integrator.io. Your agent token is automatically generated when you create the agent. To find the token:\nReturn to integrator.io and navigate to Resources \u2192 Agents.\nIn the list of agents, click Show token.\nClick Copy (\u200b\u200b) next to the unmasked access token to copy it to your clipboard.\nIn the agent software, enter the access token. The agent will automatically start communicating with integrator.io.\nThe agent should now be online. You may need to refresh the page to ensure it\u2019s online.\nIf your token is compromised, you can generate a new token:\nIn integrator.io, navigate to Resources \u2192 Agents.\nClick Actions \u2192 Generate token.\nThis will automatically revoke the old token, so you must enter the new one.\nNot required: Run the agent as a Windows service instead of an executable\nIf you\u2019ve followed the steps up to this point, the agent is running as a standard Windows executable. You can configure it to run as a Windows service instead to take advantage of the following benefits:\nThe agent will run silently and continuously after installation and each time the system starts up\nThe agent will run regardless of which user account is logged on\nAs a service, the agent can be managed and monitored from remote systems\nTo run the agent as a Windows service:\nDouble-click the Agent shortcut on your desktop.\nChoose Install service.\nThe agent will confirm the successful installation.\nClick OK to dismiss the user interface.\nYour system must restart to run the agent as a service.\nThe agent will automatically launch as a Windows service once the system restarts. However, you can choose to launch it right away. To launch it immediately:\nDouble-click the Agent shortcut on your desktop.\nClick Close.\nClick Stop to stop it from running in the background.\nIn Windows, search for the Services application.\nFrom the list of services, find Celigo Agent.\nRight-click the service and click Start to run it right away.\nIf you later decide that you prefer to run it as an executable, in the taskbar or in the background, double-click the Agent shortcut and click Uninstall service.\nSet up your connection\nCreate a connection in integrator.io from Resources \u2192 Connections or by adding a new Source or Destination app in Flow builder.\nFor the Mode, choose On-premise (the default is Cloud).\nChoose your Agent from the drop-down list.\nWhen creating an integrator.io connection that uses an on-premise agent, your settings will depend on whether the agent is installed on the same server as the application or database.\nSame server \u2013 set the Base URI to localhost or 127.0.0.1\nDifferent servers on the same network \u2013 Set the Base URI to the hostname or IP address of the system the application or database is running on.\nYou can run only one instance of an agent on a machine. However, the same agent can be used to talk to multiple databases and applications on the same network as the agent.\nFor example, if you\u2019re running SQL on two virtual machines (VM1 and VM2) and you\u2019ve installed an integrator.io agent on VM1, your agent can access every SQL database on the same machine and any applications in the same network. Say you have an HTTP application running on VM2 and you want to create a connection. You can use the existing agent for that connection since VM1 and VM2 are on the same network.\nSo, as long as VM1 and VM2 are on the same network, you can access any application on both machines.\nExports and imports\nExports and imports that leverage a connection through an on-premise agent do not need any special configuration, since the agent configuration is fully contained within the connection settings. Once the connection is established, you can build flows and integrations as normal.\nFAQs and troubleshooting\nAgent setup and configuration tips\nHow many instances of an agent can I set up?\nYou can run only one instance of the agent on a machine. However, the same agent can be used to allow integrator.io to talk to multiple databases and applications on the same network as the agent.\nDoes the on-premise agent need to be installed on the same machine as the system to be integrated?\nTypically, agents can be installed on any machine within the same network as the server running the system to be integrated. However, there can be exceptions such as when setting up a JDBC connection.\nHow can I reopen the agent window and make changes, after it\u2019s set to run in the background?\nMost agent installations choose to Run in background, in which case the program, On-Premise Agent \u2013 integrator.io, is not visible in the Windows taskbar. To restore the agent user interface and make changes or view activity logs, double-click the program\u2019s shortcut.\nHow can I keep my agent from stopping unexpectedly?\nTo stop your agent from closing unexpectedly, use a small and dedicated Windows server, then run the agent in the background. We recommend you don't use that machine for anything else, though you technically can.\nWhich IP addresses does the agent use to communicate with integrator.io?\nThe agent uses several IP addresses to communicate with integrator.io. If you need to open up outbound traffic for an agent running in your environment:\nWhitelist the integrator.io IP addresses for the U.S. or the EU for the HTTPS protocol.\nWhitelist the agent-extension server IP addresses for the U.S. or the EU for the SSH protocol.\nAudit logs\nTo view activity logs, double-click the program\u2019s shortcut and click View logs in the desktop application, or you can browse the folder C:\\users\\\\AppData\\Roaming\\Agent.\nClone agents in sandbox and production environments\nIf the integrated sandbox and production systems are on the same network, then the best approach is to use the same on-premise agent for connections to both sandbox and production systems. If you choose to use separate agents for each environment, remember that they must be installed on different machines.\nEPERM: Uncaught exception while uninstalling agent service\nAfter uninstalling the agent service, you may get an uncaught exception error. The error will include Error: EPERM: operation not permitted, unlink.\nTo uninstall the agent service after receiving this error, click Uninstall service again."",
  ""date"": ""Thu, 24 Apr 2025 20:14:48 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/360004081771-Integrate-data-through-firewall-with-Windows-on-premise-agent"",
  ""label_names"": [
  ""whitelisting integrator.io IPs"",
  ""IO Agent"",
  ""IO On-Premise Agent"",
  ""Connection using IO Agent"",
  ""Establish connection using IO On-Premise Agent"",
  ""Agent on Mac"",
  ""Agent on Linux"",
  ""Agent on Windows"",
  ""Firewall"",
  ""on-premise agent"",
  ""on premise agent"",
  ""what is a mode?"",
  ""mode on premise"",
  ""mode on-premise"",
  ""on-prem"",
  ""on-premise mode"",
  ""mode"",
  ""What is an on-premise agent?"",
  ""What is an on premise agent?"",
  ""How to use an on-premise agent"",
  ""on prem"",
  ""Windows""
  ],
  ""linkedArticleIds"": [
  ""35906518854299"",
  ""35906518854299"",
  ""226974368"",
  ""360025919171"",
  ""18738569017243"",
  ""115003717028"",
  ""115003717028"",
  ""115003717028""
  ],
  ""numTokens"": 1664.0,
  ""score"": 0.835594952,
  ""section_id"": 28918161546779.0,
  ""source"": ""articles"",
  ""title"": ""Integrate data through firewall with Windows on-premise agent""
  },
  {
  ""articleId"": 33715103651227.0,
  ""content"": ""IBM Db2 is a relational database to store and manage structured data efficiently. It is widely used to process transactions, analyze data, and support AI-driven applications in banking, insurance, and manufacturing.\nSupported platforms\nLinux, Unix, Windows\nAS400\nBefore you begin\nIf you have an on-premise Db2 setup, then you must complete the following steps:\nDownload and install the Java 17 JDK. (We've tested Java 17 JDK. You can use a higher version also.)\nAfter installing the JDK software, configure the PATH and JAVA_HOME variables if you're using Windows on your machine:\nSet the JAVA_HOME variable to the JDK installation directory. For example,\nOn Windows: JAVA_HOME=C:\\Program Files\\Java\\jdk-17.0.1\nSet the PATH variable by appending bin to the path you specified in JAVA_HOME. For example,\nOn Windows: PATH=C:\\Program Files\\Java\\jdk-17.0.1\\bin\nFor Oracle Java, see also Setting JAVA_HOME. Similarly, if you're using OpenJDK or the like, refer to the related documentation to set these variables.\nSet up the connection\nSetting\nInstructions\nName *\nEnter a clear and distinguishable name. Throughout the Celigo platform (integrator.io) imports and exports, you will have the option to choose this new connection. A unique identifier will prove helpful later when selecting it from a list of the connections in your account.\nApplication\nShows the database connector, IBM Db2, that you had earlier selected.\nMode *\nChoose either Cloud (remote) or On-premise (network) server, based on your requirement.\nCloud \u2013 connects to a publicly accessible server application\nOn-premise \u2013 connects to a publicly inaccessible server that has an integrator.io agent installed on it. For On-premise, there\u2019s an additional step to specify an Agent.\nNote\nCeligo on-premise agents work only on Windows and Linux platforms.\nAgent * (enabled when Mode is On-premise)\nSelect an agent from the list.\nTo connect to an on-premise application, the platform (integrator.io) requires that an agent be installed on a networked computer. A single agent can be used by multiple different connections.\nHost *\nEnter the hostname or IP address of the Db2 server.\nSee also, IBM Db2 documentation.\nPort *\nEnter the port used to connect to the server hosting the Db2 database. The default is 50000.\nDatabase *\nEnter the name of the Db2 database.\nUsername *\nEnter your username to authenticate and access the database.\nPassword *\nEnter the password to authenticate and access the database\nUse SSL\nCheck this field if you want to establish a secure connection to the database. This ensures that data in transit is encrypted.\nCertificate (enabled when Use SSL is selected)\nEnter the certificate chain in PEM format only if the database server is using client certificate authentication. For details, see IBM Db2 documentation.\nAfter configuring the settings, click Test Connection. When the connection is verified, you will receive the following message:\nThe connection is now added to your account.\u00a0You can see it listed under Resources \u2192 Connections. If a connection is part of an integration, you can see it listed under Home \u2192 Integration name \u2192 Connections, and you may proceed to\u00a0register the connection."",
  ""date"": ""Thu, 17 Apr 2025 10:55:46 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/33715103651227-Set-up-a-connection-to-IBM-Db2"",
  ""label_names"": [],
  ""linkedArticleIds"": [
  ""6851018128795"",
  ""360007568091""
  ],
  ""numTokens"": 669.0,
  ""score"": 0.830073714,
  ""section_id"": 33713097709339.0,
  ""source"": ""articles"",
  ""title"": ""Set up a connection to IBM Db2""
  },
  {
  ""articleId"": 17596154148123.0,
  ""content"": ""Databricks is a unified analytics platform that provides a collaborative environment for data science, data engineering, and machine learning.\nNote: Celigo uses a JDBC connection to connect to Databricks; we use the databricks-jdbc-2.6.33.jar. If you would like to connect to any application that supports this JAR file but is currently not available in our connector list, you're welcome to submit it as an idea on the Product portal.\nSet up a connection\nAfter you start the connection, configure it in the Create connection panel and complete all of the required * authentication settings:\nSetting\nInstructions\nName *\nEnter a clear and distinguishable name for your connection.Throughout integrator.io imports and exports, you will have the option to choose this new connection. A unique identifier will prove helpful later when selecting it from a list of the connections in your account.\nHost *\nEnter the connection string/URL to the server that hosts the database. For example,\n://:\n/[?\n]jdbc://localhost:5001/[?properties]\nHow to find host details\nSign in to Databricks.\nIn the Account console, click Workspaces.\nSelect a workspace and click Open at the end of the row. Then, enter your credentials to authenticate.\nIn the navigation pane of the workspace, select Compute.\nThen click the required cluster.\nIn the cluster page (default Configuration tab) under Advanced options, select JDBC/ODBC.\nIn the JDBC URL, view and copy details from Databricks. This JDBC URL provides the host value. (Paste this string in the Host field in the Celigo platform. Delete the UID and PWD parts from the JDBC connection string when you paste the value. Also, make sure there's no trailing semi-colon.)\nNote: Do not include your username or password as a part of the host value (JDBC connection string) because it is stored as plain text. Instead, you can provide it in the fields below.\nTip: Your host URL should start with jdbc:databricks:. This is automatically prepended to the value you enter in the Host field if it's not included.\nUsername\nEnter your user name for authentication to Databricks.\nPassword\nEnter your password for authentication to Databricks.\nConfigure properties\nDefine additional configurations for the database connection.\u00a0For example, you can configure the connection timeout if the server you are trying to connect to is slow.\nBorrow concurrency from\nSelect another connection from the list. By default, all data flowing through a connection record is submitted to the respective endpoint application at the\u00a0Concurrency level\u00a0configured for that connection record. There are cases, however, where multiple integrator.io connections need to share the same concurrency level, and this field allows you to specify which connection you would like to borrow from. With borrowed concurrency, the data flowing through both connections will be submitted to the endpoint application together, via a shared concurrency model.\nConcurrency level\nSet this field to limit the number of concurrent HTTP requests (up to 25) allowed by the connection resource at any one time, or leave this field blank to use burst mode. With burst mode, integrator.io will make HTTP requests as fast as possible, with high levels of concurrency.\nConfigure encrypted fields\nStore all sensitive fields required by your imports and exports to access the app you are connecting to. Click the\u00a0Expand window\u00a0(\u00a0\u00a0)\u00a0button to modify the JSON sent to the app in the\u00a0Encrypted\u00a0Advanced Field Editor. For example, enter\u00a0{\""secretKey\"": \""someSecretKeyValue\""}. (You can also retrieve these sensitive values elsewhere, such as\u00a0How to test this connection?, via the example\u00a0handlebars\u00a0format\u00a0{{{connection.http.encrypted.secretKey}}}.) Multiple layers of protection are in place, including AES 256 encryption, to keep your connection\u2019s encrypted fields safe. When editing this form later, you must enter this value again; it is stored only when the connection is saved and never displayed as text.\nAfter configuring the settings, click Test Connection. When the connection is verified, you will receive the following message:\nThe connection is now added to your account.\nYou can see it listed under Resources > Connections.\nIf a connection is part of an integration, you can see it listed under Home > Integration name > Connections, and you may proceed to register the connection."",
  ""date"": ""Fri, 09 May 2025 13:32:55 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/17596154148123-Set-up-a-connection-to-Databricks"",
  ""label_names"": [
  ""connectors"",
  ""connection"",
  ""Lakehouse"",
  ""Databricks""
  ],
  ""linkedArticleIds"": [
  ""4405315105691"",
  ""226974368"",
  ""360043926372"",
  ""360043040652"",
  ""360039326071"",
  ""360007568091""
  ],
  ""numTokens"": 881.0,
  ""score"": 0.827914,
  ""section_id"": 17596116120475.0,
  ""source"": ""articles"",
  ""title"": ""Set up a connection to Databricks""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Download and install the Celigo Agent from the Resources \u2192 Agents page.""
  },
  {
  ""title"": ""Copy the agent token from the agent list in Resources \u2192 Agents.""
  },
  {
  ""title"": ""Create a new SQL database connection in Resources \u2192 Connections using the installed Celigo Agent.""
  }
  ],
  ""thread_id"": ""thread_R46CCSV2SZaa4VfQrfqpwO1n"",
  ""vector_duration"": 1.69
 }",,,,,,,,,,,,,,,,
Copilot_58,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,Imports_001.json,"{
  ""answer"": ""To import multiple things together in Celigo's integrator.io platform while you're on the `Resources - Imports` page, you can use the **\""Insert Many\""** method described in the documentation. Here's how:\n\n1. **Establish Your Connections**:\n  - Ensure that you have already set up an FTP connection to retrieve your data and a MongoDB connection where the data will be imported.\n\n2. **Prepare Your Data**:\n  - Format your FTP data appropriately. For example, your data should follow a structure like this:\n  ```json\n  { \n \""orderID\"": \""TAG-Address-1\"", \n \""FirstName\"": \""John\"", \n \""LastName\"": \""Miller\"", \n \""ItemName\"": \""ACC00002\"", \n \""Quantity\"": \""100\"", \n \""Amount\"": \""280\"", \n \""Notes\"": \""My notes here\"", \n \""units\"": \""100\"", \n \""PO#\"": \""656\"", \n \""Date\"": \""07/15/2017\"", \n \""id\"": \""1\"" \n  }\n  ```\n  - This ensures that your data can be processed correctly.\n\n3. **Set Up the Import**:\n  - On the `Resources - Imports` page, add a new import specifically for MongoDB.\n  - Fill in the import details:\n  - **Name**: Provide a name for your import.\n  - **Description**: Add a brief description for clarity.\n  - **Application**: Select `MongoDB`.\n  - **Connection**: Choose the MongoDB connection you've already established.\n  - **Method**: Select **\""Insert Many\""**. This method will allow you to import all the data coming from the FTP export at once.\n  - **Collection**: Specify the name of the MongoDB collection to which the multiple items will be imported.\n\n4. **Map Your Fields**:\n  - Click **Next** and on the Map page, click **Launch Field Mapper**.\n  - Use the \""Template\"" section to map the fields from your FTP data to the MongoDB fields.\n  - Use the \""Preview\"" section to ensure the mapping is correct.\n\n5. **Save and Import**:\n  - After mapping the fields, click **Save**.\n  - The data from the FTP export will now be imported into the MongoDB collection in bulk.\n\nBy using the **Insert Many** method and configuring your field mapping appropriately, you can handle importing multiple items together effectively.\n\nIf you need further clarity or assistance, let me know!"",
  ""completion_tokens"": """",
  ""openai_duration"": 12.08,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 360011884271.0,
  ""content"": ""Before you can import\nThere are a couple of things you'll need to understand and have in order to be able import your data.\nEstablish your connections\nYou will need to set up an FTP and MongoDB connection before you can configure your import. Make sure you understand that process.\nPut your data in the right format\nTo to pull your FTP data into MongoDB, it will need to be in a specific format.\nSample Data\n{\u00a0 \u00a0\""orderID\"": \""TAG-Address-1\"",\u00a0 \u00a0\""FirstName\"": \""John\"",\u00a0 \u00a0\""LastName\"": \""Miller\"",\u00a0 \u00a0\""ItemName\"": \""ACC00002\"",\u00a0 \u00a0\""Quantity\"": \""100\"",\u00a0 \u00a0\""Amount\"": \""280\"",\u00a0 \u00a0\""Notes\"": \""My notes here\"",\u00a0 \u00a0\""units\"": \""100\"",\u00a0 \u00a0\""PO#\"": \""656\"",\u00a0 \u00a0\""Date\"": \""07/15/2017\"",\u00a0 \u00a0\""id\"": \""1\"" }\nImport data into MongoDB\n1. Enter appropriate details in the following fields.\nField Name\nDescription\nName\nEnter a name for the MongoDB import that you are about to create. For example, MongoDB Import.\nDescription\nEnter a brief description of what MongoDB is importing.\nApplication\nSelect MongoDB.\nConnection\nSelect the MongoDB connection that you have already created.\nMethod\nSelect the method to use for adding or updating documents in your MongoDB instance.\nFor example, you can select Insert Many as you may want to import all the data that is coming from the FTP export.\nFor more information on the available methods please refer to the MongoDB documentation.\nCollection\nEnter the MongoDB collection name into which you want to import the export data. For example, MyCollection.\n2. Click Next.\n3. On the Map page, click Launch Field Mapper.\n4. On the MongoDB Data Builder window,\nUnder \""Template\"": You can specify the fields that you want to map based on the Sample Data received.\nUnder \""Preview\"": You can see the data that is mapped.\n5. Click Save."",
  ""date"": ""Mon, 27 Jun 2022 20:38:09 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/360011884271-Import-FTP-data-into-MongoDB"",
  ""linkedArticleIds"": [],
  ""numTokens"": 417.0,
  ""score"": 0.814896524,
  ""section_id"": 360007898172.0,
  ""source"": ""articles"",
  ""title"": ""Import FTP data into MongoDB""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Set up an FTP and MongoDB connection to prepare for the import.""
  },
  {
  ""title"": ""Create a new import and select \u201cInsert Many\u201d as the method for bulk imports.""
  },
  {
  ""title"": ""Map your fields using the Field Mapper and preview the data before saving.""
  }
  ],
  ""thread_id"": ""thread_LTy5AGhOmQPg0uNMcigUypbQ"",
  ""vector_duration"": 1.24
 }",,,,,,,,,,,,,,,,
Copilot_59,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,Mappings_001.json,"{
  ""answer"": ""To convert an object to a stringified form and map it to a field in the **'Flow Builder - Edit Mapping: Mssql Imports'** page on Celigo's integrator.io platform, follow these steps:\n\n### Steps to Stringify and Map an Object:\n1. **Locate the Object in the Source Field:**\n  - In the Mapper interface, find the source object you want to stringify. You can use dot-notation to navigate to nested fields if needed (e.g., `{{ShippingAddress}}` to access the `ShippingAddress` object).\n\n2. **Use a Handlebars Expression to Stringify the Object:**\n  - In the destination field where you want to map the stringified object, use the `JSON.stringify` function within a handlebars expression.\n  - For example:\n  ```handlebars\n  {{JSON.stringify(ShippingAddress)}}\n  ```\n  - This will convert the `ShippingAddress` object into its stringified JSON form.\n  - Example output:\n  ```json\n  {\""Name\"":\""JohnSmith\"",\""Phone\"":12345678,\""AddressLine1\"":\""123 Main St\"",\""City\"":\""Big City\"",\""StateOrRegion\"":\""State\""}\n  ```\n\n3. **Map the Stringified Object:**\n  - Assign this stringified value to the specific destination field where you need it to appear.\n\n4. **Adjust Data Type of the Destination Field:**\n  - If the destination field expects a string, ensure the data type of the field is set to **String**.\n  - To change the data type:\n  - Click the **Settings (gear icon)** next to the destination field while mapping.\n  - Select **string** from the \""Data Type\"" dropdown.\n\n5. **Test the Mapping:**\n  - Use the \""Preview\"" feature to validate that the object is being correctly stringified and appears as expected in the mapped field on the output.\n\n### Example Use-Case:\nLet\u2019s say you want to stringify a `ShippingAddress` object from the source JSON:\n```json\n\""ShippingAddress\"": {\n \""Name\"": \""JohnSmith\"",\n \""Phone\"": 12345678,\n \""AddressLine1\"": \""123 Main St\"",\n \""City\"": \""Big City\"",\n \""StateOrRegion\"": \""State\""\n}\n```\n\n- In the mapper, set the destination field to:\n ```handlebars\n {{JSON.stringify(ShippingAddress)}}\n ```\n\n- Output:\n ```json\n \""{\\\""Name\\\"":\\\""JohnSmith\\\"",\\\""Phone\\\"":12345678,\\\""AddressLine1\\\"":\\\""123 Main St\\\"",\\\""City\\\"":\\\""Big City\\\"",\\\""StateOrRegion\\\"":\\\""State\\\""}\""\n ```\n\nThe resulting stringified object will then be forwarded to the destination application as a JSON string.\n\n### Key Tips:\n- The `JSON.stringify` function is key to converting objects into strings for mapping.\n- Ensure that the destination field can handle string data as its type.\n- Test the flow to ensure the mapped data looks correct in the destination app.\n\nIf you have any specific object or mapping context you'd like to clarify further, let me know!"",
  ""completion_tokens"": """",
  ""openai_duration"": 20.19,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 4409527888923.0,
  ""content"": ""JSON is a JavaScript Object Notation text format for storing and transporting data. With JSON you can transfer large amounts of data between applications without losing or having to transform the data.\nJSON fundamentals\nJSON syntax is organized in name-value pairs.\n{\""Name\"": \""Value\""}\nThe name and its value are each:\nwrapped in double quotes (\""\"")\nseparated by a colon (:)\nsurrounded by curly braces ({ }).\nNumbers are not surrounded by double quotes.\n{\""Number\"": 100}\nA JSON object is a structure that includes multiple name-value pairs.\n{\n\""Object\"": {\n\""Name\"": \""Value\"",\n\""Number\"": 100,\n\""Name\"":\""Value\""\n}\n}\nJSON objects are also surrounded by curly braces {}, and each item in the set must be separated by a comma. This is a JSON object named \""employee\"" that contains three fields: \""name\"", \""age\"", and \""city\"".\n{\n\""employee\"": {\n\""name\"": \""John\"",\n\""age\"": 30,\n\""city\"": \""New York\""\n}\n}\nA name-value pair can contain multiple values within an array. Arrays are lists surrounded by brackets [ ], and the values included in an array are separated by commas.\n{\n\""Names\"":[\""Sally\"", \""John\"", \""Ben\""]\n}\nArrays can contain fields.\n[\n\""Name\"": \""Sally\"", \""Name\"": \""Joe\"", Name: \""Eunice\""\n]\nArrays can contain objects.\n[\n{\n\""Object1\"": {\n\""Name\"": \""Value\"",\n\""Number\"": 100,\n\""Name\"":\""Value\""\n}\n},\n{\n\""Object2\"": {\n\""Name\"": \""Value\"",\n\""Number\"": 100,\n\""Name\"":\""Value\""\n}\n},\n{\n\""Object3\"": {\n\""Name\"": \""Value\"",\n\""Number\"": 100,\n\""Name\"":\""Value\""\n}\n}\n]\nArrays can contain arrays.\n{\n\""ContainerArray\"": [\n\""NestedArray1\"": [\n\""field1\"": \""value\"",\n\""field2\"": \""value2\""\n],\n\""NestedArray2\"": [\n\""field1\"": \""value\"",\n\""field2\"": \""value2\""\n]\n]\n}\nObjects can also contain objects and arrays.\n{\n\""ContainerObject1\"": {\n\""NestedArray1\"": [\n\""field1\"": \""value\"",\n\""field2\"": \""value2\""\n],\n\""NestedArray2\"": [\n\""field1\"": \""value\"",\n\""field2\"": \""value2\""\n],\n{\n\""Object1\"": {\n\""Name\"": \""Value\"",\n\""Number\"": 100,\n\""Name\"":\""Value\""\n}\n}\n}\nTo learn more about JSON, visit w3schools.com.\nData types\nWhen creating destination fields from source fields during mapping, the data type assigned to each destination field determines characteristics of the JSON structure. The destination API expects some fields to be numbers, some to be strings, and usually all of them must be inconveniently nested within complex objects and arrays. It is important for you to have a model of the JSON structure you want to build readily available to reference. You can usually find some version of this in the destination API\u2019s developer documentation.\nSet the data type of a JSON element\nClick the Settings () button to the right of any field mapping to change its data type.\nSelect an option from the Data type list.\nUsing Mapper 2.0, the data type displays to the right of the name of each JSON element.\nClick the data type to select from the following:\nstring\nA string value is any combination of characters surrounded by double quotes.\n{\n\""Name\"": \""John\""\n}\nnumber\nA number value is any integer or floating point. Number values do not use quotes.\n{\n\""Number\"": 43\n}\nBoolean\nA Boolean value can either be true or false. Boolean values do not use quotes.\n{\n\""active\"": false\n}\nobject\nAn object is a set of name and value pairs surrounded by curly braces. The following JSON object contains three fields: \""name\"", \""age\"", and \""city\"".\n{\n\""Employee\"":{\n\""name\"": \""John\"",\n\""age\"": 30,\n\""city\"": \""New York\""\n}\n}\n[string]\nA string array is a list of string values separated by commas and surrounded in square brackets.\n{\n\""employees\"": [\""John\"", \""Anna\"", \""Peter\""]\n}\n[number]\nA number array is a list of number values separated with commas and surrounded in square brackets.\n{\n\""employees\"": [98, 99, 100]\n}\n[Boolean]\nA Boolean array is a list of Boolean values separated by commas and surrounded by square brackets. The values in a Boolean array do not use quotes.\n{\n\""isEmployed\"": [false, true, false]\n}\n[object]\nAn array object uses an array structure that contains a series of comma-separated items.\n{\n\""order\"": [\n{\n\""line_items\"": \""03e7eb26f1\""\n},\n{\n\""line_items\"": \""p3e66626l1\""\n}\n]\n}\nJSON record or row structures\nValid JSON structures come in one of two forms: record or row.\nJSON record structures\nRecord structures open and close with curly braces {}. JSON objects are record structures.\nThis is an example of a record structure:\n{\n\""order\"": {\n\""line_items\"": [\n{\n\""customerid\"": \""03e7eb26f1\"",\n\""quantity\"": 0\n},\n{\n\""listOfThings\"": {\n\""first_name\"": \""Joe\"",\n\""last_name\"": \""Smith\""\n}\n]\n}\nThe defining characteristics of a record structure are the opening and closing curly braces {}.\nJSON row structures\nRow structures open and close with square brackets []. Rows are arrays of JSON objects (or record structures).\nThis is an example of a row structure:\n[\n\""order\"": {\n\""line_items\"": [\n{\n\""customerid\"": \""03e7eb26f1\"",\n\""quantity\"": 0\n},\n{\n\""listOfThings\"": {\n\""first_name\"": \""Joe\"",\n\""last_name\"": \""Smith\""\n}\n]\n]\nThe defining characteristics of a row structure are the opening and closing brackets []. When you group your export data, the data passes through the rest of the flow in row structure.\nJSON in integrator.io\nintegrator.io primarily uses JSON to transfer data. While you can create and run your flows intuitively through the integrator.io interface, you will benefit from some familiarity with JSON to map your data.\nData mapping allows you to define how integrator.io formats the source application\u2019s data to fit the schema of the destination application. Simply put, data mapping is what gets your data from one application to another with minimal errors. When mapping your data, you might need to rely on handlebars expressions using handlebar syntax. These expressions help you to:\nMap export and import application fields\nPerform dynamic arithmetic calculations on exported values\nDynamically encode and decode data during integration\nReferencing fields with handlebars expressions\nA simple handlebars expression uses the following syntax:\n{{expression}}\nLet\u2019s look at example data for importing Microsoft Dynamics 365 Business Central customers. Here is an example customer record:\n{\n\""Person\"":\""Buyer\"",\n\""BuyerEmail\"":\""email@email.com\"",\n\""BuyerName\"":\""JohnSmith\"",\n\""ShippingAddress\"": {\n\""Name\"":\""JohnSmith\"",\n\""Phone\"":12345678,\n\""AddressLine1\"": \""123 Main St\"",\n\""City\"": \""Big City\"",\n\""StateOrRegion\"": \""State\""\n}\n}\nTo reference this customer data with handlebars, you can use:\n{{Person}} {{BuyerEmail}}\nIn context, you are requesting this information:\n{\n\""Person\"":\""Buyer\"",\n\""BuyerEmail\"":\""email@email.com\"",\n\""BuyerName\"":\""JohnSmith\"",\n\""ShippingAddress\"": {\n\""Name\"":\""JohnSmith\"",\n\""Phone\"":12345678,\n\""AddressLine1\"": \""123 Main St\"",\n\""City\"": \""Big City\"",\n\""StateOrRegion\"": \""State\""\n}\n}\nThe final product would be:\nBuyer email@email.com\nYou can use dot-notation to gain access to nested properties, objects, or arrays.\n{{ShippingAddress.Name}} or {{ShippingAddress.City}}\nIn context, you are requesting this information:\n{\n\""Person\"":\""Buyer\"",\n\""BuyerEmail\"":\""email@email.com\"",\n\""BuyerName\"":\""JohnSmith\"",\n\""ShippingAddress\"": {\n\""Name\"":\""JohnSmith\"" ,\n\""Phone\"":12345678,\n\""AddressLine1\"": \""123 Main St\"",\n\""City\"": \""Big City\"" ,\n\""StateOrRegion\"": \""State\""\n}\n}\nThe final product would be:\nJohnSmith or Big City\nYou can use handlebars block references when mapping fields. The following example shows the complete mapping configuration for Amazon Seller Central (source app) \u2013 Microsoft Dynamics 365 Business Central (destination app).\nExample mapping data with Mapper 1.0\nThe handlebars block reference expression demonstrated below conditionally maps BuyerName to displayName.\n{{#if BuyerName}}{{BuyerName}}{{else}}{{ShippingAddress.Name}}{{/if}}\nThe expression works as follows:\nIf the BuyerName field is not null (contains a value)...\n{{#if BuyerName}}{{BuyerName}}{{else}}{{ShippingAddress.Name}}{{/if}}\nThen use the BuyerName value for the displayName value in the destination app.\n{{#if BuyerName}}{{BuyerName}}{{else}}{{ShippingAddress.Name}}{{/if}}\nOtherwise...\n{{#if BuyerName}}{{BuyerName}}{{else}}{{ShippingAddress.Name}}{{/if}}\nUse the ShippingAddress.Name field.\n{{#if BuyerName}}{{BuyerName}}{{else}}{{ShippingAddress.Name}}{{/if}}\nThird-party apps using JSON\nVarious third-party clients can send, receive, and modify JSON data from integrator.io. Postman is the most common. Third-party clients may lack some of the benefits of the integrator.io provides, but you can access your account via a third-party client using the following endpoint:\nhttps://api.integrator.io/v1\nYou can also use integrator.io REST API resources . Use bearer token authentication to access these resources from third-party apps.\nExample Postman call requesting a connection\nJSON transformation examples\nWhile some applications transfer JSON data unchanged, integrator.io sometimes parses data from other formats (such as CSV, XML, EDI, etc.) into JSON.\nCSV to JSON\nComma-separated value (CSV) files are often and easily transformed into JSON data to be populated in an application. Each row of the CSV file contains a data record. Each record consists of one or more fields, separated by commas.\nThese files look like this:\nPerson, BuyerEmail, BuyerName\nShopper,email.at.email.com,JohnSmith\nShopper,email.at.email.com,JaneSmith\nAnd integrator.io\u2019s JSON output looks like this:\n{\n\""Person\"" : \""Shopper\"" ,\n\""BuyerEmail\"" : \""email.at.email.com\"" ,\n\""BuyerName\"" : \""JohnSmith\""\n}\n{\n\""Person\"" : \""Shopper\"" ,\n\""BuyerEmail\"" : \""email.at.email.com\"" ,\n\""BuyerName\"" : \""JaneSmith\""\n}\nXML to JSON\nLike JSON, eXtensible Markup Language (XML) is a way to send and receive data via an API. XML and JSON are similar. An XML file looks something like this:\nShopper\nemail.at.email.com\nJohnSmith\nShopper\nemail.at.email.com\nJaneSmith\nAnd when converted to JSON in integrator.io it looks similar to this:\n{\n\""Person\"" : \""Shopper\"" ,\n\""BuyerEmail\"" : \""email.at.email.com\"" ,\n\""BuyerName\"" : \""JohnSmith\""\n}\n{\n\""Person\"" : \""Shopper\"" ,\n\""BuyerEmail\"" : \""email.at.email.com\"" ,\n\""BuyerName\"" : \""JaneSmith\""\n}\nNetSuite subrecords to JSON\nSubrecords hold information about a parent record and are meaningful only within that context. The subrecord must be created, edited, removed, or viewed from its standard NetSuite record. These subrecords must be configured and mapped in integrator.io. You can map subrecords imported into NetSuite natively through integrator.io.\nNetSuite subrecords\nWhen transformed, the JSON structure can look like this:\n[\n{\n\""customer_name\"" : \""Test Customer\"" ,\n\""email\"" : \""celigo@test.com\"" ,\n\""line_items\"" : [\n{\n\""item_name\"" : \""Inventory Serial Item1\"" ,\n\""item_amount\"" : 245 ,\n\""item_qty\"" : 2 ,\n\""item_location\"" : 1 ,\n\""inv_detail\"" : [\n{\n\""serialnumber\"" : \""inv01\"" ,\n\""qty\"" : 1\n},\n{\n\""serialnumber\"" : \""inv02\"" ,\n\""qty\"" : 1\n}\n]\n},\n{\n\""item_name\"" : \""Inventory Serial Item2\"" ,\n\""item_amount\"" : 542 ,\n\""item_qty\"" : 1 ,\n\""item_location\"" : 1 ,\n\""inv_detail\"" : [\n{\n\""serialnumber\"" : \""inv03\"" ,\n\""qty\"" : 1\n}\n]\n}\n]\n}\n]\nNotice the line items, Inventory Serial Item1 and Inventory Serial Item2, with a quantity of 2 and 1 respectively. These are NetSuite serialized items, as is apparent from the associated serial numbers.\nReferences and formatting\nRead more on JSON.\nRead more on XML.\nRead more on handlebars expressions and syntax.\nRead more on handlebars expressions and syntax in integrator.io.\nClean up your JSON code to make it human-readable.\nAdd a file extension to your browser to view JSON code."",
  ""date"": ""Wed, 25 Sep 2024 13:55:11 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/4409527888923-Work-with-JSON-data-in-integrator-io"",
  ""linkedArticleIds"": [],
  ""numTokens"": 2745.0,
  ""score"": 0.797715425,
  ""section_id"": 360007773432.0,
  ""source"": ""articles"",
  ""title"": ""Work with JSON data in integrator.io""
  },
  {
  ""articleId"": 10218893744539.0,
  ""content"": ""When building a destination object array with Mapper 2.0 , you can customize the settings for each object within the array. To create a destination object array, give the destination object a name and set its data type to [object].\nIn the Source field, enter the JSON path to each source object (or object array) that you want to include in the destination object array separated by commas.\nIn this example, the destination object array references two objects and an object array from the source data:\n$.companies (source object)\n$.location (source object)\n$.social_profiles.data[*] (source object array)\nEach object in the destination object array is represented as a tab for you to independently configure. Click Settings () to define how each object will be mapped.\nCopy a source object as-is\nUse these steps to copy a source object to the destination object array without changing the structure of the object. In this example, the first object ($.companies) will be copied from the source data as-is, and if the source field has no value, \""null\"" will be the default value.\nVerify that the $.companies tab is selected.\nSet the Copy an object array from the source as-is to Yes.\nSelect Use null as default value from the Action to take if source field has no value drop-down list.\nIf you save these settings and return to Mapper 2.0, you are unable to modify the settings for this object since it is set to be copied as-is.\nOn preview, the Output window displays the first object in the ExampleObjectArray exactly as it is in the source data:\n\""ExampleObjectArray\"": [\n{\n\""type\"": \""list\"",\n\""data\"": [],\n\""url\"": \""/contacts/5fe44bcb4996d972d30c6f5e/companies\"",\n\""total_count\"": 0,\n\""has_more\"": false\n},\nCreate an object in mapping\nUse these steps to build a custom object in the destination object array using fields from an object in the source data. In this example, the second object in the destination object array is the object $.location.\nVerify that the $.location tab is selected.\nSet the Copy an object array from the source as-is to No.\nIf you save these settings and return to Mapper 2.0, you can add all fields included in this object.\nIn this example, the second object in the destination object array will have three string fields: Country , Region , and City. These three fields are automatically added to all other objects in the destination object array, but none of the other objects have source fields mapped.\nOn preview, the Output window displays the second object in the ExampleObjectArray customized according to our preferences:\n\""ExampleObjectArray\"": [\n{\n\""type\"": \""list\"",\n\""data\"": [],\n\""url\"": \""/contacts/5fe44bcb4996d972d30c6f5e/companies\"",\n\""total_count\"": 0,\n\""has_more\"": false\n},\n{ \""Country\"": \""India\"", \""Region\"": \""Telangana\"", \""City\"": \""Hyderabad\"" }\nThe third object in the ExampleObjectArray references the $.social_profiles.data[*] source object array for its data. Notice that since we've previously added three string fields to the $.location object, this third object already has those three fields added.\nSince most object arrays use parallel field structures accross all objects, any field you add to an object in a destination object array is automatically added to all other objects in that object array. You will still have to map each field in every object to a specific source, and you can manually delete any field from any object in the object array."",
  ""date"": ""Wed, 29 May 2024 19:13:21 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/10218893744539-Mapper-2-0-Include-multiple-objects-in-a-destination-object-array"",
  ""linkedArticleIds"": [],
  ""numTokens"": 737.0,
  ""score"": 0.793385684,
  ""section_id"": 26134353584923.0,
  ""source"": ""articles"",
  ""title"": ""Mapper 2.0: Include multiple objects in a destination object array""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Map the stringified object to the intended destination field in Mapper 2.0.""
  },
  {
  ""title"": ""Test the mapping preview to confirm the stringified output is correct for the destination.""
  },
  {
  ""title"": ""Adjust destination field data type to string and ensure proper JSON handling.""
  }
  ],
  ""thread_id"": ""thread_re1XtraQDqYsc1spA7a94XwK"",
  ""vector_duration"": 2.09
 }",,,,,,,,,,,,,,,,
Copilot_60,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,OutputFilter_001.json,"{
  ""answer"": ""You are currently on the **'Flow builder - Define output filter - Conditions'** page in Celigo's integrator.io platform. \n\nThis page allows you to set conditional rules that filter the data being processed in your integration flow. Specifically, on this page, you define the **output filter conditions** that determine which records should proceed to the next step of the flow and which should be excluded. \n\n### Key Features of This Page:\n- You can set **conditions** based on specific field values or other criteria within your source data.\n- These conditions are part of the **data filtering process** and ensure that only relevant data passes through your flow.\n- For example, if you're syncing orders from one system to another, you might filter out orders with a \""Pending\"" status and only allow \""Completed\"" orders.\n\n### How Output Filter Conditions Work:\n1. **Define Data Rules**: You create expressions or statements to define filtering criteria. For example, `Field X = [specific value]`.\n2. **Control Flow Outputs**: The system evaluates only data that matches your specified conditions and sends it along to the next step.\n3. **Data Validation**: This helps you limit the scope of your integration and prevents processing unnecessary or invalid records.\n\nIf you're building or editing an integration flow, this page is a critical step to tailor the data that flows between systems and ensure efficient processing.\n\n### What You Should Do on This Page:\n- Review your integration's requirements to identify what data you want to include or exclude.\n- Use the condition builder to set the appropriate filtering logic.\n- Test the conditions to ensure they work as expected and only allow intended data to pass through.\n\nIf you need further help crafting the right conditions, feel free to ask!"",
  ""completion_tokens"": """",
  ""openai_duration"": 13.74,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 24663121650459.0,
  ""content"": ""The Developer portal is where you can view the API catalog, create an application, and subscribe to existing APIs. Contact your API publisher's support team to receive the link to view the Developer portal.\nNote\nPublishers can customize the developer portal as needed. The example images below reflect the standard layout that Celigo has created. However, it may change based on your API publisher.\nHome page\nThe home page is fairly straightforward. From here, you can search for and explore APIs.\nYou should become familiar with three main tabs: Dashboard, Catalog, and Applications. Additionally, you can change various settings by clicking your email address at the top right-hand side of the screen.\nDashboard\nYour Dashboard shows you all your existing applications and subscriptions. From here, you can view, create, or edit applications. To view a full list of your applications, click See your applications.\nIf you scroll down, you'll also be able to see your existing subscriptions, including the application they're associated with and the API plan.\nTip\nClick on a subscription in your Dashboard to see an analytics overview that includes:\nMinimum response time\nMaximum response time\nAverage response time\nRequests per second\nTotal\nCatalog\nYour Catalog shows you a list of all the available APIs you can subscribe to. This includes any Featured, Starred, or Trending APIs. Your API publisher can create any number of internal or external categories that you can explore. Celigo, for example, has Internal Only, Partner Services, and Customer Services catalogs. You can also search for an API or browse all available APIs.\nApplications\nTo subscribe to an API, you'll need to create an application. Learn more about the concept of an application. After you've created your application and registered it, you can see its various features.\nGeneral info and Metadata\nThe General info tab shows you the basics of your application. This includes your domain, application type, and Client ID.\nThe Metadata tab lets you add important metadata to your application. The formats allowed are:\nString\nNumeric\nBoolean\nDate\nMail\nURL\nSubscriptions and Members\nIn your Subscriptions tab, you can see all the APIs your application is subscribed to, including the plans, security type, who subscribed to it, and the subscription status.\nYour members are those who have been added to your application. This may include an \""Inherited members\"" list based on any groups your API publisher has created. You cannot delete inherited members.\nAnalytics, logs, notifications\nYou can configure analytics, logs, and notifications through your application. Your analytics and logs are generally at the application level, though you can configure advanced settings to narrow the scope to a single API. You can filter both by length of time, including a specific start and end date.\nMost of your notifications will be based on subscriptions, such as when a subscription is accepted, rejected, or paused. However, you can also receive notifications when a support ticket related to your application has been created.\nNote\nYour notifications are at the application level. You can create Alerts to notify you when something occurs with your subscribed APIs.\nAlerts\nAlerts are based on the APIs you've subscribed to. You can configure alerts for all APIs or for a specific API, including alerts based on an HTTP status or average response time.\nAPI\nYou should learn more about APIM features before creating an application and subscribing to an API. In this example, we're using a simple SpaceX API. After finding an API, you'll see the API home page, a Documentation tab, a Contact tab, and a Tickets tab. In some cases, the home page may be the Documentation tab.\nDocumentation tab\nThe Documentation tab allows you to review the API's specifications. If your API publisher has allowed it, you can use this section to test the API using the Try it feature.\nBelow is an example of the Try it content.\nContact tab\nThe Contact tab allows you to send a message to the API publisher and ask questions about the API.\nTickets tab\nYou can see tickets your API publisher has created in the Tickets tab."",
  ""date"": ""Thu, 13 Mar 2025 18:29:28 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/24663121650459-Navigate-the-APIM-Developer-portal"",
  ""label_names"": [],
  ""linkedArticleIds"": [
  ""21203462581403"",
  ""20110543786523"",
  ""21179125401755"",
  ""21179125401755""
  ],
  ""numTokens"": 823.0,
  ""score"": 0.761983812,
  ""section_id"": 21179001937307.0,
  ""source"": ""articles"",
  ""title"": ""Navigate the APIM Developer portal""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Define a condition to filter out unnecessary data based on your integration requirements.""
  },
  {
  ""title"": ""Test your filter conditions to ensure only the desired records proceed to the next flow step.""
  },
  {
  ""title"": ""Save the output filter configuration to apply the conditions to your flow.""
  }
  ],
  ""thread_id"": ""thread_9HPT4Ixq1DnUBRjyx4oTbg0k"",
  ""vector_duration"": 1.78
 }",,,,,,,,,,,,,,,,
Copilot_61,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,Hooks_001.json,"{
  ""answer"": ""You are currently on the **'Flow Builder - Hooks Configuration'** page in Celigo's integrator.io platform, which is used to configure **hooks** for a flow. In this context, hooks refer to custom code that you can execute at specific stages of a flow, allowing you to modify the behavior of export and import processes dynamically.\n\n### What is this?\nHooks are small pieces of custom code that run during different stages of a flow. They allow you to alter, transform, validate, and enrich data at specific points of the flow's execution. For example:\n- Modifying the data fetched during export or before import.\n- Running validations or transformations on data.\n- Attaching additional information by looking up extra details from another application.\n- Executing custom logic directly in NetSuite by using SuiteScript hooks.\n\nEach hook has a specific purpose and is called at a particular stage of the flow. The page you are on allows you to configure these hooks and define how the data should be handled during the execution of the flow.\n\n---\n\n### When to use each?\n\nIn a flow, you can use hooks to customize behavior in both **export** and **import** processes. Below is a description of each hook type and when you might want to use them:\n\n---\n\n#### **Export Hooks**\n1. **preSend**:\n  - **What it does**: This hook is executed after the data is collected during an export and before it is sent to the next stage in the flow.\n  - **When to use**: \n  - If you need to add more information to the exported data using lookups or transformations before passing it on.\n  - If you want to clean or validate the data before it moves further in the flow.\n  - **Example**: For a NetSuite export, you can use a preSend hook to fetch additional details (like customer email or address) using a search and attach them to the exported records.\n\n---\n\n#### **Import Hooks**\nThere are three types of import hooks. These allow advanced customization of data before the records are mapped or imported into the destination system.\n\n1. **preMap**:\n  - **What it does**: Modifies data before it is mapped to the destination system.\n  - **When to use**: \n  - Modify raw data received from the source system before mapping fields.\n  - Perform validations, reorganize data formats, or handle errors coming from the source.\n  - **Example**: Renaming fields or combining multiple properties before mapping them to the destination.\n\n2. **postMap**:\n  - **What it does**: Modifies the data after it is mapped to the destination's format but before it is submitted for importing.\n  - **When to use**:\n  - Transform the mapped data, such as converting case formats or checking for omitted data.\n  - **Example**: Capitalizing customer names or appending a suffix after mapping.\n\n3. **postSubmit**:\n  - **What it does**: Runs after the data has been imported into the destination system.\n  - **When to use**:\n  - Perform tasks dependent on successful submission, such as triggering additional flows, recording activity logs, or notifications.\n  - **Example**: Logging a success message or notifying someone after data is successfully imported into NetSuite.\n\n---\n\n#### **SuiteScript Hooks** (Only for NetSuite Exports and Imports)\nSuiteScript hooks are specific to NetSuite integrations and allow you to write custom logic directly in NetSuite using SuiteScript.\n\n1. **preSend (Export)**:\n  - NetSuite-specific version of the preSend hook that runs directly within a NetSuite RESTlet. Use this if your export requires custom NetSuite SuiteScript logic.\n\n2. **NetSuite Import Hooks**:\n  - These SuiteScript hooks (preMap, postMap, postSubmit) perform the same tasks as their counterparts above but execute directly in NetSuite for better performance.\n  - **When to use**: If you want to leverage NetSuite's internal API for handling imports (e.g., modifying NetSuite records or running searches).\n\n---\n\n### Key Points to Consider\n1. **When to use Hooks:**\n  - Use hooks if you need to add **custom logic** to your flow beyond the built-in configurations.\n  - Hooks should be used to perform actions such as validations, lookups, or transformations between flow steps.\n\n2. **Script or SuiteScript?**\n  - Choose a standard integrator.io **script** when you want the code to run on Celigo's platform.\n  - Choose **SuiteScript hooks** for NetSuite-specific use cases where logic must be directly executed within NetSuite (e.g., leveraging NetSuite RESTlets or Saved Searches).\n\n3. **Performance and Governance:**\n  - Keep in mind that hooks can impact performance, especially in NetSuite. Pay attention to API governance points and execution time limits (e.g., 5 minutes per RESTlet call for NetSuite hooks). Optimize your flow design, such as lowering the **pageSize** of your export, to avoid overloading your system.\n\n---\n\n### Actionable Steps for You\n1. Determine where in your flow customization is needed:\n  - Before sending data in the export? Use **preSend**.\n  - Before or after mapping data in the import? Use **preMap** or **postMap**.\n  - After submitting data to the target? Use **postSubmit**.\n\n2. Decide what type of code (JavaScript, SuiteScript, etc.) you will use:\n  - For non-NetSuite applications, use **JavaScript** or **your own hosted code (Stack)**.\n  - For NetSuite-specific integrations, consider SuiteScript for better integration within the system.\n\n3. Write or upload your hook code in the appropriate spot in the **Hooks Configuration** panel.\n\n4. Test and debug your hooks by reviewing logs to ensure data behaves as expected during each step of the flow.\n\n---\n\nIf you need more specific guidance about implementing a type of hook in your flow or writing its code, let me know!"",
  ""completion_tokens"": """",
  ""openai_duration"": 29.67,
  ""prompt_tokens"": """",
  ""suggestions"": [
  {
  ""title"": ""Review hook types and decide which customization fits your flow.""
  },
  {
  ""title"": ""Write and upload preSend or preMap hook code.""
  },
  {
  ""title"": ""Test the flow and check logs for hook performance.""
  }
  ],
  ""thread_id"": ""thread_wXolol4l8aMTSq17hiioHRYB"",
  ""vector_duration"": 1.73
 }",,,,,,,,,,,,,,,,
Copilot_62,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,Error_001.json,"{
  ""answer"": ""To debug and resolve the error you're encountering on the **Flow builder - Errors: Imports** page, follow these steps based on the documentation provided:\n\n---\n\n### 1. **Locate and Understand the Error**\n  - On the **Errors** tab of the Flow Builder, review the **error details** for the specific import step where the error occurred.\n  - Key fields to focus on in the error details:\n  - **Error Code**: This unique identifier may provide insight into the cause.\n  - **Error Message**: Describes the issue (e.g., missing mappings, invalid data, or connection errors).\n  - **Error Source**: Identifies which application (e.g., Shopify, NetSuite, or HTTP connector) produced the error.\n  - **Trace Key**: Shows a unique identifier for the record causing the error.\n  - **Classification**: Indicates the type of error (e.g., \""Missing,\"" \""Duplicate,\"" \""Connection\"").\n\n---\n\n### 2. **Use Debugging Tools Specific to the Flow Builder**\n  Utilize these tools to gather more context regarding the import issue:\n  - **Run console**:\n  - The **Run console** shows in-progress details for the current flow run.\n  - Look at the specific step in the flow causing issues and inspect real-time logs or outcomes.\n  - **Diagnostics**:\n  - You can download the diagnostics ZIP file, which includes parent job, child job, and job errors. It can help you pinpoint configuration or other runtime issues.\n  - **Run history**:\n  - Check previous flow runs under **Run history** for patterns (e.g., whether the error is recurring for the same step or data).\n\n---\n\n### 3. **Debug Common Import Errors**\nBelow are approaches for common error scenarios you might find:\n\n#### a. **Mapping Errors (e.g., missing_static_lookup, value_lookup_failed)**\n  - **Error Cause**: A value in the source system does not have a mapping to its equivalent in the destination system.\n  - **Resolution**:\n  - Open the **Mappings** for your flow step (click the gear icon next to the relevant mappings in Flow Builder).\n  - Add or update the missing mappings in either a static lookup, a dynamic lookup, or a transformation.\n  - Retry the error from the **Errors** tab.\n\n#### b. **Validation Errors (e.g., user_error, invalid_key_or_ref)**\n  - **Error Cause**: Missing or invalid values in required fields (e.g., NetSuite fields such as subsidiary, currency, etc.).\n  - **Resolution**:\n  - Check the error message details to find the required field.\n  - Update the source data or modify the Retry Data:\n  - Click **Retry Data** for the failed record, and fix the missing or invalid fields directly in the JSON.\n  - Save the edited data and retry the error.\n\n#### c. **Rate Limit or Connection Errors**\n  - **Error Cause**: Overloaded API limits or broken connections between the Celigo connector and the endpoint application.\n  - **Resolution**:\n  - If it\u2019s an API rate-limit issue, enable the **Auto-recover rate limit errors** in your connection settings.\n  - If the connection to the system is down, retry the error once the system (e.g., NetSuite or Shopify) is back online.\n\n#### d. **Intermittent Errors**\n  - **Error Cause**: Temporary, recoverable third-party application issues.\n  - **Resolution**:\n  - Integrator.io will attempt **auto-retries** up to four times if **Auto-resolve** is enabled.\n  - If retries fail, click **Retry** in the Errors list once the system stabilizes.\n\n---\n\n### 4. **Take Corrective Actions**\n  - Once you\u2019ve debugged the root cause:\n  - **Retry Specific Errors**:\n  - Click the checkbox next to failed records and select **Retry**.\n  - **Resolve Errors**:\n  - If the issue no longer requires resolution (e.g., intermittent errors auto-resolved, data issue fixed), you can mark the errors as **Resolved**, so they no longer appear in the Open errors tab.\n  - **Bulk Actions**:\n  - Use the **Download errors** button to export error details, fix the data (either in the source system or Retry Data), and then re-upload the processed records using the **Upload Processed Errors** button.\n\n---\n\n### 5. **Enable Debug Logs for Imports (Optional)**\n  - If the import involves an HTTP connector, you can enable **debug logs**:\n  - These logs provide detailed request/response communication to see where the system might have failed (e.g., missing data or unsupported input from the source system).\n  - Navigate to the **Settings** of the HTTP connector and enable debugging.\n\n---\n\n### 6. **View Error Trends**\n  - If errors persist:\n  - Use the Analytics tab to monitor flow performance (e.g., error patterns over time).\n  - Investigate if recent flow configuration changes introduced issues by checking **Audit logs** for modifications.\n\n---\n\n### 7. **Additional Resources**\n  - If necessary, refer to the detailed troubleshooting steps in the documentation for the specific integration (e.g., Shopify-NetSuite).\n  - Contact Celigo support for unresolved or product-level issues, especially for errors marked as **unexpected** or with code **500**.\n\n---\n\nBy following these steps, you should be able to locate, debug, and resolve the error from the **Errors: Imports** tab efficiently. Let me know if you need further assistance!"",
  ""completion_tokens"": """",
  ""openai_duration"": 33.24,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 16182234244635.0,
  ""content"": ""The Celigo platform has several options that help you to gather information and troubleshoot errors.\nAt a high level, you can find cues or hints for troubleshooting errors in the following places:\nErrors page: important details in the error fields; and, in the case of some connections, such as HTTP, Salesforce, and NetSuite, you can find request and response details also\nFlow builder: details in the run console, run history, audit log, dashboard, diagnostics, and debug logs\nDeveloper playground: details in the advanced field editor (AFE) for Handlebar helpers and script issues\nSee also, the Investigating errors for a flow video and tools and tips for analyzing specific types of errors.\nExplore the integration dashboard\nMore specifically, these tools/options can sometimes provide insights to fix errors:\n1. Error fields\nThe error fields provide important details of where (source) and when (timestamp) an error occurred. It also helps you to find out in which record the error occurred if you\u2019ve set the trace key. Additionally, the classification indicates the type of error and provides cues on possible causes that you could fix. Based on these details, if you\u2019ve got an understanding of what the error could be, you can quickly edit the retry data and test if the error is fixed. If the error has been fixed, you won't see it in the Open errors after your retry. That's great, but make sure you correct the data in the source record.\n2. Diagnostics\nYou can download the diagnostics as a ZIP file. In addition to information on export and import flows, the ZIP file also contains information on parent job, child job, and job errors. You can use any of the diagnostic files to share and examine configurations and investigate errors. Below are excerpts from two such files.\n3. Audit logs\nAudit log: The audit log records changes made by any user to an integration\u2019s resources. These logs are retained for a year for compliance purposes, and they can also be used for troubleshooting. For example, if you find a new error caused by a missing value in a flow that had been running successfully, you can check when the error occurred in the Errors page, and then review the audit log to find out if any change caused the error.\nIf you suspect an issue in the product, raise a Support ticket so that we can investigate the matter.\n4. Advanced Field Editor\nThe Advanced Field Editor (AFE) shows any errors when you copy and paste your script. For example, if your script has a syntax error, then you can edit and debug the script before using it in the flow. For more information, see Advanced field editors AFEs and the Dev playground.\nSee also: FAQ on fixing JavaScript errors\n5. Integration & flow analytics\nThe integration flow analytics allow you to view errors, successes, ignores, and average processing time per successful record over time as line charts. At a high level, the charts are useful to monitor the health of your flows and make it easy to view trends, forecast, and analyze flows through a time period.\n6. Debug logs (for HTTP connectors)\nDebug logs (for HTTP connectors): Debug logs show communication between the Celigo platform and a flow step endpoint, including the HTTP requests and responses. These logs include details that could help in troubleshooting an error.\nFor more information, see Debug flow steps and Debug a connection. For example, the debug logs are helpful for a preSavePage hook script, if you want to view what\u2019s happening as you\u2019re manipulating, filtering, or grouping the records to ensure that each operation was yielding the expected result.\n7. Run console/Run history\nThe Run console provides details of the current flow runs; whereas, the run history provides a list of previous flow runs. You can view the error count and other statistics in the Run console/Run history. You can switch between examining details on the Errors page and the details in Flow builder when you correct errors and retry them. Run details can help you to detect patterns or to compare and detect when an issue occurs. When a retry is completed successfully, you can view the error count change in the flow step or dashboards. However, the error count you see in the run history and the run console is based on errors that occur during the flow run; it is not updated afterward. You can download diagnostics from either the run history or the run console.\nNote\nFor the latest error count after error retries, check the count for the individual step in Flow builder; the integration page (Flows tab); or dashboards. These error counts change dynamically based on the retries.\nThe Run history provides options to filter based on date range or status to study and examine patterns before you troubleshoot errors. For instance, for canceled flows, you can filter and view flows canceled by the system and investigate the causes.\n8. Dashboard\nYou can view the account and the integration dashboards for a quick breakdown of errors with run details. When you click the Run number in any flow in the account or integration dashboard, the Run history opens to display the errors at the flow and flow-step level. You can filter the run history based on a date range or the status. The dashboards reflect the latest count of errors, and you can quickly drill down to the Errors page via Error count links.\nYou can view flow runs for the past 30 days or a custom data retention period.\nFor more information, see Explore the integration dashboard.\nExample approach\nYour approach to investigating errors can include one or more of the options in the previous sections. It should be tailored to your use case and errors encountered; however, the following examples could help as you get started:\nTroubleshoot using primarily the error fields\nHigh-level approach to troubleshooting mapping errors (Community post)"",
  ""date"": ""Tue, 20 May 2025 18:12:43 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/16182234244635-Investigate-errors"",
  ""label_names"": [
  ""Audit Logs"",
  ""Dashboards"",
  ""errors"",
  ""AFE"",
  ""debug logs"",
  ""analytics"",
  ""diagnostics""
  ],
  ""linkedArticleIds"": [
  ""360048814732"",
  ""10972500798107"",
  ""360060740672"",
  ""4403697564429"",
  ""4408077000859"",
  ""4416800334235"",
  ""360001851772"",
  ""6359380074779"",
  ""7326016708763""
  ],
  ""numTokens"": 1166.0,
  ""score"": 0.772920668,
  ""section_id"": 24220435887003.0,
  ""source"": ""articles"",
  ""title"": ""Investigate errors""
  },
  {
  ""articleId"": 228211367.0,
  ""content"": ""This guide describes techniques and procedures for troubleshooting errors that might arise in the day-to-day run of the integration flows associated with your Shopify-NetSuite Integration App. The guide lists this Integration App common errors and describes a systematic approach to identifying and resolving the problem, with more probable solutions coming first.\nThis guide provides you with information on:\nIntegration Flow: Flow associated with the error.\nError Source: The application where the error occurred.\nError Code: A unique identifier associated with the error.\nError Message: A detailed error message that helps you understand the cause of the error.\nPossible Reason: Describes why this error might have occurred in your integration dashboard.\nPossible Resolution: Describes steps on how to resolve the error.\nThe troubleshooting sheet includes information about tasks you can perform to help ensure the trouble-free operation of your Shopify-NetSuite Integration App.\nFor information on the list of errors and resolution for these errors, see Troubleshooting Guide | Shopify-NetSuite Integration App.\nDashboard Page\nWhen the import/export process associated with an integration flow fails, it will generate errors on the Dashboard page. The Dashboard page allows you to fix the error and retry the flow run to successfully process the data that had caused an error to occur.\nThe dashboard page on your Shopify-NetSuite Integration App:\nAllows you to retry, resolve all, or a few job records in case of an error.\nDisplays statuses of the integration flow is executed.\nAllows you to view and troubleshoot job errors.\nAllows you to filter jobs based on their statuses and date.\nAllows you to modify data flow configuration and to run a data flow using the Action icon.\nThe dashboard is visible from the integration tile. You can also reach the dashboard from the Settings page.\nErrors on the Dashboard page\nWhen an integration flow runs into errors, the error details appear under the\u00a0Error\u00a0column of the dashboard. You can use the\u00a0View\u00a0link that appears when you hover over the number present under the\u00a0Error\u00a0column to view error details.\nAfter clicking View, a new screen is displayed with the following UI Element:\nSource: The application where the error occurred.\nCode: The error code.\nMessage: A detailed error message that helps you understand the cause of the error.\nTime: The date and time at which the error occurred.\nRetry Data: This allows you to modify the data that has caused the problem and lets you test if the incorrect record executes successfully after modification.\nRetry All: All failed records will be re-executed. However, if you select one or more records, only selected records would be re-executed.\nMark Resolved: If you mark a record as resolved, it will not appear when re-execute the job.\nDownload Errors: This allows you to download an Excel file that contains a list of errors. By default, in the Excel sheet, the resolve and retry columns are set to FALSE. You can analyze the error records and can set any one of the column value to TRUE. Note that both the resolve and retry columns cannot be set to TRUE.\nUpload Processed Errors: Once you have set values for resolve and retry columns for the desired error records in the Excel sheet (downloaded using the Download Errors button), you can upload the same Excel sheet using Upload Processed Errors. Once the file is uploaded, the integration flow is executed automatically.\nYou can fix errors one at a time and retry the flow run for the fixed data or you can fix all errors and click on the Retry All button to re-run the flow for all errors.\nTo view the export record that has an error in the Refund flow:\nOpen your Shopify URL domain in a new window. Here is the format of the Shopify URL\u00a0 https://admin.shopify.com/store/your-store-name/\nIn your integration app, follow any of the below paths:\nGo to Flows > Refund.\nSelect the flow that contains the error and click Errors.\nCopy Export Id from the error.\nGo to the Dashboard page and click Errors to view the error.\nCopy Export Id from the error.\nAt the end of the Shopify URL, paste the Export Id.\nRemove the API version \u2018admin/api/XXXX-XX\u2019 from the URL.\nClick Enter and you can now view the same record that has an error."",
  ""date"": ""Tue, 19 Mar 2024 15:59:47 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/228211367-Get-started-with-troubleshooting-Shopify-NetSuite-errors"",
  ""linkedArticleIds"": [
  ""360007252991""
  ],
  ""numTokens"": 856.0,
  ""score"": 0.770768762,
  ""section_id"": 360010869552.0,
  ""source"": ""articles"",
  ""title"": ""Get started with troubleshooting Shopify-NetSuite errors""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Review the error details tab in the Flow Builder to identify the root cause of the import failure.""
  },
  {
  ""title"": ""Edit the Retry Data directly within the Errors section to fix invalid or missing fields for the affected record.""
  },
  {
  ""title"": ""Enable debug logging in the relevant import connector settings to capture detailed request/response information for troubleshooting.""
  }
  ],
  ""thread_id"": ""thread_VUAvoEMrfCZOsELgpTvzQCwD"",
  ""vector_duration"": 1.88
 }",,,,,,,,,,,,,,,,
Copilot_63,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,APIs_001.json,"{
  ""answer"": ""APIs in Celigo refer to programmable interfaces that allow you to expose your organization's data, services, and assets for integration. They are not limited to internal Celigo APIs. Using Celigo's API Management, you can create, manage, and expose APIs for both internal and external use, including APIs for external partners and third-party applications. Developers can build custom APIs with JavaScript or other tools and publish them through Celigo's APIM Console to secure, scale, and govern those endpoints."",
  ""completion_tokens"": """",
  ""openai_duration"": 8.31,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 20084381653019.0,
  ""content"": ""Celigo\u2019s API Management allows you to expose your company\u2019s data, services, and assets. It enables business systems to connect, accelerates partner connectivity, increases business agility, and spurs product innovation. API management enables developers and organizations to secure, scale, govern, analyze, and monetize these APIs. Celigo\u2019s API Management solution empowers you to extend your digital platform, forge new communication channels, and attract new customers.\nIt is an integrated component of the Celigo platform that helps you form the backbone of your API-led integration strategy. You can build an entirely new API using the platform's core features and push them to the API Management (APIM) console to customize and govern your endpoints before exposing them.\nCeligo's API Management contains three major components:\nAPIM Console \u2013\u00a0allows API publishers to modify APIs, add policies, define plans, approve subscriptions, and monitor API usage metrics.\nAPIM Developer Portal \u2013 allows API consumers to discover APIs that may interest them, subscribe to APIs, and manage their ongoing subscriptions.\nAPIM Gateway \u2013 enforces the policies and other security controls applied to the API proxy interface.\nThe Celigo API Management lifecycle provides the core features necessary for all phases of API Management, including:\nBuild \u2013 create, edit, and version APIs to make your organization\u2019s data and processes available for reuse\nSecure \u2013 manage and implement policies (authorization, rate-limiting, throttling, masking, etc.) to protect sensitive data and enhance system reliability and performance\nSocialize \u2013 publish your APIs to a centralized and customizable portal for internal and/or external consumers to discover and subscribe\nMonitor \u2013 analyze usage patterns, performance, and consumption to guide decision-making for your API initiatives\nWhat can I do using Celigo API Management?\nAPI publishers\nAPI publishers create and publish APIs in the Celigo platform for API consumers.\nBuild and publish APIs: Configure, implement, and publish APIs for consumers and application developers. The API builder and the APIM console allow publishers to configure every aspect of an API, from its policies to its plans and documentation. Publishers using the API Management console can use various authentication methods, including, but not limited to, JWT, OAuth 2.0, and keyless authentication.\nDefine API security and traffic shaping and provide routing information to backend endpoints: When a resource is managed through the APIM console, the required API gateway policies will be added by default. You can add more policies in the APIM console at API \u2192 Policy Studio.\nExpose APIs to the gateway: You can configure and enforce policies at the API gateway level to manage and control API traffic (e.g., using rate limiting), apply transformations to requests or responses, define logs and metrics, manage caches, etc. There are 40+ policies you can use to customize your API. Some commonly used policies are:\nRate Limiting\nIP Filtering\nXML to JSON\nREST to SOAP\nMock\nCache\nTransform header\nDynamic routing\nHTTP Callout\nDefine metrics and analyze API usage: You can view prebuilt dashboards in the APIM console to monitor how well the APIs are performing and identify bottlenecks, latency issues, and other performance-related issues, which can help you optimize these APIs. You can also create custom dashboards around API performance, status, lifecycle stage, etc.\nDocument APIs: API publishers can document APIs in OpenAPI specification (OAS) format so that API consumers can have a clear and detailed explanation of how an API works and how to use it effectively. It will also reduce the learning curve for consumers, making it easier for them to adopt and integrate the API into their applications.\nThe Celigo platform will\u00a0generate the OpenAPI specification (OAS)\u00a0for your API (not including JavaScript API services). Specifications for multiple endpoints in an API will be grouped. Services pushed to the APIM console will be automatically configured with the required OAS. You can update the OAS in the APIM console at API \u2192 General \u2192 Documentation. API publishers can publish the documentation to the Developer Portal so consumers can discover, subscribe to, and adopt APIs.\nCaution\nPrerequisite elements are required to generate OAS for any APIs. JavaScript APIs are script-based resources with missing prerequisite elements; thus, the OAS will not be generated. If the OAS is generated, users must manually verify and update it in the APIM console.\nAudit changes to your API program: View audit logs of all changes done in the APIM console to quickly identify who has changed the API configuration when the API breaks and to revert to the previous API version if required.\nManage your APIM notifications: Create notifications based on set alerts (e.g., API key expiry, new subscription, etc.) using one or more of the following options:\nPortal\u2009notification to send messages to users logged in to the developer portal\nEmail notification to send messages to a specific list of email addresses\nWebhook notification to send an HTTP POST request to a specific URL\nAPI consumers\nAPI consumers are consuming the APIs that the API publisher has created.\nConsume APIs and build new applications: Access and consume APIs an organization exposes in the Developer Portal to create new APIs and applications, thereby automating business processes.\nPublic and private APIs can be published in the developer portal for external and internal consumption. API consumers can use an identity provider to sign in to the developer portal and discover, subscribe, test, and consume the APIs. API requests from a client-based web application with a different URL will be allowed via the APIM gateway using CORS (Cross-origin resource sharing).\nKey benefits\nThere are many benefits to creating, publishing, managing, and consuming APIs through Celigo's integrator.io. For most, the benefits come from customization and ease of use.\nAPI security and access control \u2013 Implements authentication, authorization, and encryption mechanisms to secure APIs and ensure only authorized users can access sensitive data and services.\nAPI performance analytics \u2013 Developers can create custom dashboards, reports, and analytics tools to monitor API usage.\nAPI governance \u2013 Controls the rate at which different users or applications can access your APIs to prevent overload and ensure fair usage.\nAPI documentation and Developer portal \u2013 Creates user-friendly documentation, interactive examples, and a developer portal to facilitate understanding and adoption of your APIs.\nUse cases\nExternal developer access and partner integration \u2013 Allows third-party developers to access and utilize your organization's APIs to build applications and services that enhance your offerings. It also enables partners to integrate their systems with your APIs for more streamlined collaboration and data sharing.\nInternal API consumption and integration \u2013 Enables different teams within your organization to use and share APIs to promote the reusability of services, reduce duplication of efforts, and accelerate development cycles.\nLegacy system modernization \u2013 Exposes APIs for legacy systems, allowing them to be integrated with modern applications and services, avoiding the need for a complete system overhaul.\nMonetization and revenue generation \u2013 Generates revenue by exposing certain APIs to external developers for a fee or subscription, creating new business models and opportunities."",
  ""date"": ""Thu, 13 Mar 2025 18:29:12 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/20084381653019-Learn-more-about-Celigo-API-Management"",
  ""label_names"": [
  ""APIs"",
  ""api management"",
  ""benefits"",
  ""api analytics"",
  ""API"",
  ""API security"",
  ""developer portal"",
  ""API integration"",
  ""API lifecycle"",
  ""API governance"",
  ""API performance"",
  ""monetization"",
  ""Primary Information""
  ],
  ""linkedArticleIds"": [
  ""30313524511643"",
  ""20105899693083"",
  ""24641124241563"",
  ""20069114524187""
  ],
  ""numTokens"": 1405.0,
  ""score"": 0.831372142,
  ""section_id"": 20083541190811.0,
  ""source"": ""articles"",
  ""title"": ""Learn more about Celigo API Management""
  },
  {
  ""articleId"": 22109716449179.0,
  ""content"": ""Contents\nWhat\u2019s new\nWhat\u2019s enhanced\nWhat\u2019s fixed\nSupport: Help Center updates\nWhat's new\nCeligo AI delivers descriptive resource summaries, intuitive code assistance, and intelligent auto-mapping\nResource descriptions\nWith the help of Celigo AI, you can instantly generate plain language descriptions of imports, exports, and flows. Gain an immediate understanding of an integration\u2019s business purpose instead of spending hours deciphering complex flow logic. Especially beneficial for those less familiar with the platform, this feature offers a concise breakdown of business logic, including transformations, mappings, and more.\nEasily understand the logic of a resource created months ago, avoiding tedious configuration checks\nSeamlessly transition integrations when a team member departs without being dependent upon manual documentation efforts\nProvide leadership teams with high-level summaries without diving into intricate details\nTransfer integrations to and from services and partner teams smoothly, maintaining confidence that users with varying degrees of platform knowledge will easily understand the resources\nCode assistance\nCeligo AI empowers more users to become proficient builders with an intuitive code assistant that enables non-technical users to craft code in various advanced field editors (AFEs) for SQL statements, handlebars , and JavaScript using plain language prompts. Additionally, the \u201cexplain selection\u201d feature provides a clear explanation of any highlighted section of existing code, bridging knowledge gaps in an instant.\nAuto-mapping\nReduce manual effort and increase task efficiency by leveraging Celigo AI to automatically populate mappings for destination fields. Existing mappings will remain untouched while any remaining fields are intuitively populated, greatly reducing time to value by enabling users to complete configurations more quickly.\nGuided experience in writing handlebars expressions\nRemove guesswork and reduce cognitive burden by allowing the Celigo platform to take the lead in writing handlebars expressions. The cursor will move through the expression and highlight one parameter at a time while offering suggested values in a dropdown menu. Auto-suggestions can be escaped or invoked again with keyboard shortcuts at any time during the writing process, allowing you to determine how much guidance you would like to receive.\nGenerate OpenAPI specifications for webhook listeners\nAs previously introduced for exports, imports, and data flows, OpenAPI specification generation is now available for listeners. With API specifications and documentation readily available, consumers of your APIs can leverage the endpoints efficiently for their business needs without gaining full access to the source code.\nNew connectors\nFind complete instructions and popular integration scenarios \u2013 along with connector resource enhancements \u2013 at Celigo platform connectors 2024.1.1 release notes.\nWhat's enhanced\nNetSuite WSDL version support updates\nIn alignment with NetSuite\u2019s recent changes to their list of supported WSDL versions:\nWSDL version 2023.1 is now supported\nAny NetSuite connections configured with WSDL version 2016.2 (deprecated) must be updated to avoid flow interruptions\nTest mode enhanced\nView test run results for blob exports and imports\nView test run debug logs without having to enable debugging\nView test run results for dynamic lookups configured within NetSuite mappings to verify the lookups are being correctly evaluated\nUser experience improvements\nThe left navigation menu has received some design changes: the Help and Account sections can now be found in the top right of the screen\nThe option to unmerge branches now has its icon:\nVarious design adjustments have been made to optimize the connection creation experience in the flow builder\nThe errors page/list can now be opened from the Home screen or Flow builder page when all runtime and test mode errors are part of the same flow step\nRestrictions have been put in place to ensure that running flows cannot be disabled\nWhen creating a connection, a search for a specific API type will return results for all related connectors\nWhat's fixed\nCustomer-reported issues resolved\nNonstandard response code from SAP Ariba listeners was interpreted as an error, marking some orders as failed\nFTP exports failed when multiple concurrent calls attempted to perform file parsing within a short period\nExport previews failed to properly display extended characters, resulting in missing data\nSalesforce imports with conditional mappings in the item sublist were creating empty lines when attempting to update records\nNetSuite JDBC exports sharing concurrent connections crashed when very large volumes of data were requested\nSupport: Help Center updates\nNew connector articles\nSet up a connection to RingCentral\nSupported RingCentral APIs\nSet up a connection to Xactly Connect\nSupported Xactly Connect APIs\nSet up a connection to Microsoft Excel Online\nSupported Microsoft Excel Online APIs\nSet up a connection to NICE CXone\nSupported NICE CXone APIs\nMigrate from Jira Server API endpoints to Jira Cloud API endpoints\nMigration connector articles\nSet up a connection to Act-On\nSupported Act-On APIs\nSet up a connection to Azure Active Directory\nSupported Azure Active Directory APIs\nSet up a connection to Campaign Monitor\nSupported Campaign Monitor APIs\nSet up a connection to Coupa\nSupported Coupa APIs\nSet up a connection to Eventbrite\nSupported Eventbrite APIs\nSet up a connection to FieldAware\nSupported FieldAware APIs\nSet up a connection to GitHub\nSupported GitHub APIs\nSet up a connection to Google Ads\nSupported Google Ads APIs\nSet up a connection to Google Analytics\nSupported Google Analytics APIs\nSet up a connection to Google Contacts\nSupported Google Contacts APIs\nSet up a connection to Google Mail\nSupported Google Mail APIs\nSet up a connection to Google Shopping\nSupported Google Shopping APIs\nSet up a connection to Harvest\nSupported Harvest APIs\nSet up a connection to LogiSense\nSupported LogiSense APIs\nSet up a connection to Lightspeed\nSupported Lightspeed APIs\nSet up a connection to Marketo\nSupported Marketo APIs\nSet up a connection to Microsoft Dynamics 365 CRM\nSupported Microsoft Dynamics 365 CRM APIs\nSet up a connection to Microsoft Dynamics 365 Finance\nSupported Microsoft Dynamics 365 Finance APIs\nSet up a connection to Microsoft Dynamics 365 Supply Chain Management\nSupported Microsoft Dynamics 365 Supply Chain Management APIs\nSet up a connection to Microsoft OneNote\nSupported Microsoft OneNote APIs\nSet up a connection to Microsoft Outlook Calendar\nSupported Microsoft Outlook Calendar APIs\nSet up a connection to Microsoft Outlook Contacts\nSupported Microsoft Outlook Contacts APIs\nSet up a connection to Microsoft Outlook Mail\nSupported Microsoft Outlook Mail APIs\nSet up a connection to Microsoft Teams\nSupported Microsoft Teams APIs\nSet up a connection to PowerBI\nSupported Microsoft Power BI APIs\nSet up a connection to Procurify\nSupported Procurify APIs\nSet up a connection to SAP Business One\nSupported SAP Business One APIs\nSet up a basic auth connection to ServiceNow\nSet up an OAuth 2.0 connection to ServiceNow\nSupported ServiceNow APIs\nSet up a connection to Smartsheet\nSupported Smartsheet APIs\nSet up a connection to Zoho Desk\nSupported Zoho Desk APIs\nSet up a connection to Zoho Mail\nSupported Zoho Mail APIs\nSet up a connection to Zuora\nSupported Zuora APIs\nRevised connector articles\nSet up a connection to an application or database via JDBC\nSet up a connection to Exact ERP\nSet up a connection to AfterShip\nSupported SysAid APIs\nSupported OpenAI APIs\nSupported HiBob APIs\nNew integrator.io articles\nCeligo NetSuite integration bundle v1.31.0 release notes SuiteScript and SuiteApp\nSet up a JWT bearer auth HTTP connection\nUnderstand AI descriptions for a flow, import, or export\nCreate or explain JavaScript using the Celigo AI component\nAuto-map field mappings using Celigo AI\nRevised integrator.io articles\nCreate a one-to-many import, including nested arrays\nImport data into Microsoft SQL\nImport data into Amazon Redshift\nImport data into Google BigQuery\nCreate your account and profile\nDebug a flow configuration\nGenerate an OpenAPI Specification Spec in integrator.io\nNew integration app articles\nPrebuilt integration apps 2024.1.1 release notes\nShopify - NetSuite integration app release notes, 2024.1.1\nShopify - Microsoft Dynamics 365 Business Central integration app release notes, 2024.1.1\nBigCommerce - NetSuite integration app release notes, 2024.1.1\nUpdate payment method mapping\nFailed to create customer deposits in NetSuite intermittently\neBay - NetSuite integration app release notes, 2024.1.1\nRevised integration app articles\nSync order refunds between Shopify and NetSuite\nSync orders from Shopify to Microsoft Dynamics 365 Business Central\nManage journal entries in NetSuite"",
  ""date"": ""Mon, 07 Jul 2025 18:30:40 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/22109716449179-Celigo-platform-2024-1-1-release-notes"",
  ""label_names"": [],
  ""linkedArticleIds"": [
  ""36185234498715"",
  ""21761439082139"",
  ""21714038891931""
  ],
  ""numTokens"": 1727.0,
  ""score"": 0.815611243,
  ""section_id"": 29500040935835.0,
  ""source"": ""articles"",
  ""title"": ""Celigo platform 2024.1.1 release notes""
  },
  {
  ""articleId"": 13523742710555.0,
  ""content"": ""The Celigo platform is built using best-of-breed technology frameworks and secure software development practices. Production and testing environments are completely segregated from each other, and customer data is never used in QA or developer testing.\nCeligo has a designated Sr. Director of Security and Compliance and Data Protection Officer to lead the Security and Compliance Team and work with technical staff to support the implementation of the security requirements needed to operate at the levels of security and compliance that our management and our customers expect.\nFor more information:\nCeligo privacy policy, cookie policy, and GDPR compliance\nTransport Layer Security (TLS)\nThe highly scalable and robust iPaaS (PDF)\nSecurity guidelines for integrator.io\nKeeping your data secure while integrating with Celigo\nProtocols\nIncoming connections: TLS 1.2\nHTTPS client traffic: TLS 1.2\nEndpoint/FTP connections: HTTP and HTTPS TLS 1.0 to TLS 1.3 (highest available automatically selected)\nApplication authorizations and trusted connections\nAccount owners and administrators completely control authorization per user and per application.\nAudit logs\nKeep track of activity on your account for up to a year. Monitor integration and flow changes over the course of the resource\u2019s lifecycle.\nRegulatory compliance\nCeligo handles all data at the highest level required for regulatory and voluntary compliance requirements, ensuring cloud security at multiple levels:\nEU and UK\nEU/UK GDPR-Ready\nContact Celigo for a Data Processing Agreement (DPA).\nUS State Privacy (A US Data Privacy addendum is available)\nCalifornia CCPA/CRPA - Ready\nNevada Chapter 603-A \u2013 Covered by GDPR and CCPA- Ready\nNew York SHIELD ACT \u2013 Covered by GDPR and CCPA- Ready\nVirginia Consumer Data Protection Act (VCDPA) - Ready\nTexas Data Privacy and Security Act (TDPSA) - Ready\nVoluntary compliance\nSOC-2 \u2013 Type 2 compliant\nAs a customer or a prospect, you may request a copy of the SOC 2 report under Mutual NDA from compliance@celigo.com.\nHIPAA \u2013 HIPAA-ready, though not HIPAA-certified\nHiTech \u2013 Not HiTech standard-certified, though qualified to support HiTech standard-certified companies (as a Business Associate to either a Covered Entity or another Business Associate) under the certified HiTech service providers\u2019 infrastructure\nStatus \u2013 The Celigo Security Team has completed implementation for infrastructure encryption: restricted access to ePHI data, trained select staff, and provided secure laptops with full-disk encryption\nFERPA-ready\nFedRAMP \u2013 Not certified, since Celigo is not directly U.S. government-facing, but we can support companies that are FedRAMP certified as part of the certified FedRAMP service providers\u2019 infrastructure\nUS, EU, UK, and Swiss Data Privacy Framework Certified\nData retention\nThe error data retention period lasts for 30 or more days, based on your Celigo license. You can delete records if you choose to or if your customers exercise their Right to Delete.\nData protection\nData is encrypted in motion and at rest, according to country-specific data protection and privacy guidelines:\nAll data in motion inside AWS VPC \u2013 Encrypted at TLS 1.2 or better\nAll data temporarily stored in AWS \u2013 Encrypted at AES 256\nStored credentials \u2013 Encrypted at either AES 256 or pbkdf2\nWe support the highest level of HTTPS API TLS encryption available. For example, when NetSuite supports TLS 1.2, then the Celigo connection is also encrypted to TLS 1.2. Therefore, it is the customer's responsibility to ensure that the endpoint encryption is at least TLS 1.2. Otherwise, that particular segment of the data flow may not be considered securely encrypted \u2013 or encrypted at all in the case of an HTTP API. (Note that TLS 1.0 and 1.1 have been deprecated by the security community.)\nEncryption keys\nCeligo has enabled SSE-S3 for all Amazon S3 buckets. Each file saved in S3 is encrypted and its Key is encrypted using a master key stored separately.\nEndpoint API credentials are provided by Celigo customers, who are responsible for updating tokens and passwords according to their security policies.\nAPI credential security\nYou provide your API credentials for your endpoint, then they are encrypted and stored in integrator.io for subsequent flow steps.\nProfessional Services customer credentials vaulting\nNeed help developing flows? You can provide your credentials to us securely so that we can build it for you."",
  ""date"": ""Fri, 27 Sep 2024 21:22:41 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/13523742710555-Security-and-compliance"",
  ""linkedArticleIds"": [
  ""360050374512"",
  ""360058028072"",
  ""360045215251"",
  ""6514515710107"",
  ""6359380074779""
  ],
  ""numTokens"": 914.0,
  ""score"": 0.812605858,
  ""section_id"": 13322551262875.0,
  ""source"": ""articles"",
  ""title"": ""Security and compliance""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Review the JavaScript API builder interface to start creating or editing custom APIs.""
  },
  {
  ""title"": ""Access the APIM Console to configure security, policies, and documentation for your JavaScript API.""
  },
  {
  ""title"": ""Test your API endpoint in a development environment to validate functionality before publishing.""
  }
  ],
  ""thread_id"": ""thread_oS25oJ2ZlXSKNEwvNIY8fpK6"",
  ""vector_duration"": 2.05
 }",,,,,,,,,,,,,,,,
Copilot_64,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,InstallIntegration_001.json,"{
  ""answer"": ""To get the integration ZIP file, you need to download it from the **Home - My integrations** page by selecting the specific integration you want to export. The integration ZIP file contains your integration's components and can then be shared or uploaded into another account.\n\n### Steps to Download the Integration ZIP File:\n1. Navigate to the **Home - My integrations** page.\n2. Locate the integration you wish to download.\n  - If you're in the **tile view**, click the overflow menu (**...**) in the upper-right corner of the integration tile.\n  - If you're in the **list view**, look in the **Actions column** for the overflow menu (**...**) corresponding to your selected integration.\n3. From the dropdown menu, select **Download integration**.\n4. The integration will be downloaded as a `.zip` file to your system's default download location.\n\n### Notes:\n- Only **custom integrations** can be downloaded as ZIP files.\n- Connections are not included in the ZIP file, so you need to configure them after uploading it to a different account.\n\nIf you\u2019re currently on the **Upload integration** page, this means someone has already shared an integration ZIP file with you. To obtain the ZIP for upload, you must first download it from the steps above or request it from the person who shared it. Once you have the ZIP file, you can upload it on your current page.\n\nLet me know if you need help with any specific step!"",
  ""completion_tokens"": """",
  ""openai_duration"": 26.42,
  ""prompt_tokens"": """",
  ""suggestions"": [
  {
  ""title"": ""Download the integration ZIP file from the Home page using the \""Download integration\"" option.""
  },
  {
  ""title"": ""Locate the previously downloaded integration ZIP file for upload on the current page.""
  },
  {
  ""title"": ""Upload the ZIP file on the \""Upload integration\"" page and click Install integration.""
  }
  ],
  ""thread_id"": ""thread_t3daDUgsspENp9WhkUO4TuL5"",
  ""vector_duration"": 2.01
 }",,,,,,,,,,,,,,,,
Copilot_saveAgent_65,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,Exports_saveAgent_001.json,"{
  ""answer"": ""Based on your current context on the **'Exports - Create export'** page and the provided documentation, you should use **Google Sheets** to fetch data from a Google spreadsheet.\n\nHere's why and how to proceed:\n\n### Why use Google Sheets?\n- The **Google Sheets connector** is specifically designed for working directly with spreadsheet data.\n- It allows you to export data using the **Google Sheets API**, which supports actions such as reading data or updating it.\n- The **Google Drive connector** is more suitable for managing files or folders in Google Drive, such as file uploads, downloads, or organization, not for accessing spreadsheet data directly.\n\n### What should you do?\n1. **Start by setting up a Google Sheets connection**:\n  - Go to the 'Connection' section in the export configuration and create a connection for Google Sheets if you don't already have one.\n  - Follow these steps:\n  - Select **Google Sheets** as the application.\n  - Name your connection clearly.\n  - Set the required **Scopes** (choose `https://www.googleapis.com/auth/spreadsheets.readonly` for read-only access or `https://www.googleapis.com/auth/spreadsheets` if both read/write access is required).\n  - Click **Save & authorize** to log in and authorize access to your Google Sheets.\n\n2. **Configure the export settings**:\n  - Enter the **Spreadsheet ID**:\n  - You can find it in the Google Sheets URL. For example, in `https://docs.google.com/spreadsheets/d/1vBy7f9a8uvnhydRsSfxp24XpocJhOrsQy-Htz1kTfUo/edit#gid=904268349`, the Spreadsheet ID is `1vBy7f9a8uvnhydRsSfxp24XpocJhOrsQy-Htz1kTfUo`.\n  - Specify the desired **Range** in A1 notation (e.g., `'Sheet1'!A2:Z100` to get rows 2 through 100 in Sheet1).\n  - Choose whether the **first record contains headers**, as this determines whether field IDs will match the column headers or column indexes.\n\n3. **Use output filters, if needed**:\n  - You can add filters to export only rows that match specific criteria.\n\n4. **Save the export and run the flow**:\n  - Save your configuration, and then test the export by running the flow.\n\nUsing **Google Sheets**, rather than Google Drive, is the correct approach for your goal of fetching spreadsheet data. If you need further assistance, feel free to ask!"",
  ""completion_tokens"": """",
  ""openai_duration"": 15.09,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 360057234491.0,
  ""content"": ""Google Sheets is a web-based application that enables users to create, update, and modify spreadsheets and share data online in real-time.\nSet up a connection\nAfter you start the connection, configure it in the Create connection panel and complete all of the required * authentication settings:\nSetting\nInstructions\nName your connection *\nEnter a clear and distinguishable name.\nThroughout integrator.io imports and exports, you will have the option to choose this new connection. A unique identifier will prove helpful later when selecting it from a list of the connections in your account.\nScope *\nScopes are OAuth 2.0 permissions that the Google Sheets API defines to limit access to your account.\n\u200b\u200bHow to configure scopes\nTip\nThis connector documentation describes only the settings shown for the Simple view. For the corresponding HTTP settings, see OAuth 2.0 auth universal connector documentation.\nHow to configure scopes\nClick Scope.\nTo access all of the scopes in the API, click the \u00bb button.\n\u2013 or \u2013\nCheck the scopes you want to add and click the > button.\nClick Save.\nThe following scopes are available:\nScope\nDefinition\nhttps://www.googleapis.com/auth/spreadsheets.readonly\nAllows read-only access to the user's sheets and their properties.\nhttps://www.googleapis.com/auth/spreadsheets\nAllows read/write access to the user's sheets and their properties.\nSave and authorize\nClick Save & authorize to commit your changes and proceed to sign in with Google Sheets.\nCaution\nDepending on your browser history and the permissions you\u2019ve granted, you may see the following screen when attempting to sign in:\nIf so, click Advanced, and then click Go to integrator.io (unsafe) to allow the exception and continue.\nAfter signing in, review the permissions for the integration and click Allow.\nCaution\nCaution: Depending on your Google permissions, you may see the following screen when attempting to sign in:\n{{img class=screenshot-frame id=3}}\nIf so, you must\u00a0 change your Google third-party application permissions before you set up a connection. If your permissions are managed by your company, you may need to speak to your security, compliance, or IT department before creating the connection.\nAdditional references\nGoogle Sheets API guide\nSupported Google Sheets APIs\nRelated topics and community posts:\nWorking with Google Sheets\nHow to: Extract data from Google Sheets and get data rows into name/value pairs"",
  ""date"": ""Thu, 19 Jun 2025 12:46:57 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/360057234491-Set-up-a-connection-to-Google-Sheets"",
  ""label_names"": [
  ""Google Sheets"",
  ""Gsheets"",
  ""Primary Information""
  ],
  ""linkedArticleIds"": [
  ""226974368"",
  ""9252706199963"",
  ""10031282236059""
  ],
  ""numTokens"": 481.0,
  ""score"": 0.81404388,
  ""section_id"": 9969095399195.0,
  ""source"": ""articles"",
  ""title"": ""Set up a connection to Google Sheets""
  },
  {
  ""articleId"": 10031282236059.0,
  ""content"": ""To export or import data into a Google spreadsheet, we recommend sourcing flow steps from the marketplace in Flow Builder or installing the Google Sheets Helper integration template from the integrator.io Marketplace (NA | EU).\nThis integration template contains one import and export that can be cloned and re-used for other flows:\nExport: Read all data from spreadsheet\nImport: Update existing rows and append new rows in spreadsheet\nExport: Read all data from spreadsheet\nUse this export to read data from a spreadsheet and output easy-to-use records. You can clone this step any number of times for your different flows and use cases.\nAfter you connect to Google Sheets you can configure your pre-filled export.\nTo configure the settings:\nSpreadsheet ID (required): Enter your spreadsheet ID. For example, if the spreadsheet URL is https://docs.google.com/spreadsheets/d/1vBy7f9a8uvnhydRsSfxp24XpocJhOrsQy-Htz1kTfUo/edit#gid=904268349, the ID is '1vBy7f9a8uvnhydRsSfxp24XpocJhOrsQy-Htz1kTfUo'.\nRange (using A1 notation) (required): Enter the range you would like to export, using A1 notation.\nNotice\nTo export the entire sheet, enter the sheet name: 'Sheet1' . To exclude the first row and include only the first 100 rows, your range would be: 'Sheet1'!2:100 . You must wrap your sheet name with single quotes.\nFirst record contains headers (optional): If there's a column header row in the range you're exporting from, the column headers will be used as the field IDs, and the header row itself will be discarded. If not, the zero-based column indexes will be used as the field IDs.\nHeaders (optional): This is calculated after you save your custom settings. These are the column headers discovered in the spreadsheet.\nWarning\nRequired: If your sheet(s) contain headers, make sure every column has one. Missing headers in any column can lead to errors.\nCalculated after save (updated range without column header row): This updated range will be used to export data from your spreadsheet. This range does not include the column header row. This range will be re-calculated every time you save your export.\nClick Save.\nOutput filters can be added to the export after it is configured; for example, to support search criteria to only export specific rows in your spreadsheet.\nImport: Update existing rows and append new rows in spreadsheet\nUse this prebuilt import to upload data into a spreadsheet. This step updates existing rows it finds based on the key column you configured. If an existing row cannot be found, it will append new rows to the bottom of the spreadsheet.\nThe step is preconfigured with the exception of the Settings at the bottom of the drawer.\nTo configure the settings:\nSpreadsheet ID (required): Enter your spreadsheet ID. For example, if the spreadsheet URL is https://docs.google.com/spreadsheets/d/1vBy7f9a8uvnhydRsSfxp24XpocJhOrsQy-Htz1kTfUo/edit#gid=904268349, then the id would be '1vBy7f9a8uvnhydRsSfxp24XpocJhOrsQy-Htz1kTfUo'.\nSheet name (required): Enter the name of the sheet you'd like to update.\nNotice\n'Sheet1' . Be sure to wrap your sheet name with single quotes!\nDoes your sheet have column headers? (optional): Is there a column header row in the sheet you are updating? If there's a column header row in the sheet you're updating, the headers will be used as the field IDs. If not, the zero-based column indexes will be used as the field IDs.\nHeaders (optional): This is calculated after you save your custom settings. These are the column headers discovered in the spreadsheet.\nWarning\nRequired: If your sheet(s) contain headers, make sure every column has one. Missing headers in any column can lead to errors.\nColumn to find existing rows (required): Enter the column header that should be used to find existing rows. For example: ID , email , task_id, etc...\nSource record field to find existing rows (required): Enter the field from the source record that should be used to lookup existing rows in the spreadsheet. For example, my_record_id.\nClick Save.\nIf you have column headers, mappings will be auto-generated, and you only need to populate the source field column. Then, run your flow!\nNote\nYou may encounter an 'Invalid JSON payload' error when you don't provide mappings in an import. The Google Sheets import requires a POST body.\nGoing deeper using Google's native APIs\nintegrator.io's connectors are incredibly powerful and give you full access to the underlying APIs. If you need to do any other API tasks related to Google Sheets, we recommend reading the following high-level guides and contacting support for help:\nSet up a connection to Google Sheets\nGoogle Sheets APIs\nMethods\nSet the Google Sheets URI according to the action you want to perform.\nappend: Appends values to a spreadsheet.\nbatchClear: Clears one or more ranges of values from a spreadsheet.\nbatchClearByDataFilter: Clears one or more ranges of values from a spreadsheet.\nbatchGet: Returns one or more ranges of values from a spreadsheet.\nbatchGetDataByFilter: Returns one or more ranges of values that match the specified data filters.\nbatchUpdate: Sets values in one or more ranges of a spreadsheet.\nbatchUpdateByDataFilter: Sets values in one or more ranges of a spreadsheet.\nclear: Clears values from a spreadsheet.\nget: Returns a range of values from a spreadsheet.\nupdate: Sets values in a range of a spreadsheet.\nClear all data from your Google Sheet\nIn this example, you\u2019re clearing your data from the Google Sheet you want to update with new data.\nChoose the POST method.\nSet your Relative URI to point to your desired sheet. Use the following format:\nv4/spreadsheets/{spreadsheetId}/values/{range}:{method}\nFor example:\nv4/spreadsheets/abcd123hijk456/values/A1:G300:clear\nIf you have multiple sheets in your range should include your sheet name:\nv4/spreadsheets/abcd123hijk456/values/Sheet1!A1:Z9000:clear"",
  ""date"": ""Thu, 19 Jun 2025 12:47:05 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/10031282236059-Working-with-Google-Sheets"",
  ""label_names"": [
  ""Google Sheets"",
  ""import"",
  ""Primary Information""
  ],
  ""linkedArticleIds"": [
  ""360057234491"",
  ""4407535381147"",
  ""360057234491""
  ],
  ""numTokens"": 1341.0,
  ""score"": 0.812757373,
  ""section_id"": 9969095399195.0,
  ""source"": ""articles"",
  ""title"": ""Working with Google Sheets""
  },
  {
  ""articleId"": 10769977254299.0,
  ""content"": ""You can use a webhook to immediately import the questionnaire data from a submitted Google Form to a destination application, as described below:\nCreate a webhook flow and verify that you have the public URL value that will be used to export data from Google Forms to integrator.io.\nCreate the destination import flow step and turn the flow on.\nCreate a Google Form, keeping mind that the form submissions will also be integrated with you destination application.\nLink the form to a Google Sheet that stores all data submitted by those who use the form.\nOpen the form you created.\nClick the Google Sheets icon in the Responses tab.\nVerify that Create a new spreadsheet is selected, give the spreadsheet a recognizable name, and then click Create.\nVerify that each of your questions maps to a column in the spreadsheet.\nAdd an Apps Script to your Google Sheet.\nOpen the Google Sheet.\nIn the Extensions tab, click Apps Script.\nOpen the Code.gs page and paste the following:\nfunction onFormSubmit(e) {\nvar namedValues = JSON.parse(JSON.stringify(e.namedValues));\nfor (let key in namedValues) {\nif (JSON.stringify(namedValues[key]) == '[\""\""]') {\nnamedValues[key] = null;\n} else {\nnamedValues[key] = namedValues[key][0];\n}\n}\nvar url = \""https://api.integrator.io/v1/exports/URL-ID-for-webhook> /data\"";\nvar options = {\n\""method\"": \""post\"",\n\""headers\"": {\n\""content-type\"": \""application/json\""\n},\n\""payload\"": JSON.stringify(namedValues)\n};\nvar response = UrlFetchApp.fetch(url, options);\n}\nReplace the value for var url with the public URL.\nClick the Triggers tab.\nClick +Add Trigger.\nConfigure the trigger with the following settings:\nChoose which function to run: onFormSubmit\nWhich runs at deployment: Head\nSelect event source: From spreadsheet\nSelect event type: On form submit\nClick Save.\nNow, when a respondent submits the form, Google Forms collects the response and initiates your real-time flow, integrating the data according to the logic in your flow. Test your flow by adding information to your Google Form to verify that the data triggers the flow when you click Submit."",
  ""date"": ""Thu, 19 Jun 2025 12:46:52 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/10769977254299-Set-up-a-webhook-to-start-a-flow-when-a-Google-Form-is-submitted"",
  ""label_names"": [],
  ""linkedArticleIds"": [],
  ""numTokens"": 442.0,
  ""score"": 0.81156075,
  ""section_id"": 10769921832603.0,
  ""source"": ""articles"",
  ""title"": ""Set up a webhook to start a flow when a Google Form is submitted""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Authenticate a Google Sheets connection by selecting Google Sheets under Applications and completing the connection settings.""
  },
  {
  ""title"": ""Enter Spreadsheet ID and Range in A1 notation to define the specific data to fetch.""
  },
  {
  ""title"": ""Save the export configuration and test the flow to verify data retrieval.""
  }
  ],
  ""thread_id"": ""thread_PxynRa69Oz7AS5aL0IIJUODx"",
  ""vector_duration"": 1.75
 }",,,,,,,,,,,,,,,,
Copilot_useAgent_66,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,Exports_useAgent_001.json,"{
    ""answer"": ""The **Google Drive connector** is primarily for managing files in Google Drive rather than interacting directly with the data inside Google Sheets.  \n\nIt allows you to locate, move, copy, or manage permissions for files stored in Google Drive, including Google Sheets, but it does not provide functionalities like reading or updating specific cell data in a sheet. For that, the Google Sheets connector is the appropriate choice.  \n\nIf your goal is to work with spreadsheet data, stick with the Google Sheets connector. Use Google Drive only for file-level actions."",
    ""completion_tokens"": """",
    ""openai_duration"": 7.97,
    ""prompt_tokens"": """",
    ""sources"": [
        {
            ""articleId"": 23123422014363.0,
            ""content"": ""What's new\nConnect to Anthropic , Hugging Face , and Jasper\nUnlock powerful AI-driven insights and automations by connecting Anthropic, Hugging Face, or Jasper with other business applications, enabling efficient workflows and transformative data intelligence across your operations.\nConnect to Arena\nStreamline product lifecycle management and enhance collaboration across engineering, manufacturing, and supply chain operations when you integrate Arena with other business applications, fostering innovation and operational efficiency across your organization.\nConnect to Arcadier , Clyde , NuOrder , Reverb , Salesforce B2B & D2C Commerce , Salesforce B2C Commerce , Shopware , X-Cart , and Wish\nCreate a cohesive commerce ecosystem by integrating these ecommerce tools with other business applications, enabling seamless data flow, personalized customer experiences, and streamlined operations across online marketplaces, sales channels, and customer service.\nConnect to Paycom\nSimplify payroll and HR processes by integrating Paycom with other business applications, ensuring data consistency, automating workflows, and enhancing compliance across your organization’s entire operational ecosystem\nConnect to Salesforce Marketing Cloud Account Engagement (Pardot)\nEnable seamless data synchronization, automate marketing workflows, and enrich customer engagement strategies when you integrate Salesforce Marketing Cloud Account Engagement with other business applications, driving personalized experiences and improved ROI across marketing and sales channels.\nConnect to Pimberly\nImprove operational efficiency and customer experience by integrating Pimberly with other business applications, enhancing product information management by streamlining data consistency and distribution across sales, marketing, and e-commerce platforms.\nConnect to Corrigo Pro\nOptimize facilities management workflows by integrating Corrigo Pro with other business applications, enabling seamless data exchange, automating service requests, and enhancing operational efficiencies across maintenance, asset management, and vendor coordination.\nConnect to Klarna\nBoost sales and operational efficiency across ecommerce platforms when you integrate Klarna with other business applications, facilitating seamless payment processing, enhancing customer buying experiences with flexible payment options, and streamlining financial workflows.\nConnect to Gladly\nEnable a unified, personalized customer support experience across all channels by integrating Gladly with other business applications, centralizing customer service interactions while streamlining workflows and improving response times.\nWhat’s enhanced\nConnectors simplified\nThis release includes enhanced connectors for the applications listed below. The migration is automatic; you don’t have to make any changes to use your existing connections.\nWooCommerce\nZoho Books\nZoho CRM\nSolidCommerce\nBlob format selection for HTTP file imports\nThe Character encoding setting in HTTP imports allows you to select the preferred blob format for file imports, including options like utf8 and base64.\nPositioned just below the HTTP Request body, this enhancement ensures seamless data compatibility and integration into the destination application. The selected encoding is also reflected in the import schema, streamlining the integration process.\nPlain text media type for HTTP imports and exports\nYou can now select the Plain text media type for Connections, Exports, Lookups, and Imports, enabling seamless parsing and processing of simple textual data.\nThis update facilitates straightforward API interactions, supporting raw string token responses for API requests and ensuring compatibility with APIs that transmit or receive data in plain text format.\nEnhanced user experience in HTTP connector form\nSimplify your configuration process by easily choosing between a prebuilt or universal HTTP connector . Simply enter your Base URI in the universal HTTP connection form and, if available, a prebuilt connector option will become available. Switch between the prebuilt and universal HTTP connector directly in the Create or edit connection panel, giving you full control over which applications you connect with. This update makes setting up your configurations simpler and your experience smoother. Whether you're dealing with multiple applications that share identifiers or need to adjust a connection, you now have the flexibility to ensure your HTTP integrations work exactly as you need them to.\nDatabase connectors enhanced: Oracle DB, MariaDB\nBulk insert : Recommended for large data volumes, use bulk insert SQL query to insert data in bulk, either by selecting a validated table name from a dropdown list or by referencing a dynamic lookup using handlebars expressions.\nAutomapper support : Allow the automapper to automatically fetch and populate columns, eliminating the manual effort of adding columns and the need to toggle between applications.\nAdded latest API versions to the Shopify connection\nIn the Shopify connection, the API versions such as 2023-07, 2023-10, and 2024-01 are added.\nUpdated connector name “Azure Active Directory” to “Microsoft Entra ID ”\nMicrosoft has rebranded and renamed “Azure Active Directory” to “Microsoft Entra ID”, and we have updated the connector name and logo for consistency.\nDeprecated Jira Server platform connector\nJira has deprecated the Jira Server platform, hence we removed the connector from integrator.io. For more information, see Migrate from Jira Server API endpoints to Jira Cloud API endpoints."",
            ""date"": ""Mon, 28 Oct 2024 20:52:32 GMT"",
            ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/23123422014363-Celigo-platform-connectors-2024-3-1-release-notes"",
            ""label_names"": [
                ""Celigo platform connectors 2024.3.1 release notes""
            ],
            ""linkedArticleIds"": [],
            ""numTokens"": 977.0,
            ""score"": 0.753070831,
            ""section_id"": 30450672606107.0,
            ""source"": ""articles"",
            ""title"": ""Celigo platform connectors 2024.3.1 release notes""
        },
        {
            ""articleId"": 20085283685915.0,
            ""content"": ""To connect two or more systems you can either choose point-to-point (direct) connections, middleware, or a customized mix of both. integrator.io can operate with either; however, it shines the most when connecting multiple applications across different business processes and departments of an organization.\nDirect connection vs middleware\nA direct connection allows for communication between two systems. To join any additional system, an extra direct connection is required. For example, when a printer is connected to a computer with a USB cable, the user can send print jobs directly from the printer to the computer.\nDirect solutions are always built for specific problems – for example, to connect an ERP system to a marketplace. This approach might right if you’re not expecting any big changes or expansions, or if your company won’t benefit from the additional functionality a middleware brings.\nIn the past, this approach worked for many years. Today, software development is moving quickly, and companies are switching systems quickly. It is hard to keep the connections up to date with changing API requirements, especially at scale as the footprint grows across many countries. With a point-to-point solution, every new marketplace would need to be enabled and maintained with the same effort as the first one.\nOn the other hand, middleware sits between two or more systems. It can manage different tasks like, but not limited to, the following:\nReceiving requests from applications\nTransforming, routing, or filtering the received data\nForwarding requests and data to other applications\nReceiving, logging, monitoring, and returning status and error information\nThis strategy automates a process between multiple departments within a company or multiple companies.  For example,  you can integrate an online shop, ERP, and third-party logistics provider (3PL). This would include receiving orders from an online shop, fulfilling them in the ERP, and initiating the shipping in the warehouse management system (WMS).\nBenefits of middleware\nGenerally speaking, the more complex the technical landscape, the more necessary middleware becomes.\nSecurity\nMiddleware can offer security features out of the box. Data governance models enable an administrator to grant specific rights and permissions to user roles, to ensure that sensitive information and the integration infrastructure are accessible only to the right people. Establishing access policies during setup minimizes the risk of human errors, while at the same time driving transparency of how the middleware is performing.\nMaintaining all integrations on one platform makes it easier, for example, for the IT department to roll out new security standards and keep track of integrations without overlooking vulnerabilities.\nFeatures\nMiddleware offers valuable features beyond the base functionality of point-to-point connections, such as:\nDashboards, reports, and analytics\nNotification service\nGraphical and intuitive user interface\nTools and templates to help build integrations\nError management\nResources management\nEnvironment management\nAccount management\nHelp, community, and customer service\nScalability\nScalability is the chief differentiator for growing businesses choosing between middleware and point-to-point connections. One of the critical benefits of middleware in this scenario is scalability. You can connect additional systems to seamlessly integrate and extend more business processes if needed. With middleware, resources can be used and reused across the enterprise. It adds unification, a common structure, and transparency to the integrations.\nFor example, when enabling a new e-commerce market, integrations from one country can be copied and adjusted to also work in another country. Features like duplicating integrations can save a lot of time and, therefore, development costs. Changing environments can be adapted to scale to give you a head start on the competition.\nRisk of failure\nPoint-to-point connections built over the years tend to introduce risk since they are often poorly documented and hard to maintain as updates are released. Oftentimes, a single person in a company knows how a connection works. It’s also not uncommon that the team member who built a specific connection leaves the company or retires.\nMiddleware reduces these risks and supports the collaboration of many users who share information. This centralized approach reduces the risk of a single point of failure.\nChoose between point-to-point or middleware\nIn the decision-making process, the key stakeholders of disparate systems come together and align on an integration strategy. Ideally, multiple departments – such as product, e-commerce, IT, marketing, and finance – collaborate on a shared automation roadmap. Then, the company can find common ground for its integration challenges, leveraging the central middleware to operate across functions. Point-to-point solutions too often increase “silo” thinking.\nTransition from point-to-point to middleware\nAnalyze which systems are most critical to migrate to middleware. Start by measuring the number of connections, how often they change, and how often new connections are needed. Continue this analysis by building a roadmap to keep track of what should be moved and when.\nBuild the needed connections with the middleware. It can be useful to keep the connections in parallel and turn on the new middleware connections one at a time to ensure a smooth transition.\nFind opportunities to sync other systems to keep profiting from the new middleware. New automations can be developed most easily by leveraging existing connections."",
            ""date"": ""Wed, 22 Nov 2023 20:12:20 GMT"",
            ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/20085283685915-Point-to-point-direct-vs-middleware-connections"",
            ""linkedArticleIds"": [],
            ""numTokens"": 1026.0,
            ""score"": 0.75263083,
            ""section_id"": 16786506755611.0,
            ""source"": ""articles"",
            ""title"": ""Point to point (direct) vs middleware connections""
        },
        {
            ""articleId"": 10841316566043.0,
            ""content"": ""There are hundreds of application connectors, one universal HTTP connector, and one universal GraphQL connector. How do you choose between a simple, pre-built application (with or without an HTTP or GraphQL form view) or a more granular universal connector? Each type provides different configuration options, and your chosen type determines how quickly, easily, and detailed your connection configurations get.\nWarning\nAuthentication types will vary depending on your connection type and connector.\nPre-built application connectors\nPre-built application connectors are designed to optimize your connection to specific platforms. The simple, pre-built application connector will grant you easy access to your application. You may not be able to modify some aspects as carefully as you’d like, but you’ll have a complete, easy, and secure connection to your application.\nAfter you find your application connector, you can toggle between the Simple, HTTP, or GraphQL forms. Whether you choose a Simple or HTTP/GraphQL form view, you're still using a pre-built application connector. The more complex HTTP/GraphQL forms include additional configuration options usually reserved for the standard universal HTTP or GraphQL connections.\nSimple vs. HTTP or GraphQL forms\nAfter choosing your pre-built application connector, you can toggle between a Simple or HTTP/GraphQL form view for more detailed customization. The Simple form view uses the minimum number of fields and configurations while maintaining a secure connection. The HTTP or GraphQL forms within your pre-built application connector provide additional configuration options pulled directly from the universal HTTP or GraphQL connectors.\nSimple vs HTTP-based form toggle\nSimple vs GraphQL-based form toggle\nThe differences between a simple or form view (HTTP or GraphQL) can be radical, and you should always consult the universal connector guides for HTTP or GraphQL, as some fields are pulled directly from these connectors.\nNote\nThis toggle will become available for more connectors over time. It’s not currently available for every connector.\nSimple forms only include the most common settings required for integrating with the application. A simple connection provides the maximum security available without configuring many fields.\nIf HTTP and GraphQL are available, you may need to pick the API type. This will determine how you use your application's API(s) and may change configuration options. Check your application's API requirements to determine which API type suits your needs.\nSimple application form view (HTTP-based API type)\nSimple application form view (GraphQL-based API type)\nThe HTTP or GraphQL forms have more configuration, control, and authentication options but are still not identical to the universal HTTP or GraphQL connectors. Consider these forms a simplified version of the universal HTTP or GraphQL connector. Toggling to the HTTP or GraphQL form allows you to configure additional details using the application connector.\nHTTP application form (see toggle)\nGraphQL application form (see toggle)\nAn HTTP or GraphQL form differs from a universal HTTP or GraphQL connector. For example, creating an iClient directly in the Simple or HTTP form view is less granular than using a universal HTTP connector.\nUniversal connectors\nThe universal HTTP or GraphQL connectors give you the finest level of granularity when configuring your connection. Using these connectors, you can customize almost every aspect of your connection. The universal connectors will grant you more control over scopes, authentication types, permissions, requests, and responses.\nWarning\nAuthentication types will vary depending on your connection type and connector.\nAfter entering the Base URI in a universal connector, you can still select the prebuilt application connector, if available. If you choose the prebuilt application connector, the Base URI field is disabled, and the Simple form view will appear.\nTo review documentation for the Simple form view, navigate to the Celigo integrator.io Help Center and search for your connector. For example, if you switch to the Simple view for the Stripe connector, you can search for \""Stripe\"" or \""Set up a connection to Stripe\"" to find the application-specific articles. To remove the simple form view, click Clear selection. You can ignore the Simple Form view and continue using the HTTP Form view.\nChoose between an application or a universal connector\nThere are differences between an application and a universal connector. Application-specific forms are based on the universal connector but include only the most common settings required for integrating with the application. You’ll know you’re using the universal connector if you select the HTTP or GraphQL application when creating your connection.\nLet’s take a quick look at the differences between these connections:\nConnection type\nMock output and response data\nAutomatic latest API version of resources and endpoints\nQuery parameters (export and import)\nApplication –Simple form view\nQuick connection\nYes\nYes\nYes\nApplication – HTTP or GraphQL form view\nConfigure HTTP/GraphQL connection\nYes\nNo\nNo\nUniversal HTTP or Universal GraphQL\nConfigure HTTP/GraphQL connection\nYes\nNo\nNo\nThese connector differences are most visible at Resources → Connections → Type. When you view your connections, the Type column will display whether the application is HTTP/GraphQL or application-based.\nSimple vs universal exports, lookups, and imports\nYou can toggle between the simple or HTTP/GraphQL form view of an export, lookup, or import. The simple views include:\nMock output and response data\nQuery parameters (export and import) – You can search through all query parameters.\nMandatory parameters are provided for you and disabled so they can’t be removed.\nSearch for, add, or delete features for query parameters.\nHandlebars to add query parameters or values.\nSelect the setting’s value from a drop-down list when the available choices are documented and limited.\nError messages remind you if a value isn’t set for a mandatory parameter.\nFor resources and endpoints with different API versions\nintegrator.io will default to the latest version.\nYou can choose a resource, endpoint, and version, but you can only choose the version if it isn’t in the connection."",
            ""date"": ""Mon, 14 Apr 2025 16:15:33 GMT"",
            ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/10841316566043-Choose-between-an-application-or-a-universal-connector"",
            ""label_names"": [
                ""UI"",
                ""HTTP"",
                ""toggle"",
                ""generic"",
                ""generic forms"",
                ""connectors"",
                ""REST API"",
                ""application connectors"",
                ""views"",
                ""Simple view""
            ],
            ""linkedArticleIds"": [
                ""360048237032"",
                ""4843857027227"",
                ""360039586072"",
                ""4843857027227"",
                ""360039586072"",
                ""4843857027227"",
                ""11933835192859"",
                ""360048237032"",
                ""4843857027227"",
                ""11900462442267""
            ],
            ""numTokens"": 1154.0,
            ""score"": 0.750852227,
            ""section_id"": 5926003362459.0,
            ""source"": ""articles"",
            ""title"": ""Choose between an application or a universal connector""
        }
    ],
    ""suggestions"": [
        {
            ""title"": ""Decide whether to fetch data using Google Sheets or manage files using Google Drive.""
        },
        {
            ""title"": ""Set up a connection for the selected Google Sheets or Google Drive connector.""
        },
        {
            ""title"": ""Configure an export to test data fetching or file management based on your selected connector.""
        }
    ],
    ""thread_id"": ""thread_dLbr2UsLI69eveQ3doNGOzOP"",
    ""vector_duration"": 2.22
}",,,,,,,,,,,,,,,,
Copilot_67,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,Marketplace_001.json,"{
  ""answer"": ""Based on the documentation and the context of your current page, here\u2019s the difference between an **Integration App (IA)** and a **Template**:\n\n### **Integration App (IA):**\n1. **Definition**: An Integration App is a full-package integration solution that connects two applications in a **standard, managed way**.\n2. **Features**:\n  - Provides **bug fixes, new functionality, and updates** from the app developer.\n  - Comes with **full documentation** and **domain-specific support**.\n  - Components within the app (flows, connections, etc.) are **managed and locked** to ensure upgrade compatibility and reliability.\n3. **Use Case**: Best for **standard workflows** between two applications where a managed, ready-to-use solution fits your requirements.\n4. **Installation**: You typically request a **demo** before installation.\n5. **Examples**: Suitable for businesses seeking ongoing support and smoother upgrades for their integrations.\n\n---\n\n### **Template**:\n1. **Definition**: A Template is a **prebuilt integration bundle** that connects two or more applications, but it is entirely **customizable** after installation.\n2. **Features**:\n  - Provides **pre-designed components** (flows, connections, exports, imports, data mappings, etc.) but allows you to modify them as needed.\n  - Does not include updates, bug fixes, or ongoing support from the developer after installation.\n  - Ideal for **customization** and flexibility since you have complete control after installation.\n3. **Use Case**: Best for **custom workflows** or when there\u2019s no Integration App available for the applications you want to connect.\n4. **Installation**: Templates are available in the Marketplace and can be installed directly through integrator.io.\n\n---\n\n### **Key Difference**:\n- **Integration Apps** are fully **managed solutions** for standard integrations and offer ongoing support, updates, and upgrades. The components are locked for reliability.\n- **Templates** are customizable and designed for flexibility, allowing you to modify them according to your business's specific needs. However, they do not receive updates or support from the creator post-installation.\n\n---\n\n### Recommendations for Your Current Page:\nOn the **Marketplace \u2013 Popular Integration Apps and Templates** page, you can:\n1. **Choose an Integration App** if you're looking for a reliable, maintained integration that works out of the box.\n2. **Choose a Template** if you need maximum flexibility and are comfortable customizing the integration to fit your unique requirements.\n\nYou can use the **search bar** or the filters on the left-hand side to explore Integration Apps and Templates based on your needs. If you're unsure, click **Preview** or **Readme** on the listing to learn more about each option before proceeding."",
  ""completion_tokens"": """",
  ""openai_duration"": 23.14,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 4403702343195.0,
  ""content"": ""The integrator.io Marketplace (U.S.\u00a0|\u00a0EU) allows you to jump-start the integration process with templates and apps. The templates provide prebuilt components for integrations between applications. You can also explore, request demos, and install new Integration Apps.\nNavigate the marketplace\nYou can find the marketplace within integrator.io at Marketplace in the main menu.\nYou can sort and search for a specific template or integration app on the left-hand side of the screen. There are four ways to sort your applications:\nSearch bar \u2013 Search for a specific template or integration app.\nBy application \u2013 Sort through the frequently selected apps or sort from A to Z.\nBy type \u2013 Sort through applications by All, Integration app, or Template.\nBy group \u2013 Sort through applications by Most popular, Most recent, or A to Z.\nYou can also combine these methods to find an application or template quickly.\nTemplates\nQuickstart integration templates are bundled integrations between two existing applications. For example, you can install a 3PL Central \u2013 NetSuite integration by downloading the existing template in the Marketplace. Integration templates provide predesigned components for integrations between applications.\nComponents\nThese components are available for each template:\nFlows: Transfers data from a source application to a destination application\nIntegrations: Data integrated between applications\nConnections: Connected applications\nExports: Data extracted from an application\nImports: Data imported to an application\nInstall template\nAfter you sign up for integrator.io, click the marketplace icon in the left navigation bar.\nYou can view all templates and integration applications by scrolling through the alphabetized list or searching for an application using the search bar.\nFind and click the template you want to install.\nClick Preview and review the components of the template.\nClick Readme for installation information\nClick Install Now to install the template.\nApps\nIntegration applications provide a full-package integration between two applications. Integration applications are installed after requesting a demo.\nLearn more about what the Celigo Marketplace offers on Celigo.com."",
  ""date"": ""Fri, 15 Mar 2024 15:04:25 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/4403702343195-Jump-start-integrations-with-templates-and-apps"",
  ""linkedArticleIds"": [
  ""235469468""
  ],
  ""numTokens"": 404.0,
  ""score"": 0.772916,
  ""section_id"": 360008075111.0,
  ""source"": ""articles"",
  ""title"": ""Jump-start integrations with templates and apps""
  },
  {
  ""articleId"": 360038510692.0,
  ""content"": ""Overview\nQuickstart templates are pre-built integrations and come packaged with all the components that you need to integrate two or more applications (including connections, exports, imports, integration flows, and data mappings). You can install a Quickstart template in your integrator.io account and modify it to fit your custom integration requirements.\nQuickstart templates help you to:\nAutomate your business processes and eliminate manual non-productive tasks like creating and maintaining dual-entries between the two apps.\nLet your IT team focus on core development instead of spending time and effort on developing and maintaining integrations.\nSave on your operational costs by automating manual tasks.\nScale-up your operations efficiently using our custom integration solutions.\nKey Celigo terms\nFamiliarise yourself with some key terms used throughout this guide.\nQuickstart templates\nThe term template refers to the integration that processes and moves data between your apps. In technical terms, it refers to a group of flows in a folder or template. Templates appear as tiles on your integrator.io homepage. You can configure, run, manage flows from the settings page within integration, and monitor flow jobs via the dashboard.\nIntegration Flows\nThe term flow refers to a stream of data exchanged between two or more apps or systems that help you achieve your data integration goals. A flow:\nExports data from one or multiple systems or apps.\nProcesses the data based on the flow settings.\nImports the processed data to another system or app.\nTypes of flows:\nA flow that performs more than one export or import is called orchestration or orchestrated Flow.\nFlows that sync data between two apps or systems in real-time are called real-time flows.\nScheduled flows are manual flows that can be scheduled to run in batches at predefined times and intervals or on an on-demand basis.\nA flow that is running is called an active flow. You can run multiple flows simultaneously.\nConnection\nYou\u2019ll need to provide adequate permissions to a template to access a system or app before you move your data. To grant template permissions, log-in to your system or app via the template set up to establish a connection between the template. Each app or system requires varied inputs to establish a connection. The most commonly used inputs for Celigo\u2019s integrations are:\nUsername and password\nAPI key\nTokens\nURLs\nSee also\nIntegration App vs Template in integrator.io\nRead more about Quickstart integration templates:\nCeligo prebuilt integrations"",
  ""date"": ""Tue, 19 Mar 2024 18:09:04 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/360038510692-Quickstart-integration-templates"",
  ""linkedArticleIds"": [
  ""235469468""
  ],
  ""numTokens"": 485.0,
  ""score"": 0.772836149,
  ""section_id"": 360007663012.0,
  ""source"": ""articles"",
  ""title"": ""Quickstart integration templates""
  },
  {
  ""articleId"": 360048995431.0,
  ""content"": ""Are you an integrator.io power user who\u2019s solved a problem that others might benefit from? Or, perhaps you have an entrepreneurial bend and you\u2019ve thought of a business automation purpose for a flexible iPaaS in addition to app-to-app data syncing?\nThe Celigo integrator.io platform provides a simple and easy way to create prebuilt integration templates and integration apps, and we\u2019re looking to join forces with the best integration developers who want to create, distribute, and support their own products.\nWhat\u2019s new\nWhile we\u2019ve long partnered with app vendors and template/integration app authors to support compatible proprietary code, the new Integration App Framework brings the development process into integrator.io. Now you can control the components, form logic, and licensing of an Integration App directly in the platform, and also make use of JavaScript and integrator.io API calls.\nYou can still rely upon server stacks or redistributable \u201cbundle\u201d code, but you\u2019ll definitely want to take advantage of the new features that essentially open integrator.io up as a development environment.\nHow to get started\nSign up for an integrator.io account.\nContact your Celigo account executive or partner manager to sign up for our 3rd Party developer program in order to build templates and integration apps on the integrator.io platform.\nIf accepted into the program, you will be granted permission to publish templates and integration apps in your account, which enables the following Resources menu options.\nIf you prefer working with the integrator.io REST API, be sure to create your API Tokens first.\nLearn more\nUnderstand the difference between Integration Apps and Quickstart integration templates\nRead our API documentation\nCreate custom settings and forms\nHow to create and publish templates and integration apps\nStep 1: Create the integration in your integrator.io account\nThis process is the same as creating any integration in your integrator.io account, by creating flows for the business use cases that you would like to package in your integration, as demonstrated below. The flows also include any exports, imports, mappings , JavaScript hooks for business logic, and so forth, that you would like to distribute with your App.\nStep 2: Create settings for your integration\nYou can add settings to your integration to ask the customer for information.\nYou will then be able to access the selected values throughout the integration to make flow decisions at runtime.\nStep 3: Publish your integration\nTo publish an integration app, follow these steps as demonstrated in the video below:\nSelect Resources \u2192 Integration apps \u2192 + Create Integration app.\nDescribe the new App and save the Integration App listing.\nIn your new Integration apps listing, enable the Published setting.\nOr, to publish a template, complete the following steps as illustrated in the video below:\nDownload the integration as a .zip file.\nSelect Resources \u2192 Templates \u2192 + Create template.\nDescribe the new template and upload it.\nIn your new template listing, enable the Published setting.\nOnce published, navigate to the integrator.io Marketplace to see your new listing with your company\u2019s name as the author.\nThe customer experience will be the same as when installing any Celigo-published template or integration app from the Marketplace.\nStep 4: Provision user licenses (integration apps only)\nIn your Integration apps page, select Licenses from the Actions overflow (...) menu. Then, in the resulting Licenses page, you can add, edit, or delete a license for each user account. See this step in action, along with viewing summaries of your licensees and install base:\nUnpublish your integration\nTo unpublish an integration app, follow the steps below:\nSign in to your Celigo platform account.\nSelect Resources \u2192 Integration apps.\nIn your Integration apps list, disable the Published setting of the integration app you want to unpublish.\nIn the Confirm Unpublish window, click Unpublish.\nTo unpublish a template, follow the steps below:\nSign in to your integrator.io account.\nSelect Resources \u2192 Templates\nIn your templates list, disable the Published setting of the template you want to unpublish.\nIn the Confirm Unpublish window, click Unpublish.\nFAQs\nDo I need a sandbox instance to develop Integration Apps?\nIt\u2019s a best practice to get started there but not a mandatory requirement at this time.\nWhat does the certification process look like?\nAt this point, you would demo your application to the Celigo product team once you are ready. Once the demo is approved, you would be able to list on the Marketplace. However, this process may evolve in the future."",
  ""date"": ""Wed, 21 May 2025 14:57:08 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/360048995431-Publish-or-unpublish-templates-and-integration-apps"",
  ""label_names"": [
  ""IA developer"",
  ""Celigo partners"",
  ""TPP"",
  ""Integration App Framework"",
  ""IAF 2.0"",
  ""Unpublish your integration"",
  ""Publish your integration"",
  ""unpublish a template"",
  ""unpublish an integration app"",
  ""publish a template""
  ],
  ""linkedArticleIds"": [],
  ""numTokens"": 905.0,
  ""score"": 0.765670955,
  ""section_id"": 360010287971.0,
  ""source"": ""articles"",
  ""title"": ""Publish or unpublish templates and integration apps""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Search for an Integration App for a standard integration solution.""
  },
  {
  ""title"": ""Search for a Template if you require a customizable integration.""
  },
  {
  ""title"": ""Preview or install a Template or Integration App to explore its components and functionality.""
  }
  ],
  ""thread_id"": ""thread_Y7qSFTH21TgJ7ZR0DgKLMT5w"",
  ""vector_duration"": 3.99
 }",,,,,,,,,,,,,,,,
Copilot_68,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,Clone_Integration_001.json,"{
  ""answer"": ""In the context of the **'Clone details - Clone integration'** page you are on, **\""Clone integration\""** means creating an **exact copy of an existing integration**. This action allows you to duplicate all flows, steps, scripts, and configurations from the original integration into a new integration that you can modify independently without affecting the original integration.\n\n### Key Details about Cloning an Integration:\n1. **Purpose**:\n  - It helps you **test changes**, develop new features, or create variations of the original integration without impacting the production environment.\n  - You can use the clone to make changes and introduce updates systematically, pulling or merging changes into clones or back into the original integration.\n\n2. **What Happens When You Clone?**:\n  - Every component of the integration (flows, exports, imports, scripts, etc.) is duplicated.\n  - However, **connections are not copied**; you will need to configure the connections for the cloned integration.\n  - Optionally, the lookup cache data is not included unless explicitly specified.\n\n3. **Creating Clones in Different Environments**:\n  - You can clone an integration to the current environment or to a different one (e.g., from Sandbox to Production).\n  - When cloning into another environment, you will need to complete the necessary configuration steps once the process is finished.\n\n4. **Steps to Clone Your Integration (Summarized)**:\n  - Enter a unique name for the cloned integration so you can easily identify it.\n  - Specify the environment where the clone will reside.\n  - Click **Clone Integration** to create the duplicate.\n\n5. **Use Cases**:\n  - If you want to test changes safely, you can clone your live integration, make updates in the clone, and then push those changes back to the original.\n  - If you maintain multiple environments (e.g., Development, Quality Assurance, and Production), cloning helps you manage workflows and move changes across each environment in an organized manner.\n\n---\n\nIn your current context, after reviewing and entering the necessary clone details (such as name and environment), you would proceed to click **Clone integration** at the bottom of the page to create the new clone."",
  ""completion_tokens"": """",
  ""openai_duration"": 14.95,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 17585804904347.0,
  ""content"": ""Develop integrations faster with stable changes\nUse cloning to speed up the process of creating integrations and managing changes. You can clone an existing integration to create an exact copy, make changes in the source integrations, and systematically pull the new changes into the cloned integration or vice versa. Snapshots are automatically created whenever you merge changes, which helps you return to a previous state if you want to revert your changes.\nTip\nTo use ILM, you need to clone an entire integration, which requires 'Manage' access to the integration. Ideally, you should create integrations with 'Administer' access and invite users to the integration based on access level (Manage or Monitor). Then, users with Manage access can create clones.\nAvoid independently cloning integration resources (like lookup caches, exports, or imports) and reusing them within the same integration, as it can lead to inconsistencies with revisions.\nImagine your original integration is running in production. You can create a clone (integration A) (Amazon-NetSuite \u2192 Clone of Amazon-NetSuite). In this case, integration A is the clone integration and original is the source. You can keep the two integrations in sync or introduce new changes in the source and then pull the new changes into your clone or vice versa.\nYou have now recreated several flows, steps, scripts, and configurations. Everything is copied. Now, to introduce a new change, use the source integration, test everything, and merge new changes to integration A using a pull request.\nApproach: Original integration \u2192 Integration A clone of the original integration Example: Amazon-NetSuite \u2192 Clone of Amazon-NetSuite\nYou can also create multiple clones of the original integration. This is helpful when you want to create and maintain a large number of integrations based on a common source integration. In this case, you can introduce changes in the source integration and then pull changes into any of the clones.\nTip\nDon't forget to repopulate the lookup cache data after reverts.\nSandbox and production integrations\nIf you have access to the sandbox environment, create an integration in the sandbox first. Then, create a clone in the production environment. When you introduce new changes, make them in the sandbox integration and, once everything is working as expected, pull the newly tested changes into the production integration.\nApproach: Original integration (sandbox environment) \u2192 Integration A, a clone of the original integration (production environment) Example: Amazon-NetSuite (sandbox environment) \u2192 Clone of Amazon-NetSuite (production environment)\nDevelop integrations collaboratively\nYou can extend the above concepts to manage work across multiple teams and environments.\nTip\nProvide Manage access only to Developers in Dev tiles and Manage access to QA only in QA tiles.\nYou can separate work according to your development and test teams. Simply create clones of each integration and provide access to the cloned tiles based on dev and test teams. When development work is finished, changes can be pulled in for testing by the QA team and, when everything is working, final changes can be pulled into the original integration.\nYou can also manage development by assigning a clone to each developer for independent development. Whenever a developer completes a task, the changes can be pulled into the original integration. Later on, other developers can pull from the original integration to incorporate the new changes into their own clones.\nYou can further separate work across different environments and make changes in both sandbox and production environments. In your Sandbox, you can develop your integrations, test any changes, and pull those stable changes into your production integration.\nTip\nUse the Sandbox environment to create Dev tiles and QA tiles and the Production environment to create Prod tile."",
  ""date"": ""Thu, 22 May 2025 17:34:36 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/17585804904347-Developing-with-integration-lifecycle-management"",
  ""label_names"": [
  ""Sandbox"",
  ""Integration Lifecycle Management"",
  ""develop integrations faster"",
  ""stable changes"",
  ""cloning integrations"",
  ""integration snapshots"",
  ""sandbox environment integrations"",
  ""production environment integrations"",
  ""managing integration changes"",
  ""collaborative integration development"",
  ""integration access levels"",
  ""Primary Information""
  ],
  ""linkedArticleIds"": [
  ""6685420081307"",
  ""115003929872"",
  ""6767143777179"",
  ""6685876051099"",
  ""17584559539355""
  ],
  ""numTokens"": 737.0,
  ""score"": 0.84949249,
  ""section_id"": 17581503702939.0,
  ""source"": ""articles"",
  ""title"": ""Developing with integration lifecycle management""
  },
  {
  ""articleId"": 6661875290523.0,
  ""content"": ""Below are the most common terms and phrases used when discussing integration lifecycles. It's important to remember these terms since they'll be used often throughout the ILM documentation.\nTerm\nDefinition\nClone\nA clone is an identical copy of an integration that you can modify independent of the original integration.\nConflict\nConflicts are competing changes between your current integration and the remote integration.\nCurrent\nA current integration is an integration that you work on and into which you pull changes from a remote integration.\nMerge\nWhen you merge integrations, you accept changes and update resources from the remote integration to your current integration. Merge is a part of a pull.\nOriginal\nAn original/source integration is an integration from which you create a copy or a clone.\npageGenerators (PG or Exports)\nExports are called pageGenerators (PG) in the JSON structure of your integration flow.\nPull\nWhen you pull from a remote integration, you compare the fetched data and merge changes into your current integration.\npageProcessors (PP or Imports)\nImports and Lookups are called pageProcessors (PP) in the JSON structure of your integration flow.\nRemote\nA remote integration is an integration from which you\u2019re pulling changes.\nResource\nA resource is any import, export, flow, script, or another part of an integration.\nRestore\nIf you restore your integration, you\u2019re recovering a previous revision and rolling back to it.\nRevert\nA revert undoes changes and allows you to roll back to a previous revision. You can roll back to any previous pull, snapshot, or revert revision.\nRevision\nA revision can be any of the following types: snapshot, pull, or revert.\nSandbox\nA sandbox environment is available for testing. If your integrator.io doesn\u2019t include a sandbox, you can test in your existing environment or contact Celigo to learn more about adding a sandbox.\nSnapshots\nA snapshot is a copy of an integration state that you create at any point to use as a backup when required.\nSource\nA source/original integration is an integration from which you create a copy or a clone."",
  ""date"": ""Fri, 20 Sep 2024 17:15:09 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/6661875290523-Integration-Lifecycle-terminology"",
  ""linkedArticleIds"": [],
  ""numTokens"": 423.0,
  ""score"": 0.844489217,
  ""section_id"": 6661790030107.0,
  ""source"": ""articles"",
  ""title"": ""Integration Lifecycle terminology""
  },
  {
  ""articleId"": 6685420081307.0,
  ""content"": ""You can clone an existing integration to create an exact copy for making, auditing, or testing changes in any environment.\nImportant\nRemember, for Integration Lifecycle Management specifically, cloning is done at the integration level, not the flow level. You can't clone one flow; you must clone the entire integration. You can still change a flow; however, this restriction means you must clone the entire integration to manage or test a flow using Integration Lifecycle Management.\nDon't forget that you can leverage concurrencies and multiple connections to help your integrations run smoothly.\nYou must have Manage access to the specific integration to create a clone. To use the pull feature, first create a clone in one of two ways using the existing clone functionality:\nClone an integration\nUse the following steps to clone an integration:\nEnsure you're in the correct environment by looking for the environment name at the top, right-hand side of the screen. If you only have one environment you won't see any other environments available.\nOn the Home page, go to the integration.\nBased on the integration view you use, follow the steps given below.\nTile view: Click the overflow (...) menu in the upper-right corner of the tile and select Clone integration.\nList view: In the Actions column, select Clone integration from the overflow (...) menu.\nIntegration dashboard: Click Clone integration.\nIn the Clone integration page, enter details.\nName: Enter a unique name for the clone. Make sure that it is unique enough that you can identify it later. You can always rename your integration tile or flow outside of the cloning process.\nEnvironment: Select your environment. Only active environments you have Manage access to (or above) are visible. After cloning to a new environment, you must complete the setup steps; however, you can choose to complete the setup later. Learn more about using multiple environments.\nReview the list of all cloned items contained within the new integration.\nClick Clone integration at the bottom of the list of cloned integration components.\nIf you've successfully cloned your integration, you'll see a success message asking if you'd like to configure your cloned integration now or later. If you've cloned to a different environment, you'll be asked if you want to switch to that environment to configure the integration.\nConfigure the cloned integration\nDepending on the integration, you must complete one or more steps to configure the cloned instance of the integration. Connections are not copied into the cloned integration, so the first step is configuring each app's connections.\nFor each connector, click the Configure link, then provide the requisite information for the connection and test your connection. See Connector solutions for help setting up connections for particular connectors.\nOnce you\u2019ve successfully configured your connections, the Integration details page will open, showing all of your data flows. Here, you can turn flows off and on, schedule or run them and modify their field mappings.\nClone using the ZIP file\nYou can clone an integration using a downloaded integration ZIP file. If you don't have access to the production environment or the integration, you can upload an integration ZIP file sent to you to install the integration as a clone."",
  ""date"": ""Thu, 30 Jan 2025 16:24:03 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/6685420081307-Clone-an-integration-for-Integration-Lifecycle-Management-ILM"",
  ""label_names"": [
  ""Clone"",
  ""ILM"",
  ""Clone an integration"",
  ""Clone button"",
  ""Clone zip file"",
  ""Integration Lifecycle Managment"",
  ""How to clone integration"",
  ""Create clone"",
  ""SDLC"",
  ""software development lifecycle"",
  ""version management"",
  ""Software Development Lifecycle"",
  ""version control"",
  ""git clone"",
  ""git"",
  ""git commit"",
  ""revision control"",
  ""revision management"",
  ""release management""
  ],
  ""linkedArticleIds"": [
  ""32873227888155"",
  ""360043926372"",
  ""115003929872"",
  ""4419194520219"",
  ""32873227888155"",
  ""115005571007"",
  ""4536629083035"",
  ""235469388""
  ],
  ""numTokens"": 619.0,
  ""score"": 0.837332547,
  ""section_id"": 5918596617883.0,
  ""source"": ""articles"",
  ""title"": ""Clone an integration for Integration Lifecycle Management (ILM)""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Enter a unique name and select the environment for the clone.""
  },
  {
  ""title"": ""Click \""Clone integration\"" to create your duplicate integration.""
  },
  {
  ""title"": ""Configure connections and test the cloned integration for proper functionality.""
  }
  ],
  ""thread_id"": ""thread_yg9HnqysBzoSU68nYuE2okox"",
  ""vector_duration"": 1.46
 }",,,,,,,,,,,,,,,,
Copilot_69,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,Integration_Settings_001.json,"{
  ""answer"": ""Integration settings are extremely useful as they allow you to customize and control the behavior of your integration and its components (flows, connections, imports, and exports) in a flexible and granular way. Since you are on the **Settings** page, this is where you can define and manage these settings effectively to achieve the following benefits:\n\n### 1. **Parameterize Logic for Customization** \n  Integration settings allow you to define custom settings and global parameters for an integration. These settings can be accessed throughout the integration to dynamically influence how your flows operate. For instance: \n  - You could define a setting so that only paid Shopify orders sync to Salesforce.\n  - Custom settings allow you to handle complex, scenario-specific integrations without hardcoding changes.\n\n### 2. **Simplify Configuration for Non-Technical Users** \n  By using the **Form Builder** in the Settings tab, you can create custom forms to expose and manage your settings fields. This eliminates the need for less technical users to interact with raw JSON data. They can easily use user-friendly forms to modify flow or integration behaviors.\n\n  - **Actionable Tip**: Click on **Launch form builder** on this page to create a custom form. This enables you to make the settings accessible to both technical and non-technical users.\n\n### 3. **Enable Advanced Behavior Across Resources** \n  Integration settings extend the flexibility of your integration by allowing custom logic to persist across:\n  - **Integration level**: Global changes that apply across all flows.\n  - **Flow level**: Behavior specific to an individual flow.\n  - **Connection level**: Settings specific to the apps you're connecting.\n  - **Export and Import levels**: Modify how data is sent or received.\n\n  For example:\n  - If you define a field at the integration level, every flow within the integration can reference it.\n  - These settings can be accessed in custom hooks, filters, and transforms to dynamically change flow behaviors at runtime.\n\n### 4. **Customization with Real-Time Inputs** \n  You can define settings that:\n  - Prompt users for inputs through forms.\n  - Automatically adjust behaviors based on those inputs.\n  - Retain these inputs for future use in your integration logic.\n\n  **Example**: Use settings to dynamically load data from one system and reformat or filter it based on custom criteria before sending it to the next system.\n\n### 5. **Scope and Accessibility of Settings** \n  Settings are cumulative, meaning you can work with the settings defined at various levels:\n  - Integration-level settings flow down to flows, exports, imports, and connections.\n  - You can access these settings within scripts, filters, or transforms to adjust behavior dynamically.\n  - For example, in a flow hook, you can access both the flow's settings and the integration's settings.\n\n### 6. **Predefine Behavior for Flow Runtime** \n  - Settings can be used to change the behavior \""at runtime\"" using scripts (such as hooks, filters, or transforms). For example:\n  - Alter flow mappings dynamically.\n  - Create logic that triggers based on specific settings values.\n\n### **Recommendations for You on the Current Settings Page** \n1. **Launch Form Builder:** If you want to simplify the management of settings, use the Form Builder to create custom forms for your integration's settings fields.\n2. **Define Custom Settings:** Use the JSON editor to add fields that define custom integration-level settings. These fields enable you to parameterize the logic for your integrations.\n3. **Access Advanced Tools:** If you're experimenting or testing, use the **Dev Playground** to try out settings and explore pre-built form templates (like dropdowns or multi-column layouts).\n\nBy effectively configuring integration settings, you can streamline your workflows, handle more complex scenarios, and tailor your integrations to meet your business needs with precision. Let me know if you'd like help with a specific action on this page!"",
  ""completion_tokens"": """",
  ""openai_duration"": 29.13,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 6844686337563.0,
  ""content"": ""You can access your Settings tab in both integration apps and standalone integrations. Modifying your settings helps you customize your forms and fields at the integration, flow, export, import, or connection level.\nIn an integration app, the Settings tab allows you to configure global options for the app or a single store.\nFor standalone integrations, this JSON-enabled resource creates parameter logic for your integration. The settings you define here customize your integration\u2019s actions in more complicated cases.\nImportant\nYou\u2019ll need at least Manager access enabled for standalone integrations.\nLet\u2019s say that in a custom Shopify \u2013 Salesforce integration, you want all the flows syncing Shopify orders to sync only paid orders. The video below shows you how to make a form, then customize your settings so all your integrations sync paid orders.\nUnderstand form builder\nClick Launch form builder to create a custom form to expose and manage your settings fields, so that less technical users don\u2019t need to work with raw JSON data.\nBefore you can create a custom form, you should become familiar with Form builder. It helps you create forms you can use within the integrator.io user interface. In this case, you can allow data to persist among integration objects and define form settings (also known as form elements, controls, fields, and inputs).\nYou can find a working Form builder model with sample JSON for each setting in Dev playground .\nCreate a custom form\nCustom forms let you control the values of custom settings in convenient forms that can be extended with scripts. Custom forms allow less technical users to easily configure flows to suit their needs.\nWhen you\u2019re creating a form, you can prompt a user for information, make changes based on those selections, and capture the results after the form is submitted.\nCreate custom settings\nCustom settings allow you to define additional attributes on resources that can then be accessed within the logic of your integration.\nYou can use custom settings in combination with custom forms to create powerful customizations to any integration, flow, or connection.\nSee also\nPublish templates and Integration Apps"",
  ""date"": ""Tue, 20 May 2025 18:34:25 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/6844686337563-Manage-your-Settings-tab"",
  ""label_names"": [
  ""settings tab"",
  ""Settings"",
  ""settings in integration"",
  ""how to use custom settings"",
  ""integration settings"",
  ""custom settings"",
  ""settings form builder"",
  ""What are settings"",
  ""How to use settings tab"",
  ""Custom forms"",
  ""flow settings"",
  ""connection custom settings""
  ],
  ""linkedArticleIds"": [
  ""360043040652""
  ],
  ""numTokens"": 406.0,
  ""score"": 0.822131813,
  ""section_id"": 6306461554075.0,
  ""source"": ""articles"",
  ""title"": ""Manage your Settings tab""
  },
  {
  ""articleId"": 360045461272.0,
  ""content"": ""Custom settings allow you to define additional attributes on resources that can then be accessed within the logic of your flows.\nCustom forms allow users of your resources to control the values of these custom settings in convenient forms that can be extended with scripts. Custom forms give less technical users the ability to easily configure flows to suit their needs.\nDefining custom settings\nYou can define custom settings for the following resource types:\nIntegrations, via the Settings tab\nFlows, via the \u200b Settings pane\nConnections, exports, and imports, via the Custom settings section of their respective forms\nSettings can be defined only for resources to which you have Manager (write) access. Once settings are defined and a custom form is built for those settings, then any user with Manager access can modify the values for those settings.\nSpecify simple custom settings\nYou can define settings by entering valid JSON objects into the Settings editor. In this example, we are introducing two fields to the settings of an integration:\nSimilarly, in this example, we are adding a field to a flow within an integration:\nUsing custom settings in flows\nAs mentioned above, integrator.io allows you to define settings for all of the key resource types. The settings on each resource are independent, which means that \u2013 for example \u2013 the settings on a flow can\u2019t be defined in terms of the integration that the connection is used in.\nScope of settings\nCustom settings are available for each integration and its resources: flow, connections, exports, and imports. Because of the relationship between these objects, custom settings are cumulative, and integrator.io makes it easy for you to access settings from the various scopes.\nFor example, when working on a hook for an export in a flow, you will have access to the settings for:\nThe export itself;\nThe flow the export is a part of;\nThe connection that the flow uses; and\nThe integration that the flow belongs to\nWhere available, you will find settings as an object at the same level as the actual flow record data itself. For example, to continue the example from the previous section, notice the settings object below and the way in which the individual integration and flow setting attributes appear within it:\nUsing settings to change flow behavior\nOnce you have defined settings, you can use them to change the behavior of your flows. You can currently access the settings object in filters, transforms, and hooks.\nFilters: the settings object is available in both the rules and script-based filter. In the rules-based view, notice that all settings appear in the drop-down list for the second operand.\nTransforms: the settings object is available only in the script-based transform, which means you can refer to elements of the settings object using the regular JavaScript dot notation.\nHooks: the settings object is available, at present, in most hooks.\nUsing settings in mappings\nCustom settings are currently not available for import mapping and response mapping. In order to work around this limitation, we recommend you copy any settings that you would like to use in mapping into the flow data itself before the mapping step, ideally as part of a preMap hook.\nIn this example, we are copying the region attribute from the flow settings back into the data so that it can be used in the mapping step:"",
  ""date"": ""Wed, 21 May 2025 14:57:01 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/360045461272-Create-custom-settings-access-them-in-integrator-io-forms"",
  ""label_names"": [
  ""Advanced Field Editor"",
  ""scripts"",
  ""custom fields"",
  ""settings"",
  ""forms"",
  ""Handlebars""
  ],
  ""linkedArticleIds"": [
  ""360058595552"",
  ""115003929872"",
  ""360025927811"",
  ""12581205330587"",
  ""360007562931"",
  ""4536629083035"",
  ""4414777521307"",
  ""20228312523931""
  ],
  ""numTokens"": 651.0,
  ""score"": 0.81093061,
  ""section_id"": 360009451832.0,
  ""source"": ""articles"",
  ""title"": ""Create custom settings & access them in integrator.io forms""
  },
  {
  ""articleId"": 360025641652.0,
  ""content"": ""When you open an integration, you can use the tools available from the following tabs to track the status of flows in your integration.\nArticle links\n1. Settings tab\n2. Flows tab\n3. Dashboard tab\n4. Connections tab\n5. Notifications tab\n6. Audit log tab\n7. Analytics tab\n8. Users tab\n9. Admin tab\n10. Aliases tab\n11. Revisions tab\nThese tabs help you navigate your integration, including reviewing errors, editing flows, monitoring your data, and cloning or deleting an integration.\nYour Settings tab helps you customize your forms and fields at the integration, flow, export, import, or connection level. Click Launch form builder to create a custom form to expose and manage your settings fields, so that less technical users do not need to work with raw JSON.\nThe Flows tab allows you to monitor the status of all your integration flows. Open this tab to review your flows\u2019 status, monitor your errors, edit your mappings, add an import, and run your flow, among other features.\nThe integration Dashboard shows you the status of every flow that ran for the last 30 days or however long your data retention plan allows. From here, you can watch your flows run in real-time, review errors as they occur, and review your run times. The dashboard has two tabs: Running flows and Completed flows.\nYour Connections tab is where you can monitor the communication channels through which you import and export data to your applications. All functional flows require validated online connections.\nWith the Notifications tab, you can be notified (or have specific users notified) by email whenever a flow encounters an error or if the connection used by a flow goes offline so that you can quickly fix the problem. Each user must belong to an integrator.io account to receive notifications.\nThe Audit log tab maintains a record of specific activities in your integrations and flows, including the fields that were changed, the type of change, and who\u2019s responsible for the change. You can view your audit logs at a flow or integration level. You can also view the audit log of the change in data retention period.\nUse the Analytics tab within an integration to view errors, successes, ignores, and average processing time per successful record over time in visual charts. They can help you spot patterns or trends and take action accordingly.\nThe Users tab displays all users accessing this integration and their assigned permission levels. You can also + Invite users to the account if you\u2019re an account owner.\nYour integration\u2019s Admin tab gives you general information about your integration, including a Name and Description. Describe your integration here so that other users can quickly understand the high-level business problems being solved. Be sure to highlight any nuances other users might need to know to work in this integration.\nYour Aliases tab provides an easy and efficient way to reference resources in your integration. For example, instead of referring to a specific connection ID in a script, you can use an alias for that connection instead. An alias makes maintaining the script easier and less error-prone because it prevents you from manually changing the referenced ID later, such as if you clone the flow and the clone uses a different connection. For more information on creating an alias, see Create and manage aliases.\nThe Revisions tab allows you to manage your version control, release management, and revisions. See Lifecycle Management (LM) overview and learn more about creating versions and merging integration changes.\nCheck out our community forum to ask questions and post tips."",
  ""date"": ""Fri, 15 Mar 2024 15:04:19 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/360025641652-Navigate-your-integration"",
  ""linkedArticleIds"": [
  ""6844686337563"",
  ""7185411750043"",
  ""7326016708763"",
  ""7364424026907"",
  ""8961419557019"",
  ""6514515710107"",
  ""7788318803867"",
  ""7442970966043"",
  ""7703422519707"",
  ""4454740861979"",
  ""5349100943003"",
  ""115000757512"",
  ""360025642872"",
  ""360043040652"",
  ""4454740861979"",
  ""5349100943003"",
  ""4844290103707""
  ],
  ""numTokens"": 703.0,
  ""score"": 0.809680104,
  ""section_id"": 6306461554075.0,
  ""source"": ""articles"",
  ""title"": ""Navigate your integration""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Launch the Form Builder to create user-friendly forms for managing settings.""
  },
  {
  ""title"": ""Define custom settings in JSON to parameterize integration logic.""
  },
  {
  ""title"": ""Access the Dev Playground to test and refine your settings configurations.""
  }
  ],
  ""thread_id"": ""thread_90Aom6NHXEANemoCaUUTAFer"",
  ""vector_duration"": 1.78
 }",,,,,,,,,,,,,,,,
Copilot_70,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,EDI_profiles_001.json,"{
  ""answer"": ""You are currently on the **'Resources - EDI profiles'** page within Celigo's integrator.io platform. This page allows you to manage **EDI profiles**, which are a centralized configuration resource for handling the exchange of EDI data with your trading partners. Instead of scattering configurations across flows, file definitions, and mappings, EDI profiles streamline EDI processing for your integrations by keeping all trading partner-specific settings in one place.\n\nHere\u2019s what the **EDI profiles** page is for:\n- It helps configure, manage, and update EDI data exchange settings with trading partners.\n- It ensures consistency and accuracy in EDI processing by minimizing errors through centralized management.\n- You can create, edit, or view audit logs for your EDI profiles to track changes or troubleshoot issues.\n\nIf you're wondering what to do here, you can:\n1. **Create a new EDI profile**: Use the \""Create EDI profile\"" button to define settings for a specific trading partner.\n2. **Edit an existing EDI profile**: If you already have profiles listed, use the \""Actions\"" (three-dot menu) next to a profile to edit or update it.\n3. **View audit logs**: Look at the change history to understand who modified a profile, when, and what changes were made.\n\nThis page is available only if you\u2019ve subscribed to **B2B Manager** as part of your integrator.io license. Let me know if you\u2019d like more help with what you need to do on this page!"",
  ""completion_tokens"": """",
  ""openai_duration"": 13.32,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 17834458166299.0,
  ""content"": ""To streamline EDI data processing, the trading partner configurations are centrally stored within the\u00a0EDI profiles\u00a0resource, avoiding scattered configurations across flows, file definition rules, and mappings. This centralized approach reduces configuration time, minimizes errors, and offers a global view of EDI-related validations and values at the header level, allowing easy updates for specific trading partner configurations.\nNote\nThe EDI profiles component is available if you have subscribed to B2B Manager as part of the integrator.io license subscription.\nNavigate to EDI profiles\nSign in to your integrator.io account.\nFrom the left navigation, go to Resources > EDI profiles.\nOr, you can access EDI profiles at the flow level by navigating to Integration > Flow > Flow Builder page > Export or Import bubbles > How would you like to parse files section > EDI profile. From the flow level, you can create or edit an EDI profile.\nCreate an EDI profile\nOn the EDI profiles page, on the top-right, click Create EDI profile.\nGeneral\nOn the Create EDI profile window, in the General section, enter the appropriate details in the respective fields:\nName: This is a mandatory field. Enter a name for the EDI profile.\nDescription: Enter the description for the EDI profile.\nEDI file type: This is a mandatory field and is non-editable. The field shows the EDI file type, currently, X12 is the file type supported.\nIdentification\nIn this section, you can find the information on the permitted value for a given trading partner in the EDI enveloping section of a file.\nInterchange identification\nISA01 (Authorization information qualifier): A code to identify the information type in the Authorization Qualifier. You can choose one option from the drop-down menu.\nISA02 (Authorization information): This is used for identifying the authorization code information about the interchange sender or the data in the interchange. Enter an appropriate value.\nISA03 (Security information qualifier): A code to identify the information type in the Security Information. You can choose one option from the drop-down menu.\nISA04 (Security information): This is used for identifying the security information about the interchange sender or the data in the interchange. This is used for identifying the security information about the interchange sender or the data in the interchange; the type of information is set by the Security Information Qualifier ISA03.\nTrading Partner ID qualifier: A code indicating the system of code structure used to designate the Trading Partner ID element being qualified. This field is to be used as ISA05 for Inbound documents from Trading Partners for EDI Parsing and ISA07 for Outbound documents to Trading Partners for EDI generation.\nTrading Partner interchange ID: An identification code published by the Trading Partner for other parties to use as the ID to exchange data. This field is to be used as ISA06 for Inbound documents from Trading Partners for EDI Parsing and ISA08 for Outbound documents to Trading Partners for EDI generation.\nMy ID qualifier: A code indicating the system of code structure used to designate the Trading Partner ID element being qualified. This field is to be used as ISA07 for Inbound documents from Trading Partners for EDI Parsing and ISA05 for Outbound documents to Trading Partners for EDI generation. You can choose one option from the drop-down menu.\nMy interchange ID: A code published for you for the other parties to use as the ID to exchange data. This field is to be used as ISA08 for Inbound documents from Trading Partners for EDI Parsing and ISA06 for Outbound documents to Trading Partners for EDI generation.\nInterchange control information\nISA11 (Interchange standards ID): This field provides the delimiter used to separate repeated occurrences of a simple data element; this value must be different than the data element separator, component element separator, and segment terminator.\nISA12 (Interchange version number): A code specifying the version number of the interchange control segments.\nISA14 (Acknowledgement requested): A code indicating the sender's request for an interchange acknowledgment.\nISA15 (Usage indicator): A code to indicate whether data enclosed by this interchange envelope is test data or production data.\nISA16 (Component Element Separator): This field provides the delimiter used to separate component data elements. This value must be different from the data element separator and the segment terminator.\nGroup identification\nTrading Partner group ID: An identification code published by the Trading Partner for other parties to use as the ID to exchange data. This field is used as GS02 for Inbound documents from Trading Partners and GS03 for Outbound documents to trading partners.\nMy group ID: An identification code published by Celigo\u2019s customer for other parties to use as the ID to exchange data. This field is used as GS03 for Inbound documents from Trading Partners and GS02 for Outbound documents to trading partners.\nGS07 (Responsible agency code): A code identifying the issuer of the standard; this code is used in conjunction with Data Element GS08. You can choose one option from the drop-down menu.\nGS08 (Version): A code indicating the version, release, sublease, and industry identifier of the EDI standard being used\nControl numbers\nControl number sequence - Value: The control number, sequentially used in EDI messages for Interchange and Group level segments in fields ISA13, GS06, GE02, and IEA02, starts with a default value of '1'. This initial value is applied to the first EDI file, with each subsequent file's control number incrementing by 1. When the control number hits 999,999,999 (nine nines), it resets to the initially configured value by the user.\nAfter you enter the details and click Save on the Create EDI profile window, a new entry is created on the EDI profiles page.\nNote\nThe control number sequence is not supported for each document type.\nEDI profile at the flow level\nFor any flow, in the export or import bubbles, after you select the EDI file type as X12, the EDI profile option is enabled.\nFrom the EDI profile drop-down menu, you can select an existing profile\nYou can create a new profile\nYou can edit an existing profile\nView the audit log\nAudit logs maintain a record of specific activities in your flows, including the fields that were changed, the type of change, and who is responsible for the change. You can view your audit logs at a flow or EDI profile level. For more information, see View or download audit logs.\nIn integrator.io, on the EDI profile page, next to the appropriate profile, from the Actions overflow (\u2026) menu, select View audit log.\nIn audit logs, pageProcessors and pageGenerators represent imports and exports, respectively. It\u2019s just a system naming convention that doesn\u2019t affect your integrations or flows.\nEdit an EDI profile\nYou can edit an already-created user profile. While editing an EDI profile, the EDI file can\u2019t be edited.\nIn integrator.io, on the EDI profile page, next to the appropriate profile, from the Actions overflow (\u2026) menu, select Edit EDI profile.\nAfter you edit the profile, on the EDI profiles page, the Last updated field will be updated with the latest timestamp.\nEDI profile preview\nBefore executing the EDI profile at the flow level, you can preview the profile for validations. In integrator.io, on the Flow Builder page, from the export or import bubbles, after you select the Parser/Generation definition, click Launch to view the \u2018EDI parser helper/generator\u2019 Advanced Field Editor (AFE).\nFor the EDI parser helper and EDI generator, the EDI profile execution at the flow level is similar to that of the preview. When you open AFE, by default, the Auto preview is enabled, and the EDI profile is executed.\nIn the EDI parser helper, the AFE must validate the EDI data received from the Trading partner and convert the data to a JSON format. The preview and Auto preview options are applicable for the execution of the EDI profiles.\nIn the EDI generator helper, the AFE must recognize the EDI profile selection from import, replace the EDI data received from the Trading Partner, and convert the data to a target ERP format. The preview and Auto preview options are applicable for the execution of the EDI profiles."",
  ""date"": ""Thu, 17 Jul 2025 15:59:49 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/17834458166299-Create-EDI-profiles-for-EDI-X12-flows"",
  ""label_names"": [],
  ""linkedArticleIds"": [
  ""6514515710107""
  ],
  ""numTokens"": 1694.0,
  ""score"": 0.857086182,
  ""section_id"": 28037707232539.0,
  ""source"": ""articles"",
  ""title"": ""Create EDI profiles for EDI X12 flows""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Create a new EDI profile for a trading partner using the \""Create EDI profile\"" button.""
  },
  {
  ""title"": ""Edit an existing EDI profile by selecting it and choosing \""Edit EDI profile\"" from the Actions menu.""
  },
  {
  ""title"": ""View the audit log of an EDI profile to review changes by selecting \""View audit log\"" from the Actions menu.""
  }
  ],
  ""thread_id"": ""thread_XulkbV90p3c9es9qQ5gdyY9l"",
  ""vector_duration"": 2.5
 }",,,,,,,,,,,,,,,,
Copilot_71,To verify the copilot answer similarity based on query and expected result,Copilot,Simple,QA Query,/gptverse/knowledge-base/answer,Security_001.json,"{
  ""answer"": ""No, your sensitive data is not visible to Celigo. Based on the provided documentation, Celigo\u2019s **integrator.io platform** prioritizes data security and privacy by ensuring that user data is encrypted and processed securely.\n\nHere\u2019s a detailed explanation:\n\n1. **Data Encryption**: All data in transit and at rest is encrypted.\n  - All data in transit inside AWS VPC is encrypted at **TLS 1.2 or better**.\n  - All temporarily stored data within AWS is encrypted using **AES-256** encryption.\n\n2. **Data Handling**:\n  - Celigo\u2019s integrator.io platform does not persistently store any processed data. It acts as a **pipeline**, transferring data from one endpoint to another.\n  - Any data temporarily held (e.g., for error analysis during an integration's failure) is encrypted and stored on **S3 buckets**. This data is automatically deleted after 30 days, or earlier if the flow is retried and resolved successfully.\n\n3. **Access to Credentials**:\n  - Your API credentials, which you provide for integration connections, are encrypted and securely stored.\n  - If you require **Professional Services assistance**, you can securely provide credentials to Celigo for flow-building, ensuring they remain encrypted and protected.\n\n4. **Complementary Responsibility**:\n  - Celigo ensures and enforces strict encryption and security practices, but users are responsible for securing their endpoints (e.g., using HTTPS connections and TLS 1.2 or higher).\n\n### Regarding SSO (Single Sign-On):\n  - SSO is a secure method for user authentication. It ensures that only authorized individuals can access your Celigo environment. Celigo does not access or store your SSO passwords or sensitive authentication details.\n\n### Summary:\n1. Your sensitive data is encrypted and secured during the integration process.\n2. Celigo does not persistently store any of your processed data or make it visible to unauthorized users.\n3. You remain responsible for ensuring secure endpoints and access credentials on your side.\n\nIf you have additional concerns about specific data aspects, contacting the **Celigo Support Team** or emailing **compliance@celigo.com** would provide further assurance."",
  ""completion_tokens"": """",
  ""openai_duration"": 15.78,
  ""prompt_tokens"": """",
  ""sources"": [
  {
  ""articleId"": 13523742710555.0,
  ""content"": ""The Celigo platform is built using best-of-breed technology frameworks and secure software development practices. Production and testing environments are completely segregated from each other, and customer data is never used in QA or developer testing.\nCeligo has a designated Sr. Director of Security and Compliance and Data Protection Officer to lead the Security and Compliance Team and work with technical staff to support the implementation of the security requirements needed to operate at the levels of security and compliance that our management and our customers expect.\nFor more information:\nCeligo privacy policy, cookie policy, and GDPR compliance\nTransport Layer Security (TLS)\nThe highly scalable and robust iPaaS (PDF)\nSecurity guidelines for integrator.io\nKeeping your data secure while integrating with Celigo\nProtocols\nIncoming connections: TLS 1.2\nHTTPS client traffic: TLS 1.2\nEndpoint/FTP connections: HTTP and HTTPS TLS 1.0 to TLS 1.3 (highest available automatically selected)\nApplication authorizations and trusted connections\nAccount owners and administrators completely control authorization per user and per application.\nAudit logs\nKeep track of activity on your account for up to a year. Monitor integration and flow changes over the course of the resource\u2019s lifecycle.\nRegulatory compliance\nCeligo handles all data at the highest level required for regulatory and voluntary compliance requirements, ensuring cloud security at multiple levels:\nEU and UK\nEU/UK GDPR-Ready\nContact Celigo for a Data Processing Agreement (DPA).\nUS State Privacy (A US Data Privacy addendum is available)\nCalifornia CCPA/CRPA - Ready\nNevada Chapter 603-A \u2013 Covered by GDPR and CCPA- Ready\nNew York SHIELD ACT \u2013 Covered by GDPR and CCPA- Ready\nVirginia Consumer Data Protection Act (VCDPA) - Ready\nTexas Data Privacy and Security Act (TDPSA) - Ready\nVoluntary compliance\nSOC-2 \u2013 Type 2 compliant\nAs a customer or a prospect, you may request a copy of the SOC 2 report under Mutual NDA from compliance@celigo.com.\nHIPAA \u2013 HIPAA-ready, though not HIPAA-certified\nHiTech \u2013 Not HiTech standard-certified, though qualified to support HiTech standard-certified companies (as a Business Associate to either a Covered Entity or another Business Associate) under the certified HiTech service providers\u2019 infrastructure\nStatus \u2013 The Celigo Security Team has completed implementation for infrastructure encryption: restricted access to ePHI data, trained select staff, and provided secure laptops with full-disk encryption\nFERPA-ready\nFedRAMP \u2013 Not certified, since Celigo is not directly U.S. government-facing, but we can support companies that are FedRAMP certified as part of the certified FedRAMP service providers\u2019 infrastructure\nUS, EU, UK, and Swiss Data Privacy Framework Certified\nData retention\nThe error data retention period lasts for 30 or more days, based on your Celigo license. You can delete records if you choose to or if your customers exercise their Right to Delete.\nData protection\nData is encrypted in motion and at rest, according to country-specific data protection and privacy guidelines:\nAll data in motion inside AWS VPC \u2013 Encrypted at TLS 1.2 or better\nAll data temporarily stored in AWS \u2013 Encrypted at AES 256\nStored credentials \u2013 Encrypted at either AES 256 or pbkdf2\nWe support the highest level of HTTPS API TLS encryption available. For example, when NetSuite supports TLS 1.2, then the Celigo connection is also encrypted to TLS 1.2. Therefore, it is the customer's responsibility to ensure that the endpoint encryption is at least TLS 1.2. Otherwise, that particular segment of the data flow may not be considered securely encrypted \u2013 or encrypted at all in the case of an HTTP API. (Note that TLS 1.0 and 1.1 have been deprecated by the security community.)\nEncryption keys\nCeligo has enabled SSE-S3 for all Amazon S3 buckets. Each file saved in S3 is encrypted and its Key is encrypted using a master key stored separately.\nEndpoint API credentials are provided by Celigo customers, who are responsible for updating tokens and passwords according to their security policies.\nAPI credential security\nYou provide your API credentials for your endpoint, then they are encrypted and stored in integrator.io for subsequent flow steps.\nProfessional Services customer credentials vaulting\nNeed help developing flows? You can provide your credentials to us securely so that we can build it for you."",
  ""date"": ""Fri, 27 Sep 2024 21:22:41 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/13523742710555-Security-and-compliance"",
  ""linkedArticleIds"": [
  ""360050374512"",
  ""360058028072"",
  ""360045215251"",
  ""6514515710107"",
  ""6359380074779""
  ],
  ""numTokens"": 914.0,
  ""score"": 0.836036623,
  ""section_id"": 13322551262875.0,
  ""source"": ""articles"",
  ""title"": ""Security and compliance""
  },
  {
  ""articleId"": 31449567156891.0,
  ""content"": ""Celigo Private Cloud provides the following data residency and compliance options.\nRegional hosting\nCeligo Private Cloud allows you to host an instance of Celigo in your preferred region with robust security controls. You can choose from a global network of data centers to meet regional compliance requirements, such as GDPR (General Data Protection Regulation) or CCPA (California Consumer Privacy Act). Regional hosting also reduces latency by placing your instance closer to operational centers or end users. You can specify your hosting region during onboarding, to ensure data remains within the geographic boundaries required by applicable regulations. Supported hosting regions include North America, Europe, Asia-Pacific, and others, with specific options available upon request.\nStandards compliance\nCeligo Private Cloud provides compliance with the following global standards:\nSOC 2 Type II for secure data management.\nISO/IEC 27001 for information security management.\nGDPR for data protection in the EU.\nCCPA for privacy requirements in California.\nData protection measures\nEncryption: All data is encrypted at rest and in transit using strong encryption protocols, ensuring data confidentiality and integrity.\nAccess controls: Granular, role-based access control (RBAC) ensures only authorized users can access sensitive data.\nAudit logs: Comprehensive logging and monitoring provide visibility into data access and system activity for compliance reporting.\nCustom compliance configurations: If you have specific industry requirements, such as Health Insurance Portability and Accountability Act (HIPAA) for healthcare or Payment Card Industry Data Security Standard (PCI DSS) for payment data, Celigo can tailor your dedicated instance to meet these needs.\nCompliance monitoring and reporting\nProactive Monitoring: Celigo employs advanced monitoring tools to track the security and performance of private cloud instances, helping identify and resolve potential compliance risks proactively.\nAudit Support: You can access logs, compliance documentation, and third-party certifications (like, SOC 2 reports) to facilitate internal and external audits. Celigo provides assistance during compliance reviews or audits as needed.\nIncident Response: Celigo\u2019s incident response plan ensures rapid detection, communication, and resolution of any security or compliance incidents. You will be promptly notified of incidents that could impact compliance."",
  ""date"": ""Thu, 12 Dec 2024 16:52:02 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/31449567156891-Data-residency-and-compliance-for-Celigo-Private-Cloud"",
  ""label_names"": [],
  ""linkedArticleIds"": [
  ""31449555282459""
  ],
  ""numTokens"": 427.0,
  ""score"": 0.824409068,
  ""section_id"": 31448031655707.0,
  ""source"": ""articles"",
  ""title"": ""Data residency and compliance for Celigo Private Cloud""
  },
  {
  ""articleId"": 360058028072.0,
  ""content"": ""This document is for all integrator.io customers, including Free Trial (30-day Free Trial for full use) and Free Edition (a single flow used free) users.\nintegrator.io is an iPaaS integration tool for integrating your data from one endpoint to another (typically two SaaS endpoints used for business). integrator.io doesn\u2019t persistently store any processed data. Think of it as a pipeline of data traveling between endpoints. All in-process data is deleted on completion of the integration. These endpoints can be nearly anything that has an API, and Celigo has many prebuilt flows and templates to build upon. Your endpoint security is of paramount importance to keeping your data secure. Just as critical are the processes used to manage your integrations in integrator.io\nDisclaimer: This information does not constitute legal, security, or compliance advice. Rather, it is intended to start the thought process for a secure and compliant integration implementation by you and your team. Establishing and maintaining appropriate measures to achieve a secure and compliant integration implementation is solely your responsibility. We encourage you to use best practices in doing so, including to discuss all such matters with your own legal, privacy, security, and compliance teams.\nCeligo prioritizes the security of your data (both in motion, and for the short time it is at rest during processing). We do not persistently store any data we integrate for you. integrator.io encrypts data at all points throughout the integration process, with the caveat that you are responsible for selecting secure endpoints. For some sites, like NetSuite, those endpoints are always HTTPS and TLS 1.2 or better. Endpoint security cannot always be assumed. TLS 1.0 and 1.1 have been deprecated for security vulnerabilities, and security threats can jeopardize the safety of data traveling to or from unencrypted HTTP endpoints or encrypted HTTPS endpoints that use TLS less than 1.2. Celigo recommends always using HTTPS endpoints. While integrator.io will select the highest level of TLS available, it still requires due diligence on your part to ensure the endpoint meets your security requirements.\nThis document describes other aspects of managing your integrations that are also your responsibility:\nAccount management\nYou and your account owner and administrator(s) are responsible for establishing and terminating user accounts within integrator.io. You and your staff are responsible for keeping integrator.io user account credentials secure and managing user accounts.\nRecommendations for account security\nThe first person to sign in to your instance of integrator.io becomes the account owner. Account owners have full permission to perform all possible actions in an account, and can transfer their ownership of the account to a different user if necessary.\nYou can assign administrator, manage, or monitor permissions to any user who accepts an invitation to access your instance of integrator.io. Both account owners and administrators can promote or demote other users as needed. See Manage account and integration permissions for all information related to user permissions.\nintegrator.io passwords must meet the following system-enforced requirements:\nat least one uppercase letter\nat least one number\nat least 10 characters long\nshould not match with the previous 20 passwords (when changing the password).\nNote: integrator.io also has functions for users to reset forgotten passwords.\nAccount ownership\nMultiple options exist for provisioning users of Celigo\u2019s integrator.io products. Those options may all meet a customer\u2019s security needs based on internal analysis. Celigo recommends the following approach as a best practice for your consideration.\nThe account owner email address should ALWAYS be on the same domain as the company.\nNever use an email that is not under your full control as the account owner email. You should be able to activate, deactivate, and access the account owner\u2019s email should the account owner leave the company.\nUse an email alias such as celigo_owner@mycompany.com for the account owner. Avoid using an email address that is bound to the name of the person acting as the account owner in your organization.\nStore the password for the account owner email in a secure location accessible by at least two people with disaster redundancy in mind. Examples could include:\nA secure password manager with credentials accessible by two users.\nCredentials printed and stored safely in two locations accessible by different users.\nA combination of both approaches.\nImportant: Do NOT use the account owner email for day-to-day access or administration of integrator.io.\nLog in as the account owner initially, and assign at least one administrator using a named email address (terry.smith@mycompany.com).\nNote: The only reason to log in as the account owner would be to transfer account ownership to another email address, which is unlikely except for changes in corporate structure.\nThe designated administrator(s) will login in with their personal credentials and perform administrative functions. External vendors, consultants, or contractors should never be the owner of your account.\nDo not use aliased user accounts such as celigo@mycompany.com for anything but read-only purposes or the account owner login. integrator.io users are responsible for creating connections to endpoints, and ensuring that the credentials used by such connections have the appropriate level of privilege. Follow the principle of least privilege to ensure that connections have only enough access to perform the tasks required by the job.\nAdministrator role\nAdministrators have all the same permissions that an account owner has to modify user roles; but, an administrator cannot alter the permissions of an account owner or transfer ownership of the account to another user.\nThe administrator can invite other integrator.io users to their integration environment and configure their permissions to view or change integrator.io components.\nAdministrators can invite new users and assign or modify user permissions. (Administrators can also add new administrators.) Administrators can delete users from an account, change their own permissions, and delete themselves from an account.\nRecommended user-permission model\nStart with a secure and dedicated account owner email address that is not bound to a specific user\u2019s name, then create one or more administrator accounts that can fulfill all other permission assignments for additional users.\nEndpoint management\nAs mentioned above, always use HTTPS. You are responsible for using secure encrypted HTTPS (SSL/TLS) connections for all of their interactions and connections made from one SaaS endpoint to another SaaS endpoint with integrator.io.\nYou are solely responsible for the rotation of your endpoint API access tokens or credentials. Where possible within your endpoint systems, create customized roles for the integration access used by integrator.io. These roles should provide the least-privileged access required to accomplish the integration objective. Concerns for efficiency may result in the usage of administrative roles for integration development, but these should be changed at go-live to less-privileged roles. The effort to design and test custom roles in the endpoint systems is solely your responsibility unless explicitly stated otherwise in a statement of work.\nFTP encryption\nAs with endpoint management, always encrypt by using SFTP or FTPS. These are available when you develop your integration, but it requires that your FTP endpoint supports Secure FTP (if possible, block insecure FTP at the endpoint site as well).\nData privacy\nData privacy and security is an increasingly prominent concern of customers, and Celigo takes it very seriously. The integrator.io environment and the general company management practices are annually audited for meeting requirements based on the trust services criteria relevant to Security and Availability (applicable trust services criteria) set forth in TSP section 100, 2017 Trust Services Criteria for Security, Availability, Processing Integrity, Confidentiality, and Privacy (AICPA, Trust Services Criteria), and Celigo has the resulting SOC 2 Type 2 report available under NDA. We also perform annual Pen Testing, and Quarterly Vulnerability testing; and where needed remediation, and a Summary is available, again under NDA.\nIn addition, Celigo is GDPR, CCPA, HIPAA, and FERPA ready, and always processes data in line with GDPR CCPA, HIPAA and FERPA requirements; and have the following recommendations for some of these specifically \u2013 Celigo is not PCI-DSS certified; however:\nPCI-DSS \u2013 never integrate cardholder data unless it is first tokenized and has been approved by your PCI-DSS governance processes.\nGPDR \u2013 EU/UK residents are not protected without a Data Processing Agreement (DPA) in place. You can request a DPA by emailing compliance@celigo.com\nHIPAA \u2013 never integrate ePHI patient data unless a BAA has been executed between your company and Celigo.\nAdditionally, integrator.io only holds personally identifiable information (PII) from your connections and flows in the event that a connection is down. This data is written to an S3 bucket with AES-256 encryption, timestamped, and stored for 30 days from the timestamp. If the flow runs successfully within those 30 days the information is immediately deleted. If it has an error, the data is held for 30 days from the timestamp for you to perform error analysis and retry your flow. If you successfully resolve your flows within the 30-day timestamp, your PII is immediately deleted.\nYou should collect or integrate only the minimum personal data needed to accomplish your purpose, to comply with best practices for handling and using personal data. This practice should be designed in from the time you first collect the data, and subsequent processing of any kind should take the same approach: is the data absolutely necessary for the job at hand?\nEndpoint security is your responsibility\nYou are responsible for any newly created endpoints or endpoints connecting with integrator.io. Some integrations may extract data from one source to populate resources at another new endpoint. This may be an ongoing process, and the new endpoint can be used for many diverse purposes. You are responsible for verifying the security of these endpoints. When developing integrations that use these endpoints, you take full responsibility for protecting your data with robust security and backups when stored in source and destination systems.\nData encryption and environmental security\nBest practices include using the tools securely, and while we enforce the best available encryption end-to-end for integrator.io, connecting to a plain HTTP endpoint or using an HTTPS endpoint less than TLS 1.2 will compromise security for that leg of the integration Always verify the endpoint security for HTTPS and for TLS 1.2 or better.\nUse full-disk encryption on the computing device used for managing integrations. Do not use Microsoft Internet Explorer as a browser because it has been deprecated as insecure. Keep all browsers used with integrator.io up-to-date with security updates.\nBackups\nYou are responsible for backing up data within the SaaS endpoints that integrator.io is connecting with. integrator.io integrates data and never persistently stores any data nor backs it up to prevent data proliferation. Any backups are your responsibility, and that is generally done at one or more of the endpoints. Discuss backups with your BCP team.\nBusiness continuity planning\nWhile we have a resilient AWS production environment with an service-level agreement (SLA) of 99.95% uptime \u2013 with no planned down time - and have a documented and tested Business Continuity Plan (BCP), you are also responsible for developing your own Disaster Recovery and BCP that address any inability to access or utilize Celigo services.\nSecurity incidents or events\nYou are responsible for notifying Celigo at security@celigo.com if you detect or suspect a security incident related to integrator.io or if you suspect any security weaknesses or gaps.\nComplementary user entity controls\nImplement the following user entity controls to provide additional assurance that the general security and the Trust Services Criteria requirements described within this report are met. Your security auditors should review and select the appropriate complementary user entity controls according to your organization's needs.\nYou are responsible for understanding and complying with your contractual obligations to Celigo.\nYou should maintain formal policies that provide guidance for information security and data classification within your organization and the supporting IT environment.\nYou are responsible for using secure encrypted HTTPS (TLS/SSL) connections for all interactions and connections made from one SaaS to another SaaS with Integration Apps or integrator.io.\nYou are responsible for the establishment and termination of user accounts within integrator.io.\nYou are responsible for keeping your user account credentials secure for integrator.io.\nYou are responsible for ensuring the supervision, management, and control of the use of Celigo services by your personnel.\nYou are responsible for reviewing notifications from Celigo about changes to Integration Apps or integrator.io.\nYou are responsible for any resources created with integrator.io.\nYou are responsible for backing up data within your SaaS endpoints, Integration Apps, or integrator.io is connecting to.\nYou are responsible for developing your own disaster recovery and business continuity plans that address the inability to access or utilize Celigo services.\nYou are responsible for notifying Celigo if you detect or suspect a security incident related to Integration Apps or integrator.io.\nYou are responsible for using secure FTP (SFTP, FTPS) for all of your FTP transfers made from one SaaS to another SaaS with Integration Apps or integrator.io."",
  ""date"": ""Mon, 09 Jun 2025 14:46:02 GMT"",
  ""html_url"": ""https://docs.celigo.com/hc/en-us/articles/360058028072-Platform-security-guidelines"",
  ""label_names"": [
  ""Public key infrastructure (PKI)"",
  ""data retention""
  ],
  ""linkedArticleIds"": [
  ""4414582961819"",
  ""115003929872"",
  ""115003929872"",
  ""115003929872"",
  ""360021134551"",
  ""115003929872""
  ],
  ""numTokens"": 2618.0,
  ""score"": 0.816323698,
  ""section_id"": 5914460555675.0,
  ""source"": ""articles"",
  ""title"": ""Platform security guidelines""
  }
  ],
  ""suggestions"": [
  {
  ""title"": ""Enable SSO by configuring your identity provider in the Security section.""
  },
  {
  ""title"": ""Verify endpoint security by ensuring HTTPS and TLS 1.2 compliance for all integrations.""
  },
  {
  ""title"": ""Contact compliance@celigo.com for further data security or privacy assurance inquiries.""
  }
  ],
  ""thread_id"": ""thread_O9z64m44LEn8DUMYcddlcwNj"",
  ""vector_duration"": 1.85
 }",,,,,,,,,,,,,,,,